2025-06-22 22:29:59,752 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.23 (main, Jun  5 2025, 13:40:20) [GCC 11.2.0]
CUDA available: True
GPU 0,1,2: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda-11.7
NVCC: Cuda compilation tools, release 11.7, V11.7.64
GCC: gcc (Ubuntu 11.4.0-2ubuntu1~20.04) 11.4.0
PyTorch: 1.13.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.1
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.7
MMDetection: 2.25.1+
------------------------------------------------------------

2025-06-22 22:30:03,429 - mmdet - INFO - Distributed training: False
2025-06-22 22:30:07,119 - mmdet - INFO - Config:
dataset_type = 'PanopticSceneGraphDataset'
ann_file = './data/psg/psg.json'
coco_root = './data/coco'
seg_root = './data/coco/annotations'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadPanopticSceneGraphAnnotations',
        with_bbox=True,
        with_rel=True,
        with_mask=True,
        with_seg=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[{
            'type':
            'Resize',
            'img_scale': [(480, 800), (512, 800), (544, 800), (576, 800),
                          (608, 800), (640, 800), (672, 800), (704, 800),
                          (736, 800), (768, 800), (800, 800)],
            'multiscale_mode':
            'value',
            'keep_ratio':
            True
        }],
                  [{
                      'type': 'Resize',
                      'img_scale': [(400, 800), (500, 800), (600, 800)],
                      'multiscale_mode': 'value',
                      'keep_ratio': True
                  }, {
                      'type': 'RelRandomCrop',
                      'crop_type': 'absolute_range',
                      'crop_size': (384, 600),
                      'allow_negative_crop': False
                  }, {
                      'type':
                      'Resize',
                      'img_scale': [(480, 800), (512, 800), (544, 800),
                                    (576, 800), (608, 800), (640, 800),
                                    (672, 800), (704, 800), (736, 800),
                                    (768, 800), (800, 800)],
                      'multiscale_mode':
                      'value',
                      'override':
                      True,
                      'keep_ratio':
                      True
                  }]]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=1),
    dict(type='RelsFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_rels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadSceneGraphAnnotations', with_bbox=True, with_rel=True),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_bboxes', 'gt_labels']),
            dict(
                type='ToDataContainer',
                fields=({
                    'key': 'gt_bboxes'
                }, {
                    'key': 'gt_labels'
                })),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=dict(
        type='PanopticSceneGraphDataset',
        ann_file='./data/psg/psg.json',
        img_prefix='./data/coco',
        seg_prefix='./data/coco/annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadPanopticSceneGraphAnnotations',
                with_bbox=True,
                with_rel=True,
                with_mask=True,
                with_seg=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='AutoAugment',
                policies=[[{
                    'type':
                    'Resize',
                    'img_scale': [
                        (480, 800), (512, 800), (544, 800), (576, 800),
                        (608, 800), (640, 800), (672, 800), (704, 800),
                        (736, 800), (768, 800), (800, 800)
                    ],
                    'multiscale_mode':
                    'value',
                    'keep_ratio':
                    True
                }],
                          [{
                              'type': 'Resize',
                              'img_scale': [(400, 800), (500, 800),
                                            (600, 800)],
                              'multiscale_mode': 'value',
                              'keep_ratio': True
                          }, {
                              'type': 'RelRandomCrop',
                              'crop_type': 'absolute_range',
                              'crop_size': (384, 600),
                              'allow_negative_crop': False
                          }, {
                              'type':
                              'Resize',
                              'img_scale': [(480, 800), (512, 800), (544, 800),
                                            (576, 800), (608, 800), (640, 800),
                                            (672, 800), (704, 800), (736, 800),
                                            (768, 800), (800, 800)],
                              'multiscale_mode':
                              'value',
                              'override':
                              True,
                              'keep_ratio':
                              True
                          }]]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='RelsFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_rels', 'gt_masks'])
        ],
        split='train',
        all_bboxes=True),
    val=dict(
        type='PanopticSceneGraphDataset',
        ann_file='./data/psg/psg.json',
        img_prefix='./data/coco',
        seg_prefix='./data/coco/annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadSceneGraphAnnotations',
                with_bbox=True,
                with_rel=True),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='ToTensor', keys=['gt_bboxes', 'gt_labels']),
                    dict(
                        type='ToDataContainer',
                        fields=({
                            'key': 'gt_bboxes'
                        }, {
                            'key': 'gt_labels'
                        })),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='test',
        all_bboxes=True),
    test=dict(
        type='PanopticSceneGraphDataset',
        ann_file='./data/psg/psg.json',
        img_prefix='./data/coco',
        seg_prefix='./data/coco/annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadSceneGraphAnnotations',
                with_bbox=True,
                with_rel=True),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='ToTensor', keys=['gt_bboxes', 'gt_labels']),
                    dict(
                        type='ToDataContainer',
                        fields=({
                            'key': 'gt_bboxes'
                        }, {
                            'key': 'gt_labels'
                        })),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='test',
        all_bboxes=True),
    pin_memory=True)
checkpoint_config = dict(interval=1, max_keep_ckpts=15)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'pretrain/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
num_object_classes = 133
num_relation_classes = 56
find_unused_parameters = True
model = dict(
    type='PSGTr',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    bbox_head=dict(
        type='CrossHead2',
        num_classes=133,
        num_relations=56,
        num_obj_query=100,
        num_rel_query=100,
        mapper='conv_tiny',
        in_channels=[256, 512, 1024, 2048],
        feat_channels=256,
        out_channels=256,
        num_transformer_feat_level=3,
        embed_dims=256,
        enforce_decoder_input_project=False,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=256,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=128, normalize=True),
            init_cfg=None),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=False,
            num_layers=9,
            transformerlayers=dict(
                type='BaseTransformerLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True),
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm'))),
        relation_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=6,
            transformerlayers=dict(
                type='BaseTransformerLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.1,
                    dropout_layer=None,
                    add_identity=True),
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm'))),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        rel_cls_loss=dict(
            type='SeesawLoss',
            num_classes=56,
            return_dict=True,
            loss_weight=2.0),
        subobj_cls_loss=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=4.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0
            ]),
        importance_match_loss=dict(
            type='BCEWithLogitsLoss', reduction='mean', loss_weight=5.0),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        id_assigner=dict(
            type='IdMatcher',
            sub_id_cost=dict(type='ClassificationCost', weight=1.0),
            obj_id_cost=dict(type='ClassificationCost', weight=1.0),
            r_cls_cost=dict(type='ClassificationCost', weight=0.0)),
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        mask_assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(max_per_img=100))
custom_imports = dict(
    imports=[
        'pairnet.models.frameworks.psgtr', 'pairnet.models.losses.seg_losses',
        'pairnet.datasets', 'pairnet.datasets.pipelines.loading',
        'pairnet.datasets.pipelines.rel_randomcrop',
        'pairnet.models.relation_heads.approaches.matcher', 'pairnet.utils'
    ],
    allow_failed_imports=False)
evaluation = dict(
    interval=100000000,
    metric='sgdet',
    relation_mode=True,
    classwise=True,
    iou_thrs=0.5,
    detection_method='pan_seg')
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(lr_mult=0.1, decay_mult=1),
            transformer_decoder=dict(lr_mult=0.1, decay_mult=1),
            pixel_decoder=dict(lr_mult=0.1, decay_mult=1),
            decoder_input_projs=dict(lr_mult=0.1, decay_mult=1)),
        norm_decay_mult=0.0))
optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
lr_config = dict(policy='step', gamma=0.5, step=[5, 10])
runner = dict(type='EpochBasedRunner', max_epochs=15)
project_name = 'ATM'
expt_name = '3090_wjh'
work_dir = './work_dirs/3090_wjh'
auto_scale_lr = dict(enable=True, base_batch_size=8)
auto_resume = False
gpu_ids = [2]

2025-06-22 22:30:07,120 - mmdet - INFO - Set random seed to 10086, deterministic: True
2025-06-22 22:30:07,681 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

bbox_head.relation_decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.relation_decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.relation_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_query_embed2.weight - torch.Size([200, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_query_embed3.weight - torch.Size([200, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.0.0.weight - torch.Size([64, 1, 7, 7]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.0.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.1.0.weight - torch.Size([64, 64, 7, 7]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.1.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.2.0.weight - torch.Size([1, 64, 7, 7]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.update_importance.conv_layers.2.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.6.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.7.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.ffns.0.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in CrossHead2  

bbox_head.transformer_decoder.layers.8.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.cls_embed.weight - torch.Size([134, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.cls_embed.bias - torch.Size([134]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.sub_query_update.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.obj_query_update.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_cls_embed.weight - torch.Size([56, 256]): 
The value is the same before and after calling `init_weights` of PSGTr  

bbox_head.rel_cls_embed.bias - torch.Size([56]): 
The value is the same before and after calling `init_weights` of PSGTr  
2025-06-22 22:30:29,737 - mmdet - INFO - Training with 1 GPU(s) with 1 samples per GPU. The total batch size is 1.
2025-06-22 22:30:29,737 - mmdet - INFO - LR has been automatically scaled from 0.0001 to 1.25e-05
2025-06-22 22:30:39,414 - mmdet - INFO - load checkpoint from local path: pretrain/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth
2025-06-22 22:30:39,603 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: rpn.head.conv.weight, rpn.head.conv.bias, rpn.head.cls_logits.weight, rpn.head.cls_logits.bias, rpn.head.bbox_pred.weight, rpn.head.bbox_pred.bias, roi_heads.box_head.fc6.weight, roi_heads.box_head.fc6.bias, roi_heads.box_head.fc7.weight, roi_heads.box_head.fc7.bias, roi_heads.box_predictor.cls_score.weight, roi_heads.box_predictor.cls_score.bias, roi_heads.box_predictor.bbox_pred.weight, roi_heads.box_predictor.bbox_pred.bias, roi_heads.mask_head.mask_fcn1.weight, roi_heads.mask_head.mask_fcn1.bias, roi_heads.mask_head.mask_fcn2.weight, roi_heads.mask_head.mask_fcn2.bias, roi_heads.mask_head.mask_fcn3.weight, roi_heads.mask_head.mask_fcn3.bias, roi_heads.mask_head.mask_fcn4.weight, roi_heads.mask_head.mask_fcn4.bias, roi_heads.mask_predictor.conv5_mask.weight, roi_heads.mask_predictor.conv5_mask.bias, roi_heads.mask_predictor.mask_fcn_logits.weight, roi_heads.mask_predictor.mask_fcn_logits.bias, backbone.body.conv1.weight, backbone.body.bn1.weight, backbone.body.bn1.bias, backbone.body.bn1.running_mean, backbone.body.bn1.running_var, backbone.body.layer1.0.conv1.weight, backbone.body.layer1.0.bn1.weight, backbone.body.layer1.0.bn1.bias, backbone.body.layer1.0.bn1.running_mean, backbone.body.layer1.0.bn1.running_var, backbone.body.layer1.0.conv2.weight, backbone.body.layer1.0.bn2.weight, backbone.body.layer1.0.bn2.bias, backbone.body.layer1.0.bn2.running_mean, backbone.body.layer1.0.bn2.running_var, backbone.body.layer1.0.conv3.weight, backbone.body.layer1.0.bn3.weight, backbone.body.layer1.0.bn3.bias, backbone.body.layer1.0.bn3.running_mean, backbone.body.layer1.0.bn3.running_var, backbone.body.layer1.0.downsample.0.weight, backbone.body.layer1.0.downsample.1.weight, backbone.body.layer1.0.downsample.1.bias, backbone.body.layer1.0.downsample.1.running_mean, backbone.body.layer1.0.downsample.1.running_var, backbone.body.layer1.1.conv1.weight, backbone.body.layer1.1.bn1.weight, backbone.body.layer1.1.bn1.bias, backbone.body.layer1.1.bn1.running_mean, backbone.body.layer1.1.bn1.running_var, backbone.body.layer1.1.conv2.weight, backbone.body.layer1.1.bn2.weight, backbone.body.layer1.1.bn2.bias, backbone.body.layer1.1.bn2.running_mean, backbone.body.layer1.1.bn2.running_var, backbone.body.layer1.1.conv3.weight, backbone.body.layer1.1.bn3.weight, backbone.body.layer1.1.bn3.bias, backbone.body.layer1.1.bn3.running_mean, backbone.body.layer1.1.bn3.running_var, backbone.body.layer1.2.conv1.weight, backbone.body.layer1.2.bn1.weight, backbone.body.layer1.2.bn1.bias, backbone.body.layer1.2.bn1.running_mean, backbone.body.layer1.2.bn1.running_var, backbone.body.layer1.2.conv2.weight, backbone.body.layer1.2.bn2.weight, backbone.body.layer1.2.bn2.bias, backbone.body.layer1.2.bn2.running_mean, backbone.body.layer1.2.bn2.running_var, backbone.body.layer1.2.conv3.weight, backbone.body.layer1.2.bn3.weight, backbone.body.layer1.2.bn3.bias, backbone.body.layer1.2.bn3.running_mean, backbone.body.layer1.2.bn3.running_var, backbone.body.layer2.0.conv1.weight, backbone.body.layer2.0.bn1.weight, backbone.body.layer2.0.bn1.bias, backbone.body.layer2.0.bn1.running_mean, backbone.body.layer2.0.bn1.running_var, backbone.body.layer2.0.conv2.weight, backbone.body.layer2.0.bn2.weight, backbone.body.layer2.0.bn2.bias, backbone.body.layer2.0.bn2.running_mean, backbone.body.layer2.0.bn2.running_var, backbone.body.layer2.0.conv3.weight, backbone.body.layer2.0.bn3.weight, backbone.body.layer2.0.bn3.bias, backbone.body.layer2.0.bn3.running_mean, backbone.body.layer2.0.bn3.running_var, backbone.body.layer2.0.downsample.0.weight, backbone.body.layer2.0.downsample.1.weight, backbone.body.layer2.0.downsample.1.bias, backbone.body.layer2.0.downsample.1.running_mean, backbone.body.layer2.0.downsample.1.running_var, backbone.body.layer2.1.conv1.weight, backbone.body.layer2.1.bn1.weight, backbone.body.layer2.1.bn1.bias, backbone.body.layer2.1.bn1.running_mean, backbone.body.layer2.1.bn1.running_var, backbone.body.layer2.1.conv2.weight, backbone.body.layer2.1.bn2.weight, backbone.body.layer2.1.bn2.bias, backbone.body.layer2.1.bn2.running_mean, backbone.body.layer2.1.bn2.running_var, backbone.body.layer2.1.conv3.weight, backbone.body.layer2.1.bn3.weight, backbone.body.layer2.1.bn3.bias, backbone.body.layer2.1.bn3.running_mean, backbone.body.layer2.1.bn3.running_var, backbone.body.layer2.2.conv1.weight, backbone.body.layer2.2.bn1.weight, backbone.body.layer2.2.bn1.bias, backbone.body.layer2.2.bn1.running_mean, backbone.body.layer2.2.bn1.running_var, backbone.body.layer2.2.conv2.weight, backbone.body.layer2.2.bn2.weight, backbone.body.layer2.2.bn2.bias, backbone.body.layer2.2.bn2.running_mean, backbone.body.layer2.2.bn2.running_var, backbone.body.layer2.2.conv3.weight, backbone.body.layer2.2.bn3.weight, backbone.body.layer2.2.bn3.bias, backbone.body.layer2.2.bn3.running_mean, backbone.body.layer2.2.bn3.running_var, backbone.body.layer2.3.conv1.weight, backbone.body.layer2.3.bn1.weight, backbone.body.layer2.3.bn1.bias, backbone.body.layer2.3.bn1.running_mean, backbone.body.layer2.3.bn1.running_var, backbone.body.layer2.3.conv2.weight, backbone.body.layer2.3.bn2.weight, backbone.body.layer2.3.bn2.bias, backbone.body.layer2.3.bn2.running_mean, backbone.body.layer2.3.bn2.running_var, backbone.body.layer2.3.conv3.weight, backbone.body.layer2.3.bn3.weight, backbone.body.layer2.3.bn3.bias, backbone.body.layer2.3.bn3.running_mean, backbone.body.layer2.3.bn3.running_var, backbone.body.layer3.0.conv1.weight, backbone.body.layer3.0.bn1.weight, backbone.body.layer3.0.bn1.bias, backbone.body.layer3.0.bn1.running_mean, backbone.body.layer3.0.bn1.running_var, backbone.body.layer3.0.conv2.weight, backbone.body.layer3.0.bn2.weight, backbone.body.layer3.0.bn2.bias, backbone.body.layer3.0.bn2.running_mean, backbone.body.layer3.0.bn2.running_var, backbone.body.layer3.0.conv3.weight, backbone.body.layer3.0.bn3.weight, backbone.body.layer3.0.bn3.bias, backbone.body.layer3.0.bn3.running_mean, backbone.body.layer3.0.bn3.running_var, backbone.body.layer3.0.downsample.0.weight, backbone.body.layer3.0.downsample.1.weight, backbone.body.layer3.0.downsample.1.bias, backbone.body.layer3.0.downsample.1.running_mean, backbone.body.layer3.0.downsample.1.running_var, backbone.body.layer3.1.conv1.weight, backbone.body.layer3.1.bn1.weight, backbone.body.layer3.1.bn1.bias, backbone.body.layer3.1.bn1.running_mean, backbone.body.layer3.1.bn1.running_var, backbone.body.layer3.1.conv2.weight, backbone.body.layer3.1.bn2.weight, backbone.body.layer3.1.bn2.bias, backbone.body.layer3.1.bn2.running_mean, backbone.body.layer3.1.bn2.running_var, backbone.body.layer3.1.conv3.weight, backbone.body.layer3.1.bn3.weight, backbone.body.layer3.1.bn3.bias, backbone.body.layer3.1.bn3.running_mean, backbone.body.layer3.1.bn3.running_var, backbone.body.layer3.2.conv1.weight, backbone.body.layer3.2.bn1.weight, backbone.body.layer3.2.bn1.bias, backbone.body.layer3.2.bn1.running_mean, backbone.body.layer3.2.bn1.running_var, backbone.body.layer3.2.conv2.weight, backbone.body.layer3.2.bn2.weight, backbone.body.layer3.2.bn2.bias, backbone.body.layer3.2.bn2.running_mean, backbone.body.layer3.2.bn2.running_var, backbone.body.layer3.2.conv3.weight, backbone.body.layer3.2.bn3.weight, backbone.body.layer3.2.bn3.bias, backbone.body.layer3.2.bn3.running_mean, backbone.body.layer3.2.bn3.running_var, backbone.body.layer3.3.conv1.weight, backbone.body.layer3.3.bn1.weight, backbone.body.layer3.3.bn1.bias, backbone.body.layer3.3.bn1.running_mean, backbone.body.layer3.3.bn1.running_var, backbone.body.layer3.3.conv2.weight, backbone.body.layer3.3.bn2.weight, backbone.body.layer3.3.bn2.bias, backbone.body.layer3.3.bn2.running_mean, backbone.body.layer3.3.bn2.running_var, backbone.body.layer3.3.conv3.weight, backbone.body.layer3.3.bn3.weight, backbone.body.layer3.3.bn3.bias, backbone.body.layer3.3.bn3.running_mean, backbone.body.layer3.3.bn3.running_var, backbone.body.layer3.4.conv1.weight, backbone.body.layer3.4.bn1.weight, backbone.body.layer3.4.bn1.bias, backbone.body.layer3.4.bn1.running_mean, backbone.body.layer3.4.bn1.running_var, backbone.body.layer3.4.conv2.weight, backbone.body.layer3.4.bn2.weight, backbone.body.layer3.4.bn2.bias, backbone.body.layer3.4.bn2.running_mean, backbone.body.layer3.4.bn2.running_var, backbone.body.layer3.4.conv3.weight, backbone.body.layer3.4.bn3.weight, backbone.body.layer3.4.bn3.bias, backbone.body.layer3.4.bn3.running_mean, backbone.body.layer3.4.bn3.running_var, backbone.body.layer3.5.conv1.weight, backbone.body.layer3.5.bn1.weight, backbone.body.layer3.5.bn1.bias, backbone.body.layer3.5.bn1.running_mean, backbone.body.layer3.5.bn1.running_var, backbone.body.layer3.5.conv2.weight, backbone.body.layer3.5.bn2.weight, backbone.body.layer3.5.bn2.bias, backbone.body.layer3.5.bn2.running_mean, backbone.body.layer3.5.bn2.running_var, backbone.body.layer3.5.conv3.weight, backbone.body.layer3.5.bn3.weight, backbone.body.layer3.5.bn3.bias, backbone.body.layer3.5.bn3.running_mean, backbone.body.layer3.5.bn3.running_var, backbone.body.layer4.0.conv1.weight, backbone.body.layer4.0.bn1.weight, backbone.body.layer4.0.bn1.bias, backbone.body.layer4.0.bn1.running_mean, backbone.body.layer4.0.bn1.running_var, backbone.body.layer4.0.conv2.weight, backbone.body.layer4.0.bn2.weight, backbone.body.layer4.0.bn2.bias, backbone.body.layer4.0.bn2.running_mean, backbone.body.layer4.0.bn2.running_var, backbone.body.layer4.0.conv3.weight, backbone.body.layer4.0.bn3.weight, backbone.body.layer4.0.bn3.bias, backbone.body.layer4.0.bn3.running_mean, backbone.body.layer4.0.bn3.running_var, backbone.body.layer4.0.downsample.0.weight, backbone.body.layer4.0.downsample.1.weight, backbone.body.layer4.0.downsample.1.bias, backbone.body.layer4.0.downsample.1.running_mean, backbone.body.layer4.0.downsample.1.running_var, backbone.body.layer4.1.conv1.weight, backbone.body.layer4.1.bn1.weight, backbone.body.layer4.1.bn1.bias, backbone.body.layer4.1.bn1.running_mean, backbone.body.layer4.1.bn1.running_var, backbone.body.layer4.1.conv2.weight, backbone.body.layer4.1.bn2.weight, backbone.body.layer4.1.bn2.bias, backbone.body.layer4.1.bn2.running_mean, backbone.body.layer4.1.bn2.running_var, backbone.body.layer4.1.conv3.weight, backbone.body.layer4.1.bn3.weight, backbone.body.layer4.1.bn3.bias, backbone.body.layer4.1.bn3.running_mean, backbone.body.layer4.1.bn3.running_var, backbone.body.layer4.2.conv1.weight, backbone.body.layer4.2.bn1.weight, backbone.body.layer4.2.bn1.bias, backbone.body.layer4.2.bn1.running_mean, backbone.body.layer4.2.bn1.running_var, backbone.body.layer4.2.conv2.weight, backbone.body.layer4.2.bn2.weight, backbone.body.layer4.2.bn2.bias, backbone.body.layer4.2.bn2.running_mean, backbone.body.layer4.2.bn2.running_var, backbone.body.layer4.2.conv3.weight, backbone.body.layer4.2.bn3.weight, backbone.body.layer4.2.bn3.bias, backbone.body.layer4.2.bn3.running_mean, backbone.body.layer4.2.bn3.running_var, backbone.fpn.inner_blocks.0.weight, backbone.fpn.inner_blocks.0.bias, backbone.fpn.inner_blocks.1.weight, backbone.fpn.inner_blocks.1.bias, backbone.fpn.inner_blocks.2.weight, backbone.fpn.inner_blocks.2.bias, backbone.fpn.inner_blocks.3.weight, backbone.fpn.inner_blocks.3.bias, backbone.fpn.layer_blocks.0.weight, backbone.fpn.layer_blocks.0.bias, backbone.fpn.layer_blocks.1.weight, backbone.fpn.layer_blocks.1.bias, backbone.fpn.layer_blocks.2.weight, backbone.fpn.layer_blocks.2.bias, backbone.fpn.layer_blocks.3.weight, backbone.fpn.layer_blocks.3.bias

missing keys in source state_dict: backbone.conv1.weight, backbone.bn1.weight, backbone.bn1.bias, backbone.bn1.running_mean, backbone.bn1.running_var, backbone.layer1.0.conv1.weight, backbone.layer1.0.bn1.weight, backbone.layer1.0.bn1.bias, backbone.layer1.0.bn1.running_mean, backbone.layer1.0.bn1.running_var, backbone.layer1.0.conv2.weight, backbone.layer1.0.bn2.weight, backbone.layer1.0.bn2.bias, backbone.layer1.0.bn2.running_mean, backbone.layer1.0.bn2.running_var, backbone.layer1.0.conv3.weight, backbone.layer1.0.bn3.weight, backbone.layer1.0.bn3.bias, backbone.layer1.0.bn3.running_mean, backbone.layer1.0.bn3.running_var, backbone.layer1.0.downsample.0.weight, backbone.layer1.0.downsample.1.weight, backbone.layer1.0.downsample.1.bias, backbone.layer1.0.downsample.1.running_mean, backbone.layer1.0.downsample.1.running_var, backbone.layer1.1.conv1.weight, backbone.layer1.1.bn1.weight, backbone.layer1.1.bn1.bias, backbone.layer1.1.bn1.running_mean, backbone.layer1.1.bn1.running_var, backbone.layer1.1.conv2.weight, backbone.layer1.1.bn2.weight, backbone.layer1.1.bn2.bias, backbone.layer1.1.bn2.running_mean, backbone.layer1.1.bn2.running_var, backbone.layer1.1.conv3.weight, backbone.layer1.1.bn3.weight, backbone.layer1.1.bn3.bias, backbone.layer1.1.bn3.running_mean, backbone.layer1.1.bn3.running_var, backbone.layer1.2.conv1.weight, backbone.layer1.2.bn1.weight, backbone.layer1.2.bn1.bias, backbone.layer1.2.bn1.running_mean, backbone.layer1.2.bn1.running_var, backbone.layer1.2.conv2.weight, backbone.layer1.2.bn2.weight, backbone.layer1.2.bn2.bias, backbone.layer1.2.bn2.running_mean, backbone.layer1.2.bn2.running_var, backbone.layer1.2.conv3.weight, backbone.layer1.2.bn3.weight, backbone.layer1.2.bn3.bias, backbone.layer1.2.bn3.running_mean, backbone.layer1.2.bn3.running_var, backbone.layer2.0.conv1.weight, backbone.layer2.0.bn1.weight, backbone.layer2.0.bn1.bias, backbone.layer2.0.bn1.running_mean, backbone.layer2.0.bn1.running_var, backbone.layer2.0.conv2.weight, backbone.layer2.0.bn2.weight, backbone.layer2.0.bn2.bias, backbone.layer2.0.bn2.running_mean, backbone.layer2.0.bn2.running_var, backbone.layer2.0.conv3.weight, backbone.layer2.0.bn3.weight, backbone.layer2.0.bn3.bias, backbone.layer2.0.bn3.running_mean, backbone.layer2.0.bn3.running_var, backbone.layer2.0.downsample.0.weight, backbone.layer2.0.downsample.1.weight, backbone.layer2.0.downsample.1.bias, backbone.layer2.0.downsample.1.running_mean, backbone.layer2.0.downsample.1.running_var, backbone.layer2.1.conv1.weight, backbone.layer2.1.bn1.weight, backbone.layer2.1.bn1.bias, backbone.layer2.1.bn1.running_mean, backbone.layer2.1.bn1.running_var, backbone.layer2.1.conv2.weight, backbone.layer2.1.bn2.weight, backbone.layer2.1.bn2.bias, backbone.layer2.1.bn2.running_mean, backbone.layer2.1.bn2.running_var, backbone.layer2.1.conv3.weight, backbone.layer2.1.bn3.weight, backbone.layer2.1.bn3.bias, backbone.layer2.1.bn3.running_mean, backbone.layer2.1.bn3.running_var, backbone.layer2.2.conv1.weight, backbone.layer2.2.bn1.weight, backbone.layer2.2.bn1.bias, backbone.layer2.2.bn1.running_mean, backbone.layer2.2.bn1.running_var, backbone.layer2.2.conv2.weight, backbone.layer2.2.bn2.weight, backbone.layer2.2.bn2.bias, backbone.layer2.2.bn2.running_mean, backbone.layer2.2.bn2.running_var, backbone.layer2.2.conv3.weight, backbone.layer2.2.bn3.weight, backbone.layer2.2.bn3.bias, backbone.layer2.2.bn3.running_mean, backbone.layer2.2.bn3.running_var, backbone.layer2.3.conv1.weight, backbone.layer2.3.bn1.weight, backbone.layer2.3.bn1.bias, backbone.layer2.3.bn1.running_mean, backbone.layer2.3.bn1.running_var, backbone.layer2.3.conv2.weight, backbone.layer2.3.bn2.weight, backbone.layer2.3.bn2.bias, backbone.layer2.3.bn2.running_mean, backbone.layer2.3.bn2.running_var, backbone.layer2.3.conv3.weight, backbone.layer2.3.bn3.weight, backbone.layer2.3.bn3.bias, backbone.layer2.3.bn3.running_mean, backbone.layer2.3.bn3.running_var, backbone.layer3.0.conv1.weight, backbone.layer3.0.bn1.weight, backbone.layer3.0.bn1.bias, backbone.layer3.0.bn1.running_mean, backbone.layer3.0.bn1.running_var, backbone.layer3.0.conv2.weight, backbone.layer3.0.bn2.weight, backbone.layer3.0.bn2.bias, backbone.layer3.0.bn2.running_mean, backbone.layer3.0.bn2.running_var, backbone.layer3.0.conv3.weight, backbone.layer3.0.bn3.weight, backbone.layer3.0.bn3.bias, backbone.layer3.0.bn3.running_mean, backbone.layer3.0.bn3.running_var, backbone.layer3.0.downsample.0.weight, backbone.layer3.0.downsample.1.weight, backbone.layer3.0.downsample.1.bias, backbone.layer3.0.downsample.1.running_mean, backbone.layer3.0.downsample.1.running_var, backbone.layer3.1.conv1.weight, backbone.layer3.1.bn1.weight, backbone.layer3.1.bn1.bias, backbone.layer3.1.bn1.running_mean, backbone.layer3.1.bn1.running_var, backbone.layer3.1.conv2.weight, backbone.layer3.1.bn2.weight, backbone.layer3.1.bn2.bias, backbone.layer3.1.bn2.running_mean, backbone.layer3.1.bn2.running_var, backbone.layer3.1.conv3.weight, backbone.layer3.1.bn3.weight, backbone.layer3.1.bn3.bias, backbone.layer3.1.bn3.running_mean, backbone.layer3.1.bn3.running_var, backbone.layer3.2.conv1.weight, backbone.layer3.2.bn1.weight, backbone.layer3.2.bn1.bias, backbone.layer3.2.bn1.running_mean, backbone.layer3.2.bn1.running_var, backbone.layer3.2.conv2.weight, backbone.layer3.2.bn2.weight, backbone.layer3.2.bn2.bias, backbone.layer3.2.bn2.running_mean, backbone.layer3.2.bn2.running_var, backbone.layer3.2.conv3.weight, backbone.layer3.2.bn3.weight, backbone.layer3.2.bn3.bias, backbone.layer3.2.bn3.running_mean, backbone.layer3.2.bn3.running_var, backbone.layer3.3.conv1.weight, backbone.layer3.3.bn1.weight, backbone.layer3.3.bn1.bias, backbone.layer3.3.bn1.running_mean, backbone.layer3.3.bn1.running_var, backbone.layer3.3.conv2.weight, backbone.layer3.3.bn2.weight, backbone.layer3.3.bn2.bias, backbone.layer3.3.bn2.running_mean, backbone.layer3.3.bn2.running_var, backbone.layer3.3.conv3.weight, backbone.layer3.3.bn3.weight, backbone.layer3.3.bn3.bias, backbone.layer3.3.bn3.running_mean, backbone.layer3.3.bn3.running_var, backbone.layer3.4.conv1.weight, backbone.layer3.4.bn1.weight, backbone.layer3.4.bn1.bias, backbone.layer3.4.bn1.running_mean, backbone.layer3.4.bn1.running_var, backbone.layer3.4.conv2.weight, backbone.layer3.4.bn2.weight, backbone.layer3.4.bn2.bias, backbone.layer3.4.bn2.running_mean, backbone.layer3.4.bn2.running_var, backbone.layer3.4.conv3.weight, backbone.layer3.4.bn3.weight, backbone.layer3.4.bn3.bias, backbone.layer3.4.bn3.running_mean, backbone.layer3.4.bn3.running_var, backbone.layer3.5.conv1.weight, backbone.layer3.5.bn1.weight, backbone.layer3.5.bn1.bias, backbone.layer3.5.bn1.running_mean, backbone.layer3.5.bn1.running_var, backbone.layer3.5.conv2.weight, backbone.layer3.5.bn2.weight, backbone.layer3.5.bn2.bias, backbone.layer3.5.bn2.running_mean, backbone.layer3.5.bn2.running_var, backbone.layer3.5.conv3.weight, backbone.layer3.5.bn3.weight, backbone.layer3.5.bn3.bias, backbone.layer3.5.bn3.running_mean, backbone.layer3.5.bn3.running_var, backbone.layer4.0.conv1.weight, backbone.layer4.0.bn1.weight, backbone.layer4.0.bn1.bias, backbone.layer4.0.bn1.running_mean, backbone.layer4.0.bn1.running_var, backbone.layer4.0.conv2.weight, backbone.layer4.0.bn2.weight, backbone.layer4.0.bn2.bias, backbone.layer4.0.bn2.running_mean, backbone.layer4.0.bn2.running_var, backbone.layer4.0.conv3.weight, backbone.layer4.0.bn3.weight, backbone.layer4.0.bn3.bias, backbone.layer4.0.bn3.running_mean, backbone.layer4.0.bn3.running_var, backbone.layer4.0.downsample.0.weight, backbone.layer4.0.downsample.1.weight, backbone.layer4.0.downsample.1.bias, backbone.layer4.0.downsample.1.running_mean, backbone.layer4.0.downsample.1.running_var, backbone.layer4.1.conv1.weight, backbone.layer4.1.bn1.weight, backbone.layer4.1.bn1.bias, backbone.layer4.1.bn1.running_mean, backbone.layer4.1.bn1.running_var, backbone.layer4.1.conv2.weight, backbone.layer4.1.bn2.weight, backbone.layer4.1.bn2.bias, backbone.layer4.1.bn2.running_mean, backbone.layer4.1.bn2.running_var, backbone.layer4.1.conv3.weight, backbone.layer4.1.bn3.weight, backbone.layer4.1.bn3.bias, backbone.layer4.1.bn3.running_mean, backbone.layer4.1.bn3.running_var, backbone.layer4.2.conv1.weight, backbone.layer4.2.bn1.weight, backbone.layer4.2.bn1.bias, backbone.layer4.2.bn1.running_mean, backbone.layer4.2.bn1.running_var, backbone.layer4.2.conv2.weight, backbone.layer4.2.bn2.weight, backbone.layer4.2.bn2.bias, backbone.layer4.2.bn2.running_mean, backbone.layer4.2.bn2.running_var, backbone.layer4.2.conv3.weight, backbone.layer4.2.bn3.weight, backbone.layer4.2.bn3.bias, backbone.layer4.2.bn3.running_mean, backbone.layer4.2.bn3.running_var, bbox_head.relation_decoder.layers.0.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.0.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.0.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.0.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.0.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.0.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.0.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.0.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.0.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.0.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.0.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.0.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.0.norms.0.weight, bbox_head.relation_decoder.layers.0.norms.0.bias, bbox_head.relation_decoder.layers.0.norms.1.weight, bbox_head.relation_decoder.layers.0.norms.1.bias, bbox_head.relation_decoder.layers.0.norms.2.weight, bbox_head.relation_decoder.layers.0.norms.2.bias, bbox_head.relation_decoder.layers.1.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.1.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.1.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.1.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.1.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.1.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.1.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.1.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.1.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.1.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.1.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.1.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.1.norms.0.weight, bbox_head.relation_decoder.layers.1.norms.0.bias, bbox_head.relation_decoder.layers.1.norms.1.weight, bbox_head.relation_decoder.layers.1.norms.1.bias, bbox_head.relation_decoder.layers.1.norms.2.weight, bbox_head.relation_decoder.layers.1.norms.2.bias, bbox_head.relation_decoder.layers.2.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.2.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.2.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.2.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.2.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.2.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.2.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.2.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.2.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.2.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.2.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.2.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.2.norms.0.weight, bbox_head.relation_decoder.layers.2.norms.0.bias, bbox_head.relation_decoder.layers.2.norms.1.weight, bbox_head.relation_decoder.layers.2.norms.1.bias, bbox_head.relation_decoder.layers.2.norms.2.weight, bbox_head.relation_decoder.layers.2.norms.2.bias, bbox_head.relation_decoder.layers.3.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.3.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.3.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.3.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.3.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.3.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.3.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.3.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.3.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.3.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.3.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.3.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.3.norms.0.weight, bbox_head.relation_decoder.layers.3.norms.0.bias, bbox_head.relation_decoder.layers.3.norms.1.weight, bbox_head.relation_decoder.layers.3.norms.1.bias, bbox_head.relation_decoder.layers.3.norms.2.weight, bbox_head.relation_decoder.layers.3.norms.2.bias, bbox_head.relation_decoder.layers.4.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.4.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.4.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.4.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.4.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.4.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.4.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.4.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.4.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.4.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.4.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.4.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.4.norms.0.weight, bbox_head.relation_decoder.layers.4.norms.0.bias, bbox_head.relation_decoder.layers.4.norms.1.weight, bbox_head.relation_decoder.layers.4.norms.1.bias, bbox_head.relation_decoder.layers.4.norms.2.weight, bbox_head.relation_decoder.layers.4.norms.2.bias, bbox_head.relation_decoder.layers.5.attentions.0.attn.in_proj_weight, bbox_head.relation_decoder.layers.5.attentions.0.attn.in_proj_bias, bbox_head.relation_decoder.layers.5.attentions.0.attn.out_proj.weight, bbox_head.relation_decoder.layers.5.attentions.0.attn.out_proj.bias, bbox_head.relation_decoder.layers.5.attentions.1.attn.in_proj_weight, bbox_head.relation_decoder.layers.5.attentions.1.attn.in_proj_bias, bbox_head.relation_decoder.layers.5.attentions.1.attn.out_proj.weight, bbox_head.relation_decoder.layers.5.attentions.1.attn.out_proj.bias, bbox_head.relation_decoder.layers.5.ffns.0.layers.0.0.weight, bbox_head.relation_decoder.layers.5.ffns.0.layers.0.0.bias, bbox_head.relation_decoder.layers.5.ffns.0.layers.1.weight, bbox_head.relation_decoder.layers.5.ffns.0.layers.1.bias, bbox_head.relation_decoder.layers.5.norms.0.weight, bbox_head.relation_decoder.layers.5.norms.0.bias, bbox_head.relation_decoder.layers.5.norms.1.weight, bbox_head.relation_decoder.layers.5.norms.1.bias, bbox_head.relation_decoder.layers.5.norms.2.weight, bbox_head.relation_decoder.layers.5.norms.2.bias, bbox_head.relation_decoder.post_norm.weight, bbox_head.relation_decoder.post_norm.bias, bbox_head.rel_query_embed.weight, bbox_head.rel_query_embed2.weight, bbox_head.rel_query_embed3.weight, bbox_head.rel_query_feat.weight, bbox_head.update_importance.conv_layers.0.0.weight, bbox_head.update_importance.conv_layers.0.0.bias, bbox_head.update_importance.conv_layers.1.0.weight, bbox_head.update_importance.conv_layers.1.0.bias, bbox_head.update_importance.conv_layers.2.0.weight, bbox_head.update_importance.conv_layers.2.0.bias, bbox_head.pixel_decoder.input_convs.0.conv.weight, bbox_head.pixel_decoder.input_convs.0.conv.bias, bbox_head.pixel_decoder.input_convs.0.gn.weight, bbox_head.pixel_decoder.input_convs.0.gn.bias, bbox_head.pixel_decoder.input_convs.1.conv.weight, bbox_head.pixel_decoder.input_convs.1.conv.bias, bbox_head.pixel_decoder.input_convs.1.gn.weight, bbox_head.pixel_decoder.input_convs.1.gn.bias, bbox_head.pixel_decoder.input_convs.2.conv.weight, bbox_head.pixel_decoder.input_convs.2.conv.bias, bbox_head.pixel_decoder.input_convs.2.gn.weight, bbox_head.pixel_decoder.input_convs.2.gn.bias, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.0.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.0.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.0.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.0.norms.1.bias, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.1.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.1.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.1.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.1.norms.1.bias, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.2.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.2.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.2.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.2.norms.1.bias, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.3.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.3.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.3.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.3.norms.1.bias, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.4.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.4.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.4.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.4.norms.1.bias, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight, bbox_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias, bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight, bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias, bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight, bbox_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias, bbox_head.pixel_decoder.encoder.layers.5.norms.0.weight, bbox_head.pixel_decoder.encoder.layers.5.norms.0.bias, bbox_head.pixel_decoder.encoder.layers.5.norms.1.weight, bbox_head.pixel_decoder.encoder.layers.5.norms.1.bias, bbox_head.pixel_decoder.level_encoding.weight, bbox_head.pixel_decoder.lateral_convs.0.conv.weight, bbox_head.pixel_decoder.lateral_convs.0.gn.weight, bbox_head.pixel_decoder.lateral_convs.0.gn.bias, bbox_head.pixel_decoder.output_convs.0.conv.weight, bbox_head.pixel_decoder.output_convs.0.gn.weight, bbox_head.pixel_decoder.output_convs.0.gn.bias, bbox_head.pixel_decoder.mask_feature.weight, bbox_head.pixel_decoder.mask_feature.bias, bbox_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.0.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.0.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.0.norms.0.weight, bbox_head.transformer_decoder.layers.0.norms.0.bias, bbox_head.transformer_decoder.layers.0.norms.1.weight, bbox_head.transformer_decoder.layers.0.norms.1.bias, bbox_head.transformer_decoder.layers.0.norms.2.weight, bbox_head.transformer_decoder.layers.0.norms.2.bias, bbox_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.1.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.1.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.1.norms.0.weight, bbox_head.transformer_decoder.layers.1.norms.0.bias, bbox_head.transformer_decoder.layers.1.norms.1.weight, bbox_head.transformer_decoder.layers.1.norms.1.bias, bbox_head.transformer_decoder.layers.1.norms.2.weight, bbox_head.transformer_decoder.layers.1.norms.2.bias, bbox_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.2.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.2.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.2.norms.0.weight, bbox_head.transformer_decoder.layers.2.norms.0.bias, bbox_head.transformer_decoder.layers.2.norms.1.weight, bbox_head.transformer_decoder.layers.2.norms.1.bias, bbox_head.transformer_decoder.layers.2.norms.2.weight, bbox_head.transformer_decoder.layers.2.norms.2.bias, bbox_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.3.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.3.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.3.norms.0.weight, bbox_head.transformer_decoder.layers.3.norms.0.bias, bbox_head.transformer_decoder.layers.3.norms.1.weight, bbox_head.transformer_decoder.layers.3.norms.1.bias, bbox_head.transformer_decoder.layers.3.norms.2.weight, bbox_head.transformer_decoder.layers.3.norms.2.bias, bbox_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.4.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.4.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.4.norms.0.weight, bbox_head.transformer_decoder.layers.4.norms.0.bias, bbox_head.transformer_decoder.layers.4.norms.1.weight, bbox_head.transformer_decoder.layers.4.norms.1.bias, bbox_head.transformer_decoder.layers.4.norms.2.weight, bbox_head.transformer_decoder.layers.4.norms.2.bias, bbox_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.5.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.5.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.5.norms.0.weight, bbox_head.transformer_decoder.layers.5.norms.0.bias, bbox_head.transformer_decoder.layers.5.norms.1.weight, bbox_head.transformer_decoder.layers.5.norms.1.bias, bbox_head.transformer_decoder.layers.5.norms.2.weight, bbox_head.transformer_decoder.layers.5.norms.2.bias, bbox_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.6.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.6.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.6.norms.0.weight, bbox_head.transformer_decoder.layers.6.norms.0.bias, bbox_head.transformer_decoder.layers.6.norms.1.weight, bbox_head.transformer_decoder.layers.6.norms.1.bias, bbox_head.transformer_decoder.layers.6.norms.2.weight, bbox_head.transformer_decoder.layers.6.norms.2.bias, bbox_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.7.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.7.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.7.norms.0.weight, bbox_head.transformer_decoder.layers.7.norms.0.bias, bbox_head.transformer_decoder.layers.7.norms.1.weight, bbox_head.transformer_decoder.layers.7.norms.1.bias, bbox_head.transformer_decoder.layers.7.norms.2.weight, bbox_head.transformer_decoder.layers.7.norms.2.bias, bbox_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight, bbox_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias, bbox_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight, bbox_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias, bbox_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight, bbox_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias, bbox_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight, bbox_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias, bbox_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight, bbox_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias, bbox_head.transformer_decoder.layers.8.ffns.0.layers.1.weight, bbox_head.transformer_decoder.layers.8.ffns.0.layers.1.bias, bbox_head.transformer_decoder.layers.8.norms.0.weight, bbox_head.transformer_decoder.layers.8.norms.0.bias, bbox_head.transformer_decoder.layers.8.norms.1.weight, bbox_head.transformer_decoder.layers.8.norms.1.bias, bbox_head.transformer_decoder.layers.8.norms.2.weight, bbox_head.transformer_decoder.layers.8.norms.2.bias, bbox_head.transformer_decoder.post_norm.weight, bbox_head.transformer_decoder.post_norm.bias, bbox_head.query_embed.weight, bbox_head.query_feat.weight, bbox_head.level_embed.weight, bbox_head.cls_embed.weight, bbox_head.cls_embed.bias, bbox_head.mask_embed.0.weight, bbox_head.mask_embed.0.bias, bbox_head.mask_embed.2.weight, bbox_head.mask_embed.2.bias, bbox_head.mask_embed.4.weight, bbox_head.mask_embed.4.bias, bbox_head.rel_cls_loss.cum_samples, bbox_head.sub_query_update.0.weight, bbox_head.sub_query_update.0.bias, bbox_head.sub_query_update.2.weight, bbox_head.sub_query_update.2.bias, bbox_head.sub_query_update.4.weight, bbox_head.sub_query_update.4.bias, bbox_head.obj_query_update.0.weight, bbox_head.obj_query_update.0.bias, bbox_head.obj_query_update.2.weight, bbox_head.obj_query_update.2.bias, bbox_head.obj_query_update.4.weight, bbox_head.obj_query_update.4.bias, bbox_head.rel_cls_embed.weight, bbox_head.rel_cls_embed.bias

2025-06-22 22:30:39,613 - mmdet - INFO - Start running, host: stormai@stormai-3090x3, work_dir: /home/stormai/userfile2/yihj/Pair-Net-main/work_dirs/3090_wjh
2025-06-22 22:30:39,613 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2025-06-22 22:30:39,613 - mmdet - INFO - workflow: [('train', 1), ('val', 1)], max: 15 epochs
2025-06-22 22:30:39,614 - mmdet - INFO - Checkpoints will be saved to /home/stormai/userfile2/yihj/Pair-Net-main/work_dirs/3090_wjh by HardDiskBackend.
2025-06-22 22:31:00,887 - mmdet - INFO - Epoch [1][50/45697]	lr: 1.250e-05, eta: 3 days, 8:58:36, time: 0.425, data_time: 0.060, memory: 3000, loss_r_cls: 7.0154, loss_sub_cls: 20.2517, loss_obj_cls: 19.9309, loss_match: 6.9311, loss: 54.1291, grad_norm: 53.3473
2025-06-22 22:31:15,372 - mmdet - INFO - Epoch [1][100/45697]	lr: 1.250e-05, eta: 2 days, 20:03:40, time: 0.290, data_time: 0.014, memory: 3000, loss_r_cls: 6.0377, loss_sub_cls: 20.6446, loss_obj_cls: 20.2052, loss_match: 6.9293, loss: 53.8168, grad_norm: 42.3531
2025-06-22 22:31:29,995 - mmdet - INFO - Epoch [1][150/45697]	lr: 1.250e-05, eta: 2 days, 15:55:34, time: 0.292, data_time: 0.013, memory: 3000, loss_r_cls: 5.7606, loss_sub_cls: 20.7134, loss_obj_cls: 19.8461, loss_match: 6.9298, loss: 53.2499, grad_norm: 40.6789
2025-06-22 22:31:44,652 - mmdet - INFO - Epoch [1][200/45697]	lr: 1.250e-05, eta: 2 days, 13:53:24, time: 0.293, data_time: 0.014, memory: 3000, loss_r_cls: 5.8109, loss_sub_cls: 20.7696, loss_obj_cls: 19.8992, loss_match: 6.9300, loss: 53.4097, grad_norm: 38.4816
2025-06-22 22:31:59,301 - mmdet - INFO - Epoch [1][250/45697]	lr: 1.250e-05, eta: 2 days, 12:39:36, time: 0.293, data_time: 0.015, memory: 3000, loss_r_cls: 5.2770, loss_sub_cls: 20.8091, loss_obj_cls: 19.9988, loss_match: 6.9273, loss: 53.0121, grad_norm: 34.0044
2025-06-22 22:32:13,961 - mmdet - INFO - Epoch [1][300/45697]	lr: 1.250e-05, eta: 2 days, 11:50:46, time: 0.293, data_time: 0.014, memory: 3000, loss_r_cls: 5.6803, loss_sub_cls: 20.9374, loss_obj_cls: 19.7608, loss_match: 6.9316, loss: 53.3100, grad_norm: 33.7529
2025-06-22 22:32:28,366 - mmdet - INFO - Epoch [1][350/45697]	lr: 1.250e-05, eta: 2 days, 11:07:31, time: 0.288, data_time: 0.012, memory: 3000, loss_r_cls: 5.3842, loss_sub_cls: 20.4618, loss_obj_cls: 20.1501, loss_match: 6.9272, loss: 52.9233, grad_norm: 38.7293
2025-06-22 22:32:43,017 - mmdet - INFO - Epoch [1][400/45697]	lr: 1.250e-05, eta: 2 days, 10:42:01, time: 0.293, data_time: 0.014, memory: 3000, loss_r_cls: 5.2764, loss_sub_cls: 20.9220, loss_obj_cls: 19.8675, loss_match: 6.9297, loss: 52.9955, grad_norm: 33.7686
2025-06-22 22:32:57,587 - mmdet - INFO - Epoch [1][450/45697]	lr: 1.250e-05, eta: 2 days, 10:20:04, time: 0.291, data_time: 0.014, memory: 3000, loss_r_cls: 5.0772, loss_sub_cls: 20.9192, loss_obj_cls: 19.9178, loss_match: 6.9302, loss: 52.8445, grad_norm: 31.1397
2025-06-22 22:33:12,020 - mmdet - INFO - Epoch [1][500/45697]	lr: 1.250e-05, eta: 2 days, 9:59:21, time: 0.289, data_time: 0.013, memory: 3000, loss_r_cls: 5.0877, loss_sub_cls: 20.4564, loss_obj_cls: 19.8591, loss_match: 6.9219, loss: 52.3251, grad_norm: 34.9572
2025-06-22 22:33:26,290 - mmdet - INFO - Epoch [1][550/45697]	lr: 1.250e-05, eta: 2 days, 9:38:58, time: 0.285, data_time: 0.013, memory: 3000, loss_r_cls: 5.2257, loss_sub_cls: 20.9420, loss_obj_cls: 19.8367, loss_match: 6.9124, loss: 52.9169, grad_norm: 34.7468
2025-06-22 22:33:40,759 - mmdet - INFO - Epoch [1][600/45697]	lr: 1.250e-05, eta: 2 days, 9:25:43, time: 0.289, data_time: 0.014, memory: 3000, loss_r_cls: 5.3466, loss_sub_cls: 20.7820, loss_obj_cls: 19.9562, loss_match: 6.9369, loss: 53.0217, grad_norm: 31.2172
2025-06-22 22:33:55,253 - mmdet - INFO - Epoch [1][650/45697]	lr: 1.250e-05, eta: 2 days, 9:14:56, time: 0.290, data_time: 0.014, memory: 3000, loss_r_cls: 5.1017, loss_sub_cls: 20.9458, loss_obj_cls: 20.1555, loss_match: 6.9159, loss: 53.1188, grad_norm: 31.3386
2025-06-22 22:34:09,726 - mmdet - INFO - Epoch [1][700/45697]	lr: 1.250e-05, eta: 2 days, 9:05:18, time: 0.289, data_time: 0.013, memory: 3000, loss_r_cls: 5.1921, loss_sub_cls: 20.9279, loss_obj_cls: 19.8521, loss_match: 6.9179, loss: 52.8899, grad_norm: 34.3579
2025-06-22 22:34:24,557 - mmdet - INFO - Epoch [1][750/45697]	lr: 1.250e-05, eta: 2 days, 9:02:22, time: 0.297, data_time: 0.015, memory: 3000, loss_r_cls: 5.4004, loss_sub_cls: 20.7321, loss_obj_cls: 20.0211, loss_match: 6.9081, loss: 53.0618, grad_norm: 34.9439
2025-06-22 22:34:39,278 - mmdet - INFO - Epoch [1][800/45697]	lr: 1.250e-05, eta: 2 days, 8:58:12, time: 0.294, data_time: 0.014, memory: 3002, loss_r_cls: 5.1182, loss_sub_cls: 21.2714, loss_obj_cls: 19.8622, loss_match: 6.8700, loss: 53.1219, grad_norm: 36.9826
2025-06-22 22:34:53,654 - mmdet - INFO - Epoch [1][850/45697]	lr: 1.250e-05, eta: 2 days, 8:49:53, time: 0.288, data_time: 0.012, memory: 3002, loss_r_cls: 5.1977, loss_sub_cls: 21.0335, loss_obj_cls: 19.9050, loss_match: 6.9194, loss: 53.0556, grad_norm: 39.3259
2025-06-22 22:35:08,351 - mmdet - INFO - Epoch [1][900/45697]	lr: 1.250e-05, eta: 2 days, 8:46:30, time: 0.294, data_time: 0.014, memory: 3002, loss_r_cls: 5.1543, loss_sub_cls: 20.6986, loss_obj_cls: 19.6787, loss_match: 6.8779, loss: 52.4095, grad_norm: 33.9202
2025-06-22 22:35:22,810 - mmdet - INFO - Epoch [1][950/45697]	lr: 1.250e-05, eta: 2 days, 8:40:36, time: 0.289, data_time: 0.013, memory: 3002, loss_r_cls: 5.1505, loss_sub_cls: 21.1032, loss_obj_cls: 19.9459, loss_match: 6.9416, loss: 53.1412, grad_norm: 35.3488
2025-06-22 22:35:37,090 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:35:37,090 - mmdet - INFO - Epoch [1][1000/45697]	lr: 1.250e-05, eta: 2 days, 8:33:13, time: 0.286, data_time: 0.012, memory: 3002, loss_r_cls: 5.0104, loss_sub_cls: 20.5234, loss_obj_cls: 19.9181, loss_match: 6.8732, loss: 52.3251, grad_norm: 36.0596
2025-06-22 22:35:51,618 - mmdet - INFO - Epoch [1][1050/45697]	lr: 1.250e-05, eta: 2 days, 8:29:14, time: 0.291, data_time: 0.013, memory: 3002, loss_r_cls: 5.1062, loss_sub_cls: 20.7865, loss_obj_cls: 20.0982, loss_match: 6.8850, loss: 52.8759, grad_norm: 41.0840
2025-06-22 22:36:06,285 - mmdet - INFO - Epoch [1][1100/45697]	lr: 1.250e-05, eta: 2 days, 8:27:01, time: 0.293, data_time: 0.014, memory: 3002, loss_r_cls: 5.0773, loss_sub_cls: 20.9237, loss_obj_cls: 19.7708, loss_match: 6.8848, loss: 52.6566, grad_norm: 40.6201
2025-06-22 22:36:20,990 - mmdet - INFO - Epoch [1][1150/45697]	lr: 1.250e-05, eta: 2 days, 8:25:21, time: 0.294, data_time: 0.014, memory: 3002, loss_r_cls: 5.2369, loss_sub_cls: 21.2221, loss_obj_cls: 20.1113, loss_match: 6.8779, loss: 53.4483, grad_norm: 38.0600
2025-06-22 22:36:35,491 - mmdet - INFO - Epoch [1][1200/45697]	lr: 1.250e-05, eta: 2 days, 8:21:52, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 5.0665, loss_sub_cls: 20.8235, loss_obj_cls: 19.8048, loss_match: 6.8964, loss: 52.5911, grad_norm: 38.4942
2025-06-22 22:36:50,013 - mmdet - INFO - Epoch [1][1250/45697]	lr: 1.250e-05, eta: 2 days, 8:18:49, time: 0.290, data_time: 0.014, memory: 3005, loss_r_cls: 5.3107, loss_sub_cls: 20.8762, loss_obj_cls: 19.5380, loss_match: 6.9090, loss: 52.6339, grad_norm: 38.6888
2025-06-22 22:37:04,651 - mmdet - INFO - Epoch [1][1300/45697]	lr: 1.250e-05, eta: 2 days, 8:17:01, time: 0.293, data_time: 0.017, memory: 3005, loss_r_cls: 5.6224, loss_sub_cls: 21.4673, loss_obj_cls: 19.8977, loss_match: 6.8829, loss: 53.8703, grad_norm: 45.1541
2025-06-22 22:37:19,269 - mmdet - INFO - Epoch [1][1350/45697]	lr: 1.250e-05, eta: 2 days, 8:15:10, time: 0.292, data_time: 0.013, memory: 3005, loss_r_cls: 4.8775, loss_sub_cls: 20.9955, loss_obj_cls: 19.9247, loss_match: 6.8926, loss: 52.6902, grad_norm: 37.0974
2025-06-22 22:37:33,617 - mmdet - INFO - Epoch [1][1400/45697]	lr: 1.250e-05, eta: 2 days, 8:11:13, time: 0.287, data_time: 0.012, memory: 3005, loss_r_cls: 4.9546, loss_sub_cls: 20.8074, loss_obj_cls: 19.8522, loss_match: 6.8868, loss: 52.5010, grad_norm: 44.9895
2025-06-22 22:37:48,178 - mmdet - INFO - Epoch [1][1450/45697]	lr: 1.250e-05, eta: 2 days, 8:09:12, time: 0.291, data_time: 0.013, memory: 3005, loss_r_cls: 4.9169, loss_sub_cls: 20.8825, loss_obj_cls: 19.8828, loss_match: 6.9099, loss: 52.5921, grad_norm: 41.5426
2025-06-22 22:38:02,539 - mmdet - INFO - Epoch [1][1500/45697]	lr: 1.250e-05, eta: 2 days, 8:05:48, time: 0.287, data_time: 0.014, memory: 3005, loss_r_cls: 5.2936, loss_sub_cls: 21.0353, loss_obj_cls: 19.6238, loss_match: 6.9267, loss: 52.8794, grad_norm: 45.5363
2025-06-22 22:38:16,788 - mmdet - INFO - Epoch [1][1550/45697]	lr: 1.250e-05, eta: 2 days, 8:01:46, time: 0.285, data_time: 0.014, memory: 3005, loss_r_cls: 4.9404, loss_sub_cls: 20.7162, loss_obj_cls: 19.7473, loss_match: 6.9287, loss: 52.3326, grad_norm: 40.2670
2025-06-22 22:38:31,197 - mmdet - INFO - Epoch [1][1600/45697]	lr: 1.250e-05, eta: 2 days, 7:59:07, time: 0.288, data_time: 0.014, memory: 3005, loss_r_cls: 4.9256, loss_sub_cls: 20.9840, loss_obj_cls: 20.0752, loss_match: 6.9262, loss: 52.9110, grad_norm: 40.8336
2025-06-22 22:38:45,705 - mmdet - INFO - Epoch [1][1650/45697]	lr: 1.250e-05, eta: 2 days, 7:57:17, time: 0.290, data_time: 0.014, memory: 3005, loss_r_cls: 5.0807, loss_sub_cls: 20.8799, loss_obj_cls: 19.7188, loss_match: 6.8463, loss: 52.5257, grad_norm: 42.9432
2025-06-22 22:39:00,182 - mmdet - INFO - Epoch [1][1700/45697]	lr: 1.250e-05, eta: 2 days, 7:55:21, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 5.1493, loss_sub_cls: 21.3456, loss_obj_cls: 20.0919, loss_match: 6.8783, loss: 53.4651, grad_norm: 40.5792
2025-06-22 22:39:14,559 - mmdet - INFO - Epoch [1][1750/45697]	lr: 1.250e-05, eta: 2 days, 7:52:51, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 5.1331, loss_sub_cls: 20.7110, loss_obj_cls: 20.0976, loss_match: 6.8739, loss: 52.8156, grad_norm: 40.0844
2025-06-22 22:39:28,904 - mmdet - INFO - Epoch [1][1800/45697]	lr: 1.250e-05, eta: 2 days, 7:50:17, time: 0.287, data_time: 0.013, memory: 3005, loss_r_cls: 4.9857, loss_sub_cls: 20.9805, loss_obj_cls: 20.1863, loss_match: 6.8893, loss: 53.0419, grad_norm: 35.6121
2025-06-22 22:39:43,249 - mmdet - INFO - Epoch [1][1850/45697]	lr: 1.250e-05, eta: 2 days, 7:47:50, time: 0.287, data_time: 0.013, memory: 3005, loss_r_cls: 5.4616, loss_sub_cls: 20.6200, loss_obj_cls: 19.7910, loss_match: 6.9375, loss: 52.8101, grad_norm: 36.6523
2025-06-22 22:39:57,417 - mmdet - INFO - Epoch [1][1900/45697]	lr: 1.250e-05, eta: 2 days, 7:44:27, time: 0.283, data_time: 0.012, memory: 3005, loss_r_cls: 5.3114, loss_sub_cls: 20.8389, loss_obj_cls: 20.0989, loss_match: 6.8774, loss: 53.1267, grad_norm: 41.8305
2025-06-22 22:40:11,744 - mmdet - INFO - Epoch [1][1950/45697]	lr: 1.250e-05, eta: 2 days, 7:42:09, time: 0.287, data_time: 0.013, memory: 3005, loss_r_cls: 5.2231, loss_sub_cls: 20.5594, loss_obj_cls: 19.9847, loss_match: 6.9149, loss: 52.6821, grad_norm: 41.1238
2025-06-22 22:40:26,156 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:40:26,157 - mmdet - INFO - Epoch [1][2000/45697]	lr: 1.250e-05, eta: 2 days, 7:40:27, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 4.9574, loss_sub_cls: 20.8971, loss_obj_cls: 19.8817, loss_match: 6.9453, loss: 52.6815, grad_norm: 44.7499
2025-06-22 22:40:40,642 - mmdet - INFO - Epoch [1][2050/45697]	lr: 1.250e-05, eta: 2 days, 7:39:13, time: 0.290, data_time: 0.012, memory: 3005, loss_r_cls: 4.9054, loss_sub_cls: 21.1906, loss_obj_cls: 19.8169, loss_match: 6.8888, loss: 52.8016, grad_norm: 39.2419
2025-06-22 22:40:55,114 - mmdet - INFO - Epoch [1][2100/45697]	lr: 1.250e-05, eta: 2 days, 7:37:57, time: 0.289, data_time: 0.012, memory: 3005, loss_r_cls: 5.4721, loss_sub_cls: 21.3611, loss_obj_cls: 19.9601, loss_match: 6.8547, loss: 53.6480, grad_norm: 39.5260
2025-06-22 22:41:09,541 - mmdet - INFO - Epoch [1][2150/45697]	lr: 1.250e-05, eta: 2 days, 7:36:30, time: 0.289, data_time: 0.013, memory: 3005, loss_r_cls: 4.7941, loss_sub_cls: 20.6908, loss_obj_cls: 20.0382, loss_match: 6.9459, loss: 52.4689, grad_norm: 46.2876
2025-06-22 22:41:23,987 - mmdet - INFO - Epoch [1][2200/45697]	lr: 1.250e-05, eta: 2 days, 7:35:13, time: 0.289, data_time: 0.014, memory: 3005, loss_r_cls: 5.1494, loss_sub_cls: 20.8394, loss_obj_cls: 19.8500, loss_match: 6.8772, loss: 52.7160, grad_norm: 37.1540
2025-06-22 22:41:38,067 - mmdet - INFO - Epoch [1][2250/45697]	lr: 1.250e-05, eta: 2 days, 7:32:07, time: 0.282, data_time: 0.012, memory: 3005, loss_r_cls: 4.9653, loss_sub_cls: 20.6571, loss_obj_cls: 19.6303, loss_match: 6.8895, loss: 52.1422, grad_norm: 41.2977
2025-06-22 22:41:52,413 - mmdet - INFO - Epoch [1][2300/45697]	lr: 1.250e-05, eta: 2 days, 7:30:27, time: 0.287, data_time: 0.012, memory: 3005, loss_r_cls: 5.3740, loss_sub_cls: 21.2073, loss_obj_cls: 19.7854, loss_match: 6.8961, loss: 53.2628, grad_norm: 42.2710
2025-06-22 22:42:06,776 - mmdet - INFO - Epoch [1][2350/45697]	lr: 1.250e-05, eta: 2 days, 7:28:56, time: 0.287, data_time: 0.014, memory: 3005, loss_r_cls: 5.0216, loss_sub_cls: 20.9032, loss_obj_cls: 19.7147, loss_match: 6.8560, loss: 52.4955, grad_norm: 37.3116
2025-06-22 22:42:21,154 - mmdet - INFO - Epoch [1][2400/45697]	lr: 1.250e-05, eta: 2 days, 7:27:32, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 5.0465, loss_sub_cls: 21.0109, loss_obj_cls: 19.6589, loss_match: 6.8926, loss: 52.6089, grad_norm: 39.8099
2025-06-22 22:42:35,836 - mmdet - INFO - Epoch [1][2450/45697]	lr: 1.250e-05, eta: 2 days, 7:27:36, time: 0.294, data_time: 0.015, memory: 3005, loss_r_cls: 5.4487, loss_sub_cls: 20.9844, loss_obj_cls: 19.9650, loss_match: 6.9010, loss: 53.2991, grad_norm: 40.3985
2025-06-22 22:42:50,455 - mmdet - INFO - Epoch [1][2500/45697]	lr: 1.250e-05, eta: 2 days, 7:27:22, time: 0.292, data_time: 0.015, memory: 3005, loss_r_cls: 5.1587, loss_sub_cls: 20.9175, loss_obj_cls: 19.7175, loss_match: 6.8432, loss: 52.6369, grad_norm: 37.9870
2025-06-22 22:43:04,736 - mmdet - INFO - Epoch [1][2550/45697]	lr: 1.250e-05, eta: 2 days, 7:25:38, time: 0.286, data_time: 0.013, memory: 3005, loss_r_cls: 4.8977, loss_sub_cls: 20.6978, loss_obj_cls: 19.7374, loss_match: 6.9504, loss: 52.2833, grad_norm: 41.7912
2025-06-22 22:43:18,652 - mmdet - INFO - Epoch [1][2600/45697]	lr: 1.250e-05, eta: 2 days, 7:22:21, time: 0.278, data_time: 0.012, memory: 3005, loss_r_cls: 5.4986, loss_sub_cls: 20.7844, loss_obj_cls: 19.6199, loss_match: 6.8967, loss: 52.7996, grad_norm: 44.5404
2025-06-22 22:43:32,871 - mmdet - INFO - Epoch [1][2650/45697]	lr: 1.250e-05, eta: 2 days, 7:20:29, time: 0.284, data_time: 0.013, memory: 3005, loss_r_cls: 4.9695, loss_sub_cls: 20.9522, loss_obj_cls: 19.7191, loss_match: 6.8852, loss: 52.5260, grad_norm: 36.5710
2025-06-22 22:43:47,339 - mmdet - INFO - Epoch [1][2700/45697]	lr: 1.250e-05, eta: 2 days, 7:19:44, time: 0.289, data_time: 0.014, memory: 3005, loss_r_cls: 5.2519, loss_sub_cls: 20.5653, loss_obj_cls: 19.9359, loss_match: 6.9119, loss: 52.6651, grad_norm: 44.4137
2025-06-22 22:44:01,919 - mmdet - INFO - Epoch [1][2750/45697]	lr: 1.250e-05, eta: 2 days, 7:19:27, time: 0.292, data_time: 0.014, memory: 3005, loss_r_cls: 4.5488, loss_sub_cls: 20.4919, loss_obj_cls: 19.7628, loss_match: 6.8716, loss: 51.6751, grad_norm: 37.4953
2025-06-22 22:44:16,532 - mmdet - INFO - Epoch [1][2800/45697]	lr: 1.250e-05, eta: 2 days, 7:19:19, time: 0.292, data_time: 0.014, memory: 3005, loss_r_cls: 5.8146, loss_sub_cls: 20.9015, loss_obj_cls: 19.9968, loss_match: 6.9020, loss: 53.6150, grad_norm: 45.1889
2025-06-22 22:44:30,912 - mmdet - INFO - Epoch [1][2850/45697]	lr: 1.250e-05, eta: 2 days, 7:18:15, time: 0.288, data_time: 0.014, memory: 3005, loss_r_cls: 4.9119, loss_sub_cls: 20.8317, loss_obj_cls: 19.4958, loss_match: 6.8559, loss: 52.0953, grad_norm: 39.9263
2025-06-22 22:44:45,231 - mmdet - INFO - Epoch [1][2900/45697]	lr: 1.250e-05, eta: 2 days, 7:16:58, time: 0.286, data_time: 0.014, memory: 3005, loss_r_cls: 5.3863, loss_sub_cls: 21.2574, loss_obj_cls: 19.8343, loss_match: 6.8933, loss: 53.3713, grad_norm: 45.3865
2025-06-22 22:44:59,458 - mmdet - INFO - Epoch [1][2950/45697]	lr: 1.250e-05, eta: 2 days, 7:15:22, time: 0.285, data_time: 0.014, memory: 3005, loss_r_cls: 4.8952, loss_sub_cls: 20.7801, loss_obj_cls: 19.6776, loss_match: 6.9574, loss: 52.3102, grad_norm: 43.5889
2025-06-22 22:45:13,927 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:45:13,927 - mmdet - INFO - Epoch [1][3000/45697]	lr: 1.250e-05, eta: 2 days, 7:14:44, time: 0.289, data_time: 0.013, memory: 3005, loss_r_cls: 5.1461, loss_sub_cls: 20.7902, loss_obj_cls: 19.5258, loss_match: 6.8592, loss: 52.3213, grad_norm: 39.5606
2025-06-22 22:45:28,351 - mmdet - INFO - Epoch [1][3050/45697]	lr: 1.250e-05, eta: 2 days, 7:13:56, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 4.9935, loss_sub_cls: 21.2357, loss_obj_cls: 20.1716, loss_match: 6.8673, loss: 53.2681, grad_norm: 45.1830
2025-06-22 22:45:42,707 - mmdet - INFO - Epoch [1][3100/45697]	lr: 1.250e-05, eta: 2 days, 7:12:54, time: 0.287, data_time: 0.012, memory: 3005, loss_r_cls: 5.1025, loss_sub_cls: 21.1500, loss_obj_cls: 19.7948, loss_match: 6.9076, loss: 52.9549, grad_norm: 43.2595
2025-06-22 22:45:56,927 - mmdet - INFO - Epoch [1][3150/45697]	lr: 1.250e-05, eta: 2 days, 7:11:25, time: 0.284, data_time: 0.012, memory: 3005, loss_r_cls: 4.9922, loss_sub_cls: 21.2361, loss_obj_cls: 19.8102, loss_match: 6.9046, loss: 52.9432, grad_norm: 41.0607
2025-06-22 22:46:11,029 - mmdet - INFO - Epoch [1][3200/45697]	lr: 1.250e-05, eta: 2 days, 7:09:33, time: 0.282, data_time: 0.013, memory: 3005, loss_r_cls: 4.6311, loss_sub_cls: 20.9081, loss_obj_cls: 19.6283, loss_match: 6.8497, loss: 52.0172, grad_norm: 36.9515
2025-06-22 22:46:25,130 - mmdet - INFO - Epoch [1][3250/45697]	lr: 1.250e-05, eta: 2 days, 7:07:43, time: 0.282, data_time: 0.012, memory: 3005, loss_r_cls: 5.2045, loss_sub_cls: 21.0199, loss_obj_cls: 19.8815, loss_match: 6.9683, loss: 53.0741, grad_norm: 49.7549
2025-06-22 22:46:39,714 - mmdet - INFO - Epoch [1][3300/45697]	lr: 1.250e-05, eta: 2 days, 7:07:37, time: 0.292, data_time: 0.014, memory: 3005, loss_r_cls: 5.2476, loss_sub_cls: 20.9547, loss_obj_cls: 19.7791, loss_match: 6.8764, loss: 52.8578, grad_norm: 37.7311
2025-06-22 22:46:54,138 - mmdet - INFO - Epoch [1][3350/45697]	lr: 1.250e-05, eta: 2 days, 7:06:57, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 4.8829, loss_sub_cls: 21.1439, loss_obj_cls: 19.8643, loss_match: 6.8687, loss: 52.7599, grad_norm: 38.6632
2025-06-22 22:47:08,472 - mmdet - INFO - Epoch [1][3400/45697]	lr: 1.250e-05, eta: 2 days, 7:06:00, time: 0.287, data_time: 0.013, memory: 3005, loss_r_cls: 5.1572, loss_sub_cls: 20.6513, loss_obj_cls: 19.7873, loss_match: 6.9216, loss: 52.5174, grad_norm: 42.6035
2025-06-22 22:47:22,745 - mmdet - INFO - Epoch [1][3450/45697]	lr: 1.250e-05, eta: 2 days, 7:04:52, time: 0.285, data_time: 0.013, memory: 3005, loss_r_cls: 5.0642, loss_sub_cls: 21.3623, loss_obj_cls: 19.7217, loss_match: 6.8399, loss: 52.9882, grad_norm: 39.7032
2025-06-22 22:47:37,365 - mmdet - INFO - Epoch [1][3500/45697]	lr: 1.250e-05, eta: 2 days, 7:04:54, time: 0.292, data_time: 0.013, memory: 3005, loss_r_cls: 4.7942, loss_sub_cls: 21.6243, loss_obj_cls: 20.0533, loss_match: 6.8921, loss: 53.3639, grad_norm: 36.8303
2025-06-22 22:47:51,657 - mmdet - INFO - Epoch [1][3550/45697]	lr: 1.250e-05, eta: 2 days, 7:03:52, time: 0.286, data_time: 0.012, memory: 3005, loss_r_cls: 4.8542, loss_sub_cls: 21.0314, loss_obj_cls: 19.6259, loss_match: 6.9169, loss: 52.4285, grad_norm: 49.7665
2025-06-22 22:48:06,167 - mmdet - INFO - Epoch [1][3600/45697]	lr: 1.250e-05, eta: 2 days, 7:03:33, time: 0.290, data_time: 0.012, memory: 3005, loss_r_cls: 5.0343, loss_sub_cls: 21.1768, loss_obj_cls: 19.7602, loss_match: 6.8646, loss: 52.8359, grad_norm: 41.3207
2025-06-22 22:48:20,568 - mmdet - INFO - Epoch [1][3650/45697]	lr: 1.250e-05, eta: 2 days, 7:02:53, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 5.0169, loss_sub_cls: 20.4863, loss_obj_cls: 19.9706, loss_match: 6.9280, loss: 52.4019, grad_norm: 40.6424
2025-06-22 22:48:34,895 - mmdet - INFO - Epoch [1][3700/45697]	lr: 1.250e-05, eta: 2 days, 7:02:00, time: 0.287, data_time: 0.013, memory: 3005, loss_r_cls: 5.2776, loss_sub_cls: 20.6094, loss_obj_cls: 20.0148, loss_match: 6.8726, loss: 52.7744, grad_norm: 41.5262
2025-06-22 22:48:49,279 - mmdet - INFO - Epoch [1][3750/45697]	lr: 1.250e-05, eta: 2 days, 7:01:19, time: 0.288, data_time: 0.015, memory: 3005, loss_r_cls: 4.6260, loss_sub_cls: 21.0687, loss_obj_cls: 20.0654, loss_match: 6.8821, loss: 52.6422, grad_norm: 40.5392
2025-06-22 22:49:03,709 - mmdet - INFO - Epoch [1][3800/45697]	lr: 1.250e-05, eta: 2 days, 7:00:47, time: 0.289, data_time: 0.013, memory: 3005, loss_r_cls: 4.5975, loss_sub_cls: 20.8899, loss_obj_cls: 19.6965, loss_match: 6.8874, loss: 52.0713, grad_norm: 40.2994
2025-06-22 22:49:18,188 - mmdet - INFO - Epoch [1][3850/45697]	lr: 1.250e-05, eta: 2 days, 7:00:24, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 5.0829, loss_sub_cls: 21.0967, loss_obj_cls: 19.8438, loss_match: 6.9006, loss: 52.9239, grad_norm: 46.6882
2025-06-22 22:49:32,378 - mmdet - INFO - Epoch [1][3900/45697]	lr: 1.250e-05, eta: 2 days, 6:59:11, time: 0.284, data_time: 0.013, memory: 3005, loss_r_cls: 5.3090, loss_sub_cls: 21.4739, loss_obj_cls: 20.0463, loss_match: 6.8795, loss: 53.7087, grad_norm: 54.0998
2025-06-22 22:49:46,817 - mmdet - INFO - Epoch [1][3950/45697]	lr: 1.250e-05, eta: 2 days, 6:58:42, time: 0.289, data_time: 0.012, memory: 3005, loss_r_cls: 5.5103, loss_sub_cls: 21.2225, loss_obj_cls: 20.1098, loss_match: 6.8280, loss: 53.6706, grad_norm: 48.1717
2025-06-22 22:50:01,332 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:50:01,332 - mmdet - INFO - Epoch [1][4000/45697]	lr: 1.250e-05, eta: 2 days, 6:58:26, time: 0.290, data_time: 0.014, memory: 3005, loss_r_cls: 5.2388, loss_sub_cls: 21.5261, loss_obj_cls: 19.7114, loss_match: 6.8294, loss: 53.3057, grad_norm: 48.6049
2025-06-22 22:50:15,814 - mmdet - INFO - Epoch [1][4050/45697]	lr: 1.250e-05, eta: 2 days, 6:58:05, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 4.9403, loss_sub_cls: 21.2039, loss_obj_cls: 19.6464, loss_match: 6.9081, loss: 52.6987, grad_norm: 44.6082
2025-06-22 22:50:30,416 - mmdet - INFO - Epoch [1][4100/45697]	lr: 1.250e-05, eta: 2 days, 6:58:04, time: 0.292, data_time: 0.013, memory: 3005, loss_r_cls: 4.9209, loss_sub_cls: 21.1021, loss_obj_cls: 19.8422, loss_match: 6.8620, loss: 52.7271, grad_norm: 46.1146
2025-06-22 22:50:44,900 - mmdet - INFO - Epoch [1][4150/45697]	lr: 1.250e-05, eta: 2 days, 6:57:43, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 4.5756, loss_sub_cls: 21.3073, loss_obj_cls: 20.1373, loss_match: 6.9004, loss: 52.9206, grad_norm: 49.1100
2025-06-22 22:50:59,425 - mmdet - INFO - Epoch [1][4200/45697]	lr: 1.250e-05, eta: 2 days, 6:57:29, time: 0.290, data_time: 0.014, memory: 3005, loss_r_cls: 5.5733, loss_sub_cls: 20.9947, loss_obj_cls: 20.0893, loss_match: 6.7808, loss: 53.4382, grad_norm: 45.1027
2025-06-22 22:51:14,048 - mmdet - INFO - Epoch [1][4250/45697]	lr: 1.250e-05, eta: 2 days, 6:57:31, time: 0.292, data_time: 0.015, memory: 3005, loss_r_cls: 4.9506, loss_sub_cls: 20.8390, loss_obj_cls: 20.1454, loss_match: 6.8597, loss: 52.7947, grad_norm: 52.6519
2025-06-22 22:51:28,421 - mmdet - INFO - Epoch [1][4300/45697]	lr: 1.250e-05, eta: 2 days, 6:56:53, time: 0.287, data_time: 0.014, memory: 3005, loss_r_cls: 5.4709, loss_sub_cls: 20.5933, loss_obj_cls: 19.9067, loss_match: 6.9515, loss: 52.9224, grad_norm: 48.1441
2025-06-22 22:51:42,923 - mmdet - INFO - Epoch [1][4350/45697]	lr: 1.250e-05, eta: 2 days, 6:56:35, time: 0.290, data_time: 0.013, memory: 3005, loss_r_cls: 4.7522, loss_sub_cls: 20.8598, loss_obj_cls: 19.7840, loss_match: 6.9103, loss: 52.3064, grad_norm: 41.1921
2025-06-22 22:51:57,168 - mmdet - INFO - Epoch [1][4400/45697]	lr: 1.250e-05, eta: 2 days, 6:55:38, time: 0.285, data_time: 0.013, memory: 3005, loss_r_cls: 5.0995, loss_sub_cls: 20.9015, loss_obj_cls: 19.7817, loss_match: 6.8577, loss: 52.6405, grad_norm: 40.0099
2025-06-22 22:52:11,506 - mmdet - INFO - Epoch [1][4450/45697]	lr: 1.250e-05, eta: 2 days, 6:54:56, time: 0.287, data_time: 0.012, memory: 3005, loss_r_cls: 4.5896, loss_sub_cls: 20.5790, loss_obj_cls: 19.8844, loss_match: 6.8878, loss: 51.9408, grad_norm: 41.2981
2025-06-22 22:52:26,072 - mmdet - INFO - Epoch [1][4500/45697]	lr: 1.250e-05, eta: 2 days, 6:54:49, time: 0.291, data_time: 0.015, memory: 3005, loss_r_cls: 4.8010, loss_sub_cls: 21.1385, loss_obj_cls: 19.9974, loss_match: 6.8951, loss: 52.8320, grad_norm: 42.4789
2025-06-22 22:52:40,445 - mmdet - INFO - Epoch [1][4550/45697]	lr: 1.250e-05, eta: 2 days, 6:54:13, time: 0.287, data_time: 0.014, memory: 3005, loss_r_cls: 5.1787, loss_sub_cls: 21.3358, loss_obj_cls: 20.0513, loss_match: 6.8703, loss: 53.4362, grad_norm: 43.0286
2025-06-22 22:52:54,820 - mmdet - INFO - Epoch [1][4600/45697]	lr: 1.250e-05, eta: 2 days, 6:53:38, time: 0.288, data_time: 0.013, memory: 3005, loss_r_cls: 5.0848, loss_sub_cls: 21.0634, loss_obj_cls: 19.7972, loss_match: 6.8635, loss: 52.8089, grad_norm: 46.5056
2025-06-22 22:53:09,419 - mmdet - INFO - Epoch [1][4650/45697]	lr: 1.250e-05, eta: 2 days, 6:53:36, time: 0.292, data_time: 0.014, memory: 3005, loss_r_cls: 5.1642, loss_sub_cls: 21.1084, loss_obj_cls: 20.0709, loss_match: 6.8852, loss: 53.2288, grad_norm: 44.3299
2025-06-22 22:53:23,768 - mmdet - INFO - Epoch [1][4700/45697]	lr: 1.250e-05, eta: 2 days, 6:52:58, time: 0.287, data_time: 0.012, memory: 3005, loss_r_cls: 5.1230, loss_sub_cls: 21.0320, loss_obj_cls: 19.8027, loss_match: 6.8598, loss: 52.8175, grad_norm: 47.0997
2025-06-22 22:53:38,346 - mmdet - INFO - Epoch [1][4750/45697]	lr: 1.250e-05, eta: 2 days, 6:52:53, time: 0.292, data_time: 0.013, memory: 3005, loss_r_cls: 5.3515, loss_sub_cls: 21.0327, loss_obj_cls: 19.6774, loss_match: 6.8427, loss: 52.9043, grad_norm: 53.0589
2025-06-22 22:53:52,884 - mmdet - INFO - Epoch [1][4800/45697]	lr: 1.250e-05, eta: 2 days, 6:52:42, time: 0.291, data_time: 0.014, memory: 3005, loss_r_cls: 4.4483, loss_sub_cls: 20.9247, loss_obj_cls: 19.5784, loss_match: 6.8357, loss: 51.7871, grad_norm: 40.7154
2025-06-22 22:54:07,512 - mmdet - INFO - Epoch [1][4850/45697]	lr: 1.250e-05, eta: 2 days, 6:52:44, time: 0.293, data_time: 0.013, memory: 3005, loss_r_cls: 5.2498, loss_sub_cls: 21.0323, loss_obj_cls: 19.6078, loss_match: 6.7864, loss: 52.6762, grad_norm: 47.6935
2025-06-22 22:54:22,078 - mmdet - INFO - Epoch [1][4900/45697]	lr: 1.250e-05, eta: 2 days, 6:52:36, time: 0.291, data_time: 0.013, memory: 3005, loss_r_cls: 5.0259, loss_sub_cls: 20.9420, loss_obj_cls: 20.1051, loss_match: 6.8303, loss: 52.9032, grad_norm: 50.5130
2025-06-22 22:54:36,609 - mmdet - INFO - Epoch [1][4950/45697]	lr: 1.250e-05, eta: 2 days, 6:52:24, time: 0.291, data_time: 0.014, memory: 3005, loss_r_cls: 5.3819, loss_sub_cls: 20.9329, loss_obj_cls: 19.8672, loss_match: 6.8707, loss: 53.0528, grad_norm: 49.5035
2025-06-22 22:54:51,146 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:54:51,146 - mmdet - INFO - Epoch [1][5000/45697]	lr: 1.250e-05, eta: 2 days, 6:52:13, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.7409, loss_sub_cls: 21.0168, loss_obj_cls: 19.9747, loss_match: 6.8225, loss: 52.5549, grad_norm: 40.7693
2025-06-22 22:55:05,778 - mmdet - INFO - Epoch [1][5050/45697]	lr: 1.250e-05, eta: 2 days, 6:52:14, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.8182, loss_sub_cls: 21.0368, loss_obj_cls: 19.8130, loss_match: 6.9040, loss: 52.5719, grad_norm: 44.1972
2025-06-22 22:55:20,766 - mmdet - INFO - Epoch [1][5100/45697]	lr: 1.250e-05, eta: 2 days, 6:53:02, time: 0.300, data_time: 0.015, memory: 3006, loss_r_cls: 4.5006, loss_sub_cls: 21.0805, loss_obj_cls: 19.9433, loss_match: 6.8605, loss: 52.3850, grad_norm: 44.8783
2025-06-22 22:55:35,737 - mmdet - INFO - Epoch [1][5150/45697]	lr: 1.250e-05, eta: 2 days, 6:53:47, time: 0.299, data_time: 0.014, memory: 3006, loss_r_cls: 4.7905, loss_sub_cls: 21.0593, loss_obj_cls: 19.8398, loss_match: 6.8398, loss: 52.5294, grad_norm: 37.9906
2025-06-22 22:55:50,185 - mmdet - INFO - Epoch [1][5200/45697]	lr: 1.250e-05, eta: 2 days, 6:53:22, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.2038, loss_sub_cls: 20.9415, loss_obj_cls: 19.6405, loss_match: 6.9289, loss: 52.7147, grad_norm: 43.5436
2025-06-22 22:56:04,611 - mmdet - INFO - Epoch [1][5250/45697]	lr: 1.250e-05, eta: 2 days, 6:52:55, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 5.2722, loss_sub_cls: 21.2356, loss_obj_cls: 20.0411, loss_match: 6.8545, loss: 53.4034, grad_norm: 58.9704
2025-06-22 22:56:18,970 - mmdet - INFO - Epoch [1][5300/45697]	lr: 1.250e-05, eta: 2 days, 6:52:19, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.7310, loss_sub_cls: 20.8504, loss_obj_cls: 20.2219, loss_match: 6.8480, loss: 52.6514, grad_norm: 45.9966
2025-06-22 22:56:33,338 - mmdet - INFO - Epoch [1][5350/45697]	lr: 1.250e-05, eta: 2 days, 6:51:45, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 5.0918, loss_sub_cls: 20.7481, loss_obj_cls: 20.1486, loss_match: 6.8539, loss: 52.8424, grad_norm: 48.1570
2025-06-22 22:56:47,572 - mmdet - INFO - Epoch [1][5400/45697]	lr: 1.250e-05, eta: 2 days, 6:50:55, time: 0.285, data_time: 0.012, memory: 3006, loss_r_cls: 4.6998, loss_sub_cls: 20.9135, loss_obj_cls: 19.6976, loss_match: 6.9123, loss: 52.2232, grad_norm: 48.3227
2025-06-22 22:57:02,177 - mmdet - INFO - Epoch [1][5450/45697]	lr: 1.250e-05, eta: 2 days, 6:50:51, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 5.5210, loss_sub_cls: 21.3237, loss_obj_cls: 20.1654, loss_match: 6.8738, loss: 53.8840, grad_norm: 51.4757
2025-06-22 22:57:16,695 - mmdet - INFO - Epoch [1][5500/45697]	lr: 1.250e-05, eta: 2 days, 6:50:36, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.6849, loss_sub_cls: 21.0923, loss_obj_cls: 19.9957, loss_match: 6.7964, loss: 52.5693, grad_norm: 39.0995
2025-06-22 22:57:31,473 - mmdet - INFO - Epoch [1][5550/45697]	lr: 1.250e-05, eta: 2 days, 6:50:54, time: 0.296, data_time: 0.012, memory: 3006, loss_r_cls: 5.0790, loss_sub_cls: 21.6894, loss_obj_cls: 19.9373, loss_match: 6.8346, loss: 53.5403, grad_norm: 47.2584
2025-06-22 22:57:46,114 - mmdet - INFO - Epoch [1][5600/45697]	lr: 1.250e-05, eta: 2 days, 6:50:54, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.5678, loss_sub_cls: 20.0863, loss_obj_cls: 19.4214, loss_match: 6.7805, loss: 50.8560, grad_norm: 48.2130
2025-06-22 22:58:00,615 - mmdet - INFO - Epoch [1][5650/45697]	lr: 1.250e-05, eta: 2 days, 6:50:37, time: 0.290, data_time: 0.012, memory: 3006, loss_r_cls: 5.1189, loss_sub_cls: 20.8103, loss_obj_cls: 19.9658, loss_match: 6.8315, loss: 52.7265, grad_norm: 50.8365
2025-06-22 22:58:15,050 - mmdet - INFO - Epoch [1][5700/45697]	lr: 1.250e-05, eta: 2 days, 6:50:12, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 5.0378, loss_sub_cls: 21.0637, loss_obj_cls: 20.0261, loss_match: 6.7639, loss: 52.8915, grad_norm: 47.3421
2025-06-22 22:58:29,669 - mmdet - INFO - Epoch [1][5750/45697]	lr: 1.250e-05, eta: 2 days, 6:50:09, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.8733, loss_sub_cls: 20.6523, loss_obj_cls: 19.9574, loss_match: 6.7689, loss: 52.2519, grad_norm: 44.2940
2025-06-22 22:58:44,273 - mmdet - INFO - Epoch [1][5800/45697]	lr: 1.250e-05, eta: 2 days, 6:50:04, time: 0.292, data_time: 0.012, memory: 3006, loss_r_cls: 4.9871, loss_sub_cls: 21.3033, loss_obj_cls: 19.7105, loss_match: 6.7017, loss: 52.7026, grad_norm: 52.2524
2025-06-22 22:58:58,759 - mmdet - INFO - Epoch [1][5850/45697]	lr: 1.250e-05, eta: 2 days, 6:49:45, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 5.1797, loss_sub_cls: 21.2963, loss_obj_cls: 20.3229, loss_match: 6.7118, loss: 53.5108, grad_norm: 51.7468
2025-06-22 22:59:13,146 - mmdet - INFO - Epoch [1][5900/45697]	lr: 1.250e-05, eta: 2 days, 6:49:15, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.7422, loss_sub_cls: 21.3676, loss_obj_cls: 19.9258, loss_match: 6.8398, loss: 52.8755, grad_norm: 50.5491
2025-06-22 22:59:27,728 - mmdet - INFO - Epoch [1][5950/45697]	lr: 1.250e-05, eta: 2 days, 6:49:07, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.7147, loss_sub_cls: 20.3497, loss_obj_cls: 19.8203, loss_match: 6.7119, loss: 51.5966, grad_norm: 48.6309
2025-06-22 22:59:41,943 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 22:59:41,944 - mmdet - INFO - Epoch [1][6000/45697]	lr: 1.250e-05, eta: 2 days, 6:48:18, time: 0.284, data_time: 0.013, memory: 3006, loss_r_cls: 5.0943, loss_sub_cls: 21.0221, loss_obj_cls: 20.0651, loss_match: 6.6575, loss: 52.8390, grad_norm: 59.9670
2025-06-22 22:59:56,554 - mmdet - INFO - Epoch [1][6050/45697]	lr: 1.250e-05, eta: 2 days, 6:48:14, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.5742, loss_sub_cls: 20.6377, loss_obj_cls: 19.7489, loss_match: 6.6523, loss: 51.6131, grad_norm: 56.3633
2025-06-22 23:00:11,192 - mmdet - INFO - Epoch [1][6100/45697]	lr: 1.250e-05, eta: 2 days, 6:48:13, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 5.1336, loss_sub_cls: 20.6340, loss_obj_cls: 19.7548, loss_match: 6.7726, loss: 52.2951, grad_norm: 70.4022
2025-06-22 23:00:25,680 - mmdet - INFO - Epoch [1][6150/45697]	lr: 1.250e-05, eta: 2 days, 6:47:54, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.7231, loss_sub_cls: 20.7441, loss_obj_cls: 19.9168, loss_match: 6.5673, loss: 51.9514, grad_norm: 65.2871
2025-06-22 23:00:40,109 - mmdet - INFO - Epoch [1][6200/45697]	lr: 1.250e-05, eta: 2 days, 6:47:30, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 4.8149, loss_sub_cls: 20.0933, loss_obj_cls: 19.5699, loss_match: 6.5377, loss: 51.0158, grad_norm: 65.6654
2025-06-22 23:00:54,538 - mmdet - INFO - Epoch [1][6250/45697]	lr: 1.250e-05, eta: 2 days, 6:47:05, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.7569, loss_sub_cls: 20.2181, loss_obj_cls: 20.1383, loss_match: 6.6415, loss: 51.7549, grad_norm: 81.3961
2025-06-22 23:01:08,835 - mmdet - INFO - Epoch [1][6300/45697]	lr: 1.250e-05, eta: 2 days, 6:46:27, time: 0.286, data_time: 0.014, memory: 3006, loss_r_cls: 5.4524, loss_sub_cls: 20.6155, loss_obj_cls: 19.9477, loss_match: 6.4706, loss: 52.4862, grad_norm: 73.6429
2025-06-22 23:01:23,379 - mmdet - INFO - Epoch [1][6350/45697]	lr: 1.250e-05, eta: 2 days, 6:46:15, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.4784, loss_sub_cls: 20.8613, loss_obj_cls: 20.1381, loss_match: 6.5960, loss: 52.0738, grad_norm: 85.4681
2025-06-22 23:01:37,791 - mmdet - INFO - Epoch [1][6400/45697]	lr: 1.250e-05, eta: 2 days, 6:45:50, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 5.1088, loss_sub_cls: 20.8225, loss_obj_cls: 20.0569, loss_match: 6.5618, loss: 52.5499, grad_norm: 84.6393
2025-06-22 23:01:52,381 - mmdet - INFO - Epoch [1][6450/45697]	lr: 1.250e-05, eta: 2 days, 6:45:43, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 5.1099, loss_sub_cls: 20.5039, loss_obj_cls: 20.0548, loss_match: 6.4111, loss: 52.0797, grad_norm: 87.4234
2025-06-22 23:02:06,680 - mmdet - INFO - Epoch [1][6500/45697]	lr: 1.250e-05, eta: 2 days, 6:45:05, time: 0.286, data_time: 0.012, memory: 3006, loss_r_cls: 4.8745, loss_sub_cls: 20.6374, loss_obj_cls: 20.0534, loss_match: 6.5232, loss: 52.0885, grad_norm: 103.6433
2025-06-22 23:02:21,095 - mmdet - INFO - Epoch [1][6550/45697]	lr: 1.250e-05, eta: 2 days, 6:44:40, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.9438, loss_sub_cls: 20.4624, loss_obj_cls: 20.0050, loss_match: 6.4504, loss: 51.8616, grad_norm: 95.7498
2025-06-22 23:02:35,572 - mmdet - INFO - Epoch [1][6600/45697]	lr: 1.250e-05, eta: 2 days, 6:44:22, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.8218, loss_sub_cls: 20.0568, loss_obj_cls: 20.0083, loss_match: 6.3637, loss: 51.2506, grad_norm: 99.9145
2025-06-22 23:02:49,941 - mmdet - INFO - Epoch [1][6650/45697]	lr: 1.250e-05, eta: 2 days, 6:43:53, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 5.1896, loss_sub_cls: 20.4644, loss_obj_cls: 20.1853, loss_match: 6.4602, loss: 52.2996, grad_norm: 106.4375
2025-06-22 23:03:04,425 - mmdet - INFO - Epoch [1][6700/45697]	lr: 1.250e-05, eta: 2 days, 6:43:35, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 5.0167, loss_sub_cls: 20.2867, loss_obj_cls: 20.0002, loss_match: 6.3095, loss: 51.6131, grad_norm: 93.8800
2025-06-22 23:03:18,774 - mmdet - INFO - Epoch [1][6750/45697]	lr: 1.250e-05, eta: 2 days, 6:43:04, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.7183, loss_sub_cls: 20.9725, loss_obj_cls: 19.8456, loss_match: 6.2227, loss: 51.7591, grad_norm: 103.7915
2025-06-22 23:03:33,331 - mmdet - INFO - Epoch [1][6800/45697]	lr: 1.250e-05, eta: 2 days, 6:42:54, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.7215, loss_sub_cls: 20.8090, loss_obj_cls: 20.2528, loss_match: 6.0698, loss: 51.8531, grad_norm: 98.9336
2025-06-22 23:03:48,023 - mmdet - INFO - Epoch [1][6850/45697]	lr: 1.250e-05, eta: 2 days, 6:42:57, time: 0.294, data_time: 0.014, memory: 3006, loss_r_cls: 5.1103, loss_sub_cls: 20.8095, loss_obj_cls: 20.1036, loss_match: 6.5436, loss: 52.5670, grad_norm: 122.1328
2025-06-22 23:04:02,386 - mmdet - INFO - Epoch [1][6900/45697]	lr: 1.250e-05, eta: 2 days, 6:42:28, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.5601, loss_sub_cls: 20.3151, loss_obj_cls: 20.3921, loss_match: 6.3277, loss: 51.5949, grad_norm: 92.3244
2025-06-22 23:04:17,016 - mmdet - INFO - Epoch [1][6950/45697]	lr: 1.250e-05, eta: 2 days, 6:42:25, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.8197, loss_sub_cls: 20.4375, loss_obj_cls: 19.9222, loss_match: 5.9813, loss: 51.1607, grad_norm: 128.8847
2025-06-22 23:04:31,434 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:04:31,434 - mmdet - INFO - Epoch [1][7000/45697]	lr: 1.250e-05, eta: 2 days, 6:42:01, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.6059, loss_sub_cls: 21.0710, loss_obj_cls: 19.5991, loss_match: 6.1863, loss: 51.4623, grad_norm: 117.3573
2025-06-22 23:04:45,886 - mmdet - INFO - Epoch [1][7050/45697]	lr: 1.250e-05, eta: 2 days, 6:41:41, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 5.1287, loss_sub_cls: 20.7820, loss_obj_cls: 19.9399, loss_match: 6.3658, loss: 52.2164, grad_norm: 124.3054
2025-06-22 23:05:00,272 - mmdet - INFO - Epoch [1][7100/45697]	lr: 1.250e-05, eta: 2 days, 6:41:14, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.6040, loss_sub_cls: 20.6669, loss_obj_cls: 20.0366, loss_match: 6.3010, loss: 51.6085, grad_norm: 114.4299
2025-06-22 23:05:14,714 - mmdet - INFO - Epoch [1][7150/45697]	lr: 1.250e-05, eta: 2 days, 6:40:53, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.0332, loss_sub_cls: 20.6026, loss_obj_cls: 19.6531, loss_match: 6.4547, loss: 51.7436, grad_norm: 138.6828
2025-06-22 23:05:29,330 - mmdet - INFO - Epoch [1][7200/45697]	lr: 1.250e-05, eta: 2 days, 6:40:48, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.5966, loss_sub_cls: 20.7810, loss_obj_cls: 19.9713, loss_match: 6.1235, loss: 51.4724, grad_norm: 115.8340
2025-06-22 23:05:43,683 - mmdet - INFO - Epoch [1][7250/45697]	lr: 1.250e-05, eta: 2 days, 6:40:19, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.6202, loss_sub_cls: 21.0291, loss_obj_cls: 20.0873, loss_match: 6.2996, loss: 52.0362, grad_norm: 126.0449
2025-06-22 23:05:58,449 - mmdet - INFO - Epoch [1][7300/45697]	lr: 1.250e-05, eta: 2 days, 6:40:28, time: 0.295, data_time: 0.014, memory: 3006, loss_r_cls: 5.2104, loss_sub_cls: 21.1449, loss_obj_cls: 20.0752, loss_match: 5.8180, loss: 52.2485, grad_norm: 113.7386
2025-06-22 23:06:13,029 - mmdet - INFO - Epoch [1][7350/45697]	lr: 1.250e-05, eta: 2 days, 6:40:20, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.9437, loss_sub_cls: 20.8829, loss_obj_cls: 19.8193, loss_match: 6.2154, loss: 51.8614, grad_norm: 150.9075
2025-06-22 23:06:27,292 - mmdet - INFO - Epoch [1][7400/45697]	lr: 1.250e-05, eta: 2 days, 6:39:43, time: 0.285, data_time: 0.014, memory: 3006, loss_r_cls: 4.7846, loss_sub_cls: 20.4011, loss_obj_cls: 19.8127, loss_match: 6.1727, loss: 51.1711, grad_norm: 123.8956
2025-06-22 23:06:41,858 - mmdet - INFO - Epoch [1][7450/45697]	lr: 1.250e-05, eta: 2 days, 6:39:33, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.7954, loss_sub_cls: 20.4380, loss_obj_cls: 19.8965, loss_match: 6.4942, loss: 51.6240, grad_norm: 132.8036
2025-06-22 23:06:56,229 - mmdet - INFO - Epoch [1][7500/45697]	lr: 1.250e-05, eta: 2 days, 6:39:06, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.8265, loss_sub_cls: 20.7291, loss_obj_cls: 19.9830, loss_match: 6.0850, loss: 51.6237, grad_norm: 113.1361
2025-06-22 23:07:10,560 - mmdet - INFO - Epoch [1][7550/45697]	lr: 1.250e-05, eta: 2 days, 6:38:35, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.9074, loss_sub_cls: 20.8269, loss_obj_cls: 19.7803, loss_match: 6.0991, loss: 51.6137, grad_norm: 134.2410
2025-06-22 23:07:25,011 - mmdet - INFO - Epoch [1][7600/45697]	lr: 1.250e-05, eta: 2 days, 6:38:16, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 4.8060, loss_sub_cls: 20.9993, loss_obj_cls: 20.4442, loss_match: 5.8296, loss: 52.0791, grad_norm: 129.7809
2025-06-22 23:07:39,574 - mmdet - INFO - Epoch [1][7650/45697]	lr: 1.250e-05, eta: 2 days, 6:38:06, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 5.0183, loss_sub_cls: 20.2685, loss_obj_cls: 20.2709, loss_match: 6.5218, loss: 52.0795, grad_norm: 144.7109
2025-06-22 23:07:53,974 - mmdet - INFO - Epoch [1][7700/45697]	lr: 1.250e-05, eta: 2 days, 6:37:42, time: 0.288, data_time: 0.014, memory: 3006, loss_r_cls: 4.5031, loss_sub_cls: 20.8352, loss_obj_cls: 20.1413, loss_match: 6.1169, loss: 51.5965, grad_norm: 114.1998
2025-06-22 23:08:08,425 - mmdet - INFO - Epoch [1][7750/45697]	lr: 1.250e-05, eta: 2 days, 6:37:22, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.7246, loss_sub_cls: 20.5030, loss_obj_cls: 20.3347, loss_match: 6.2150, loss: 51.7773, grad_norm: 113.7263
2025-06-22 23:08:23,132 - mmdet - INFO - Epoch [1][7800/45697]	lr: 1.250e-05, eta: 2 days, 6:37:25, time: 0.294, data_time: 0.013, memory: 3006, loss_r_cls: 4.5212, loss_sub_cls: 20.3531, loss_obj_cls: 19.7701, loss_match: 5.9365, loss: 50.5809, grad_norm: 111.1576
2025-06-22 23:08:37,671 - mmdet - INFO - Epoch [1][7850/45697]	lr: 1.250e-05, eta: 2 days, 6:37:13, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.6477, loss_sub_cls: 20.2090, loss_obj_cls: 19.8734, loss_match: 5.6810, loss: 50.4112, grad_norm: 115.4059
2025-06-22 23:08:52,306 - mmdet - INFO - Epoch [1][7900/45697]	lr: 1.250e-05, eta: 2 days, 6:37:09, time: 0.293, data_time: 0.015, memory: 3006, loss_r_cls: 5.6001, loss_sub_cls: 20.0392, loss_obj_cls: 20.0455, loss_match: 6.1909, loss: 51.8756, grad_norm: 142.7405
2025-06-22 23:09:06,750 - mmdet - INFO - Epoch [1][7950/45697]	lr: 1.250e-05, eta: 2 days, 6:36:49, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.3603, loss_sub_cls: 21.3369, loss_obj_cls: 20.0209, loss_match: 5.7113, loss: 51.4295, grad_norm: 139.2423
2025-06-22 23:09:21,449 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:09:21,450 - mmdet - INFO - Epoch [1][8000/45697]	lr: 1.250e-05, eta: 2 days, 6:36:50, time: 0.294, data_time: 0.014, memory: 3006, loss_r_cls: 4.9814, loss_sub_cls: 20.6341, loss_obj_cls: 19.6998, loss_match: 5.6119, loss: 50.9271, grad_norm: 137.8166
2025-06-22 23:09:35,992 - mmdet - INFO - Epoch [1][8050/45697]	lr: 1.250e-05, eta: 2 days, 6:36:39, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 5.1638, loss_sub_cls: 20.4872, loss_obj_cls: 20.0101, loss_match: 6.1013, loss: 51.7624, grad_norm: 133.1004
2025-06-22 23:09:50,502 - mmdet - INFO - Epoch [1][8100/45697]	lr: 1.250e-05, eta: 2 days, 6:36:24, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 4.8663, loss_sub_cls: 20.4810, loss_obj_cls: 19.6171, loss_match: 6.3928, loss: 51.3572, grad_norm: 136.6029
2025-06-22 23:10:05,148 - mmdet - INFO - Epoch [1][8150/45697]	lr: 1.250e-05, eta: 2 days, 6:36:21, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.8875, loss_sub_cls: 20.1870, loss_obj_cls: 19.9118, loss_match: 6.0701, loss: 51.0563, grad_norm: 121.2615
2025-06-22 23:10:19,759 - mmdet - INFO - Epoch [1][8200/45697]	lr: 1.250e-05, eta: 2 days, 6:36:14, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.7350, loss_sub_cls: 20.6462, loss_obj_cls: 20.3816, loss_match: 5.4616, loss: 51.2243, grad_norm: 122.3488
2025-06-22 23:10:34,184 - mmdet - INFO - Epoch [1][8250/45697]	lr: 1.250e-05, eta: 2 days, 6:35:52, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.5988, loss_sub_cls: 20.9913, loss_obj_cls: 20.2269, loss_match: 5.8845, loss: 51.7015, grad_norm: 142.2922
2025-06-22 23:10:48,647 - mmdet - INFO - Epoch [1][8300/45697]	lr: 1.250e-05, eta: 2 days, 6:35:34, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 5.0232, loss_sub_cls: 20.7414, loss_obj_cls: 19.8573, loss_match: 6.0254, loss: 51.6473, grad_norm: 131.0510
2025-06-22 23:11:03,160 - mmdet - INFO - Epoch [1][8350/45697]	lr: 1.250e-05, eta: 2 days, 6:35:20, time: 0.290, data_time: 0.015, memory: 3006, loss_r_cls: 4.8475, loss_sub_cls: 20.9626, loss_obj_cls: 20.0363, loss_match: 5.8393, loss: 51.6857, grad_norm: 133.3387
2025-06-22 23:11:17,814 - mmdet - INFO - Epoch [1][8400/45697]	lr: 1.250e-05, eta: 2 days, 6:35:16, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.7871, loss_sub_cls: 20.4410, loss_obj_cls: 20.0769, loss_match: 5.3635, loss: 50.6684, grad_norm: 137.4275
2025-06-22 23:11:32,285 - mmdet - INFO - Epoch [1][8450/45697]	lr: 1.250e-05, eta: 2 days, 6:34:59, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.8739, loss_sub_cls: 20.6960, loss_obj_cls: 19.5696, loss_match: 6.1039, loss: 51.2434, grad_norm: 149.4896
2025-06-22 23:11:46,657 - mmdet - INFO - Epoch [1][8500/45697]	lr: 1.250e-05, eta: 2 days, 6:34:33, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.8511, loss_sub_cls: 20.8949, loss_obj_cls: 20.1476, loss_match: 5.9880, loss: 51.8816, grad_norm: 122.3836
2025-06-22 23:12:01,409 - mmdet - INFO - Epoch [1][8550/45697]	lr: 1.250e-05, eta: 2 days, 6:34:37, time: 0.295, data_time: 0.015, memory: 3006, loss_r_cls: 4.8378, loss_sub_cls: 20.9513, loss_obj_cls: 20.3878, loss_match: 5.8959, loss: 52.0728, grad_norm: 113.2177
2025-06-22 23:12:15,882 - mmdet - INFO - Epoch [1][8600/45697]	lr: 1.250e-05, eta: 2 days, 6:34:20, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.5306, loss_sub_cls: 20.9425, loss_obj_cls: 20.0922, loss_match: 6.1226, loss: 51.6879, grad_norm: 123.1218
2025-06-22 23:12:30,310 - mmdet - INFO - Epoch [1][8650/45697]	lr: 1.250e-05, eta: 2 days, 6:33:58, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.1111, loss_sub_cls: 20.0766, loss_obj_cls: 19.9641, loss_match: 5.5912, loss: 50.7431, grad_norm: 136.6420
2025-06-22 23:12:44,601 - mmdet - INFO - Epoch [1][8700/45697]	lr: 1.250e-05, eta: 2 days, 6:33:27, time: 0.286, data_time: 0.012, memory: 3006, loss_r_cls: 4.5480, loss_sub_cls: 20.0398, loss_obj_cls: 19.9198, loss_match: 5.4113, loss: 49.9188, grad_norm: 135.7632
2025-06-22 23:12:58,986 - mmdet - INFO - Epoch [1][8750/45697]	lr: 1.250e-05, eta: 2 days, 6:33:02, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.7940, loss_sub_cls: 19.9460, loss_obj_cls: 20.0486, loss_match: 5.7254, loss: 50.5140, grad_norm: 130.6137
2025-06-22 23:13:13,549 - mmdet - INFO - Epoch [1][8800/45697]	lr: 1.250e-05, eta: 2 days, 6:32:52, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.2424, loss_sub_cls: 20.5766, loss_obj_cls: 19.7392, loss_match: 5.1858, loss: 49.7440, grad_norm: 128.8832
2025-06-22 23:13:27,725 - mmdet - INFO - Epoch [1][8850/45697]	lr: 1.250e-05, eta: 2 days, 6:32:12, time: 0.284, data_time: 0.012, memory: 3006, loss_r_cls: 5.2076, loss_sub_cls: 20.1516, loss_obj_cls: 19.6684, loss_match: 5.6722, loss: 50.6999, grad_norm: 162.7179
2025-06-22 23:13:42,162 - mmdet - INFO - Epoch [1][8900/45697]	lr: 1.250e-05, eta: 2 days, 6:31:52, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.1342, loss_sub_cls: 20.2544, loss_obj_cls: 19.7330, loss_match: 5.4024, loss: 50.5239, grad_norm: 147.6182
2025-06-22 23:13:56,549 - mmdet - INFO - Epoch [1][8950/45697]	lr: 1.250e-05, eta: 2 days, 6:31:28, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.7037, loss_sub_cls: 20.0406, loss_obj_cls: 19.9307, loss_match: 5.0852, loss: 49.7602, grad_norm: 150.8658
2025-06-22 23:14:11,250 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:14:11,251 - mmdet - INFO - Epoch [1][9000/45697]	lr: 1.250e-05, eta: 2 days, 6:31:28, time: 0.294, data_time: 0.013, memory: 3006, loss_r_cls: 4.7490, loss_sub_cls: 20.1620, loss_obj_cls: 19.7376, loss_match: 5.0113, loss: 49.6599, grad_norm: 161.9257
2025-06-22 23:14:25,563 - mmdet - INFO - Epoch [1][9050/45697]	lr: 1.250e-05, eta: 2 days, 6:30:59, time: 0.286, data_time: 0.011, memory: 3006, loss_r_cls: 4.5082, loss_sub_cls: 20.5634, loss_obj_cls: 19.8948, loss_match: 4.9873, loss: 49.9536, grad_norm: 183.3882
2025-06-22 23:14:40,008 - mmdet - INFO - Epoch [1][9100/45697]	lr: 1.250e-05, eta: 2 days, 6:30:40, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.8340, loss_sub_cls: 19.8416, loss_obj_cls: 19.8718, loss_match: 5.5167, loss: 50.0640, grad_norm: 176.9751
2025-06-22 23:14:54,282 - mmdet - INFO - Epoch [1][9150/45697]	lr: 1.250e-05, eta: 2 days, 6:30:08, time: 0.285, data_time: 0.012, memory: 3006, loss_r_cls: 5.2387, loss_sub_cls: 19.9579, loss_obj_cls: 19.7010, loss_match: 4.9601, loss: 49.8577, grad_norm: 169.1483
2025-06-22 23:15:08,981 - mmdet - INFO - Epoch [1][9200/45697]	lr: 1.250e-05, eta: 2 days, 6:30:08, time: 0.294, data_time: 0.013, memory: 3006, loss_r_cls: 4.6315, loss_sub_cls: 20.2220, loss_obj_cls: 19.9003, loss_match: 4.9910, loss: 49.7448, grad_norm: 157.0188
2025-06-22 23:15:23,328 - mmdet - INFO - Epoch [1][9250/45697]	lr: 1.250e-05, eta: 2 days, 6:29:42, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.6274, loss_sub_cls: 20.0986, loss_obj_cls: 19.6221, loss_match: 4.8020, loss: 49.1501, grad_norm: 162.5925
2025-06-22 23:15:37,807 - mmdet - INFO - Epoch [1][9300/45697]	lr: 1.250e-05, eta: 2 days, 6:29:25, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 5.0089, loss_sub_cls: 20.2595, loss_obj_cls: 19.6380, loss_match: 4.9534, loss: 49.8598, grad_norm: 212.0961
2025-06-22 23:15:52,557 - mmdet - INFO - Epoch [1][9350/45697]	lr: 1.250e-05, eta: 2 days, 6:29:28, time: 0.295, data_time: 0.014, memory: 3006, loss_r_cls: 4.9567, loss_sub_cls: 20.0768, loss_obj_cls: 19.6755, loss_match: 4.6520, loss: 49.3610, grad_norm: 184.2936
2025-06-22 23:16:07,148 - mmdet - INFO - Epoch [1][9400/45697]	lr: 1.250e-05, eta: 2 days, 6:29:20, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.6359, loss_sub_cls: 20.7347, loss_obj_cls: 19.5685, loss_match: 5.1191, loss: 50.0582, grad_norm: 198.9948
2025-06-22 23:16:21,621 - mmdet - INFO - Epoch [1][9450/45697]	lr: 1.250e-05, eta: 2 days, 6:29:03, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 4.9209, loss_sub_cls: 20.3013, loss_obj_cls: 19.5935, loss_match: 4.3970, loss: 49.2127, grad_norm: 203.7755
2025-06-22 23:16:36,323 - mmdet - INFO - Epoch [1][9500/45697]	lr: 1.250e-05, eta: 2 days, 6:29:02, time: 0.294, data_time: 0.013, memory: 3006, loss_r_cls: 5.0596, loss_sub_cls: 20.3040, loss_obj_cls: 19.6096, loss_match: 4.7826, loss: 49.7559, grad_norm: 215.5349
2025-06-22 23:16:51,019 - mmdet - INFO - Epoch [1][9550/45697]	lr: 1.250e-05, eta: 2 days, 6:29:01, time: 0.294, data_time: 0.015, memory: 3006, loss_r_cls: 4.8507, loss_sub_cls: 19.7599, loss_obj_cls: 19.4303, loss_match: 4.4691, loss: 48.5099, grad_norm: 210.9620
2025-06-22 23:17:05,578 - mmdet - INFO - Epoch [1][9600/45697]	lr: 1.250e-05, eta: 2 days, 6:28:50, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.8181, loss_sub_cls: 20.4124, loss_obj_cls: 19.7373, loss_match: 4.9870, loss: 49.9549, grad_norm: 212.1385
2025-06-22 23:17:20,182 - mmdet - INFO - Epoch [1][9650/45697]	lr: 1.250e-05, eta: 2 days, 6:28:42, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.4718, loss_sub_cls: 20.2523, loss_obj_cls: 19.7490, loss_match: 4.4034, loss: 48.8765, grad_norm: 202.8112
2025-06-22 23:17:34,848 - mmdet - INFO - Epoch [1][9700/45697]	lr: 1.250e-05, eta: 2 days, 6:28:38, time: 0.293, data_time: 0.012, memory: 3006, loss_r_cls: 4.5326, loss_sub_cls: 20.2455, loss_obj_cls: 19.9652, loss_match: 4.2735, loss: 49.0167, grad_norm: 225.4434
2025-06-22 23:17:49,293 - mmdet - INFO - Epoch [1][9750/45697]	lr: 1.250e-05, eta: 2 days, 6:28:19, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.7873, loss_sub_cls: 20.1357, loss_obj_cls: 19.4075, loss_match: 4.4969, loss: 48.8274, grad_norm: 203.4531
2025-06-22 23:18:03,887 - mmdet - INFO - Epoch [1][9800/45697]	lr: 1.250e-05, eta: 2 days, 6:28:10, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 5.0613, loss_sub_cls: 19.8966, loss_obj_cls: 19.3907, loss_match: 4.6948, loss: 49.0434, grad_norm: 202.6312
2025-06-22 23:18:18,368 - mmdet - INFO - Epoch [1][9850/45697]	lr: 1.250e-05, eta: 2 days, 6:27:53, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.7765, loss_sub_cls: 19.6737, loss_obj_cls: 19.6565, loss_match: 4.8438, loss: 48.9506, grad_norm: 221.0034
2025-06-22 23:18:32,812 - mmdet - INFO - Epoch [1][9900/45697]	lr: 1.250e-05, eta: 2 days, 6:27:34, time: 0.289, data_time: 0.015, memory: 3006, loss_r_cls: 4.4620, loss_sub_cls: 20.3321, loss_obj_cls: 19.5501, loss_match: 4.5776, loss: 48.9218, grad_norm: 183.5103
2025-06-22 23:18:47,216 - mmdet - INFO - Epoch [1][9950/45697]	lr: 1.250e-05, eta: 2 days, 6:27:13, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 5.1444, loss_sub_cls: 19.5860, loss_obj_cls: 19.0350, loss_match: 4.3349, loss: 48.1004, grad_norm: 220.2802
2025-06-22 23:19:01,575 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:19:01,576 - mmdet - INFO - Epoch [1][10000/45697]	lr: 1.250e-05, eta: 2 days, 6:26:48, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.6186, loss_sub_cls: 19.7659, loss_obj_cls: 19.2583, loss_match: 4.9103, loss: 48.5530, grad_norm: 245.0004
2025-06-22 23:19:15,873 - mmdet - INFO - Epoch [1][10050/45697]	lr: 1.250e-05, eta: 2 days, 6:26:19, time: 0.286, data_time: 0.013, memory: 3006, loss_r_cls: 5.0533, loss_sub_cls: 20.4259, loss_obj_cls: 19.5368, loss_match: 4.4974, loss: 49.5134, grad_norm: 243.5529
2025-06-22 23:19:30,496 - mmdet - INFO - Epoch [1][10100/45697]	lr: 1.250e-05, eta: 2 days, 6:26:12, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.8481, loss_sub_cls: 20.3307, loss_obj_cls: 19.3158, loss_match: 4.6168, loss: 49.1114, grad_norm: 211.3983
2025-06-22 23:19:45,128 - mmdet - INFO - Epoch [1][10150/45697]	lr: 1.250e-05, eta: 2 days, 6:26:06, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 4.4108, loss_sub_cls: 20.4613, loss_obj_cls: 19.4614, loss_match: 4.6372, loss: 48.9706, grad_norm: 223.7903
2025-06-22 23:19:59,699 - mmdet - INFO - Epoch [1][10200/45697]	lr: 1.250e-05, eta: 2 days, 6:25:55, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.8784, loss_sub_cls: 20.3264, loss_obj_cls: 19.5812, loss_match: 5.2594, loss: 50.0454, grad_norm: 223.9157
2025-06-22 23:20:14,035 - mmdet - INFO - Epoch [1][10250/45697]	lr: 1.250e-05, eta: 2 days, 6:25:29, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 5.0584, loss_sub_cls: 19.9442, loss_obj_cls: 19.4273, loss_match: 4.2375, loss: 48.6674, grad_norm: 183.9075
2025-06-22 23:20:28,492 - mmdet - INFO - Epoch [1][10300/45697]	lr: 1.250e-05, eta: 2 days, 6:25:12, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.9237, loss_sub_cls: 20.5158, loss_obj_cls: 18.9826, loss_match: 5.1688, loss: 49.5910, grad_norm: 227.2992
2025-06-22 23:20:43,034 - mmdet - INFO - Epoch [1][10350/45697]	lr: 1.250e-05, eta: 2 days, 6:24:59, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.3092, loss_sub_cls: 20.1099, loss_obj_cls: 19.7003, loss_match: 4.7618, loss: 48.8813, grad_norm: 182.9798
2025-06-22 23:20:57,523 - mmdet - INFO - Epoch [1][10400/45697]	lr: 1.250e-05, eta: 2 days, 6:24:43, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 4.7181, loss_sub_cls: 20.1231, loss_obj_cls: 19.2297, loss_match: 4.4853, loss: 48.5562, grad_norm: 191.5117
2025-06-22 23:21:12,040 - mmdet - INFO - Epoch [1][10450/45697]	lr: 1.250e-05, eta: 2 days, 6:24:29, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.5933, loss_sub_cls: 19.9936, loss_obj_cls: 19.1935, loss_match: 4.2520, loss: 48.0324, grad_norm: 203.5062
2025-06-22 23:21:26,616 - mmdet - INFO - Epoch [1][10500/45697]	lr: 1.250e-05, eta: 2 days, 6:24:19, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.7268, loss_sub_cls: 20.2073, loss_obj_cls: 18.9408, loss_match: 4.2131, loss: 48.0880, grad_norm: 213.1632
2025-06-22 23:21:40,951 - mmdet - INFO - Epoch [1][10550/45697]	lr: 1.250e-05, eta: 2 days, 6:23:53, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.5374, loss_sub_cls: 20.2499, loss_obj_cls: 19.8566, loss_match: 4.9257, loss: 49.5696, grad_norm: 227.1258
2025-06-22 23:21:55,579 - mmdet - INFO - Epoch [1][10600/45697]	lr: 1.250e-05, eta: 2 days, 6:23:47, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.9440, loss_sub_cls: 19.8412, loss_obj_cls: 19.4526, loss_match: 4.5303, loss: 48.7682, grad_norm: 219.8146
2025-06-22 23:22:09,906 - mmdet - INFO - Epoch [1][10650/45697]	lr: 1.250e-05, eta: 2 days, 6:23:21, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 5.1891, loss_sub_cls: 20.0611, loss_obj_cls: 19.4828, loss_match: 4.7876, loss: 49.5205, grad_norm: 228.0523
2025-06-22 23:22:24,306 - mmdet - INFO - Epoch [1][10700/45697]	lr: 1.250e-05, eta: 2 days, 6:22:59, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.5971, loss_sub_cls: 19.9806, loss_obj_cls: 19.3154, loss_match: 5.0634, loss: 48.9564, grad_norm: 260.6436
2025-06-22 23:22:38,846 - mmdet - INFO - Epoch [1][10750/45697]	lr: 1.250e-05, eta: 2 days, 6:22:47, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.7552, loss_sub_cls: 20.4585, loss_obj_cls: 19.5913, loss_match: 4.7472, loss: 49.5521, grad_norm: 178.7796
2025-06-22 23:22:53,153 - mmdet - INFO - Epoch [1][10800/45697]	lr: 1.250e-05, eta: 2 days, 6:22:20, time: 0.286, data_time: 0.013, memory: 3006, loss_r_cls: 4.9879, loss_sub_cls: 20.5634, loss_obj_cls: 19.5346, loss_match: 4.3458, loss: 49.4316, grad_norm: 217.7390
2025-06-22 23:23:07,601 - mmdet - INFO - Epoch [1][10850/45697]	lr: 1.250e-05, eta: 2 days, 6:22:01, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.0088, loss_sub_cls: 20.5733, loss_obj_cls: 19.3124, loss_match: 4.9444, loss: 49.8389, grad_norm: 228.5798
2025-06-22 23:23:22,211 - mmdet - INFO - Epoch [1][10900/45697]	lr: 1.250e-05, eta: 2 days, 6:21:53, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 4.5193, loss_sub_cls: 19.5586, loss_obj_cls: 19.6848, loss_match: 4.6549, loss: 48.4176, grad_norm: 245.3351
2025-06-22 23:23:36,461 - mmdet - INFO - Epoch [1][10950/45697]	lr: 1.250e-05, eta: 2 days, 6:21:23, time: 0.285, data_time: 0.012, memory: 3006, loss_r_cls: 5.0029, loss_sub_cls: 20.0034, loss_obj_cls: 19.1386, loss_match: 4.7827, loss: 48.9276, grad_norm: 230.5776
2025-06-22 23:23:50,710 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:23:50,710 - mmdet - INFO - Epoch [1][11000/45697]	lr: 1.250e-05, eta: 2 days, 6:20:53, time: 0.285, data_time: 0.012, memory: 3006, loss_r_cls: 5.0056, loss_sub_cls: 20.4636, loss_obj_cls: 19.5444, loss_match: 3.9949, loss: 49.0084, grad_norm: 222.2430
2025-06-22 23:24:05,135 - mmdet - INFO - Epoch [1][11050/45697]	lr: 1.250e-05, eta: 2 days, 6:20:33, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.7615, loss_sub_cls: 20.6684, loss_obj_cls: 19.5003, loss_match: 4.3015, loss: 49.2317, grad_norm: 272.9046
2025-06-22 23:24:19,658 - mmdet - INFO - Epoch [1][11100/45697]	lr: 1.250e-05, eta: 2 days, 6:20:20, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.7540, loss_sub_cls: 19.9694, loss_obj_cls: 19.4793, loss_match: 4.1004, loss: 48.3030, grad_norm: 262.6129
2025-06-22 23:24:34,127 - mmdet - INFO - Epoch [1][11150/45697]	lr: 1.250e-05, eta: 2 days, 6:20:03, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.0093, loss_sub_cls: 21.2023, loss_obj_cls: 19.4800, loss_match: 3.7762, loss: 49.4678, grad_norm: 221.2343
2025-06-22 23:24:48,457 - mmdet - INFO - Epoch [1][11200/45697]	lr: 1.250e-05, eta: 2 days, 6:19:38, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 5.0027, loss_sub_cls: 20.6025, loss_obj_cls: 19.5807, loss_match: 4.1125, loss: 49.2983, grad_norm: 265.2014
2025-06-22 23:25:03,451 - mmdet - INFO - Epoch [1][11250/45697]	lr: 1.250e-05, eta: 2 days, 6:19:53, time: 0.300, data_time: 0.013, memory: 3006, loss_r_cls: 4.8684, loss_sub_cls: 20.1953, loss_obj_cls: 19.6266, loss_match: 4.7572, loss: 49.4474, grad_norm: 281.0010
2025-06-22 23:25:18,014 - mmdet - INFO - Epoch [1][11300/45697]	lr: 1.250e-05, eta: 2 days, 6:19:42, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.8102, loss_sub_cls: 20.4805, loss_obj_cls: 19.5234, loss_match: 4.1484, loss: 48.9625, grad_norm: 235.6171
2025-06-22 23:25:32,378 - mmdet - INFO - Epoch [1][11350/45697]	lr: 1.250e-05, eta: 2 days, 6:19:19, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.9555, loss_sub_cls: 20.4157, loss_obj_cls: 18.9890, loss_match: 4.0571, loss: 48.4174, grad_norm: 259.0773
2025-06-22 23:25:46,779 - mmdet - INFO - Epoch [1][11400/45697]	lr: 1.250e-05, eta: 2 days, 6:18:58, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.9532, loss_sub_cls: 19.9055, loss_obj_cls: 19.5045, loss_match: 3.6455, loss: 48.0086, grad_norm: 205.1190
2025-06-22 23:26:01,314 - mmdet - INFO - Epoch [1][11450/45697]	lr: 1.250e-05, eta: 2 days, 6:18:46, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 5.0221, loss_sub_cls: 19.8957, loss_obj_cls: 19.5761, loss_match: 3.7034, loss: 48.1973, grad_norm: 208.4623
2025-06-22 23:26:15,806 - mmdet - INFO - Epoch [1][11500/45697]	lr: 1.250e-05, eta: 2 days, 6:18:30, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 4.6097, loss_sub_cls: 20.3853, loss_obj_cls: 19.3228, loss_match: 3.5750, loss: 47.8928, grad_norm: 256.4540
2025-06-22 23:26:30,297 - mmdet - INFO - Epoch [1][11550/45697]	lr: 1.250e-05, eta: 2 days, 6:18:15, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.6117, loss_sub_cls: 20.3037, loss_obj_cls: 19.2823, loss_match: 4.8397, loss: 49.0375, grad_norm: 273.9525
2025-06-22 23:26:44,925 - mmdet - INFO - Epoch [1][11600/45697]	lr: 1.250e-05, eta: 2 days, 6:18:08, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.9409, loss_sub_cls: 20.4923, loss_obj_cls: 19.4421, loss_match: 4.8167, loss: 49.6920, grad_norm: 251.0243
2025-06-22 23:26:59,653 - mmdet - INFO - Epoch [1][11650/45697]	lr: 1.250e-05, eta: 2 days, 6:18:06, time: 0.295, data_time: 0.015, memory: 3006, loss_r_cls: 4.4483, loss_sub_cls: 20.5761, loss_obj_cls: 19.5403, loss_match: 4.6521, loss: 49.2168, grad_norm: 262.1114
2025-06-22 23:27:14,304 - mmdet - INFO - Epoch [1][11700/45697]	lr: 1.250e-05, eta: 2 days, 6:18:00, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 5.0160, loss_sub_cls: 20.8007, loss_obj_cls: 19.4572, loss_match: 4.1229, loss: 49.3967, grad_norm: 223.6822
2025-06-22 23:27:28,691 - mmdet - INFO - Epoch [1][11750/45697]	lr: 1.250e-05, eta: 2 days, 6:17:39, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.8454, loss_sub_cls: 20.0695, loss_obj_cls: 19.1980, loss_match: 5.0330, loss: 49.1459, grad_norm: 254.1647
2025-06-22 23:27:43,342 - mmdet - INFO - Epoch [1][11800/45697]	lr: 1.250e-05, eta: 2 days, 6:17:32, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 4.2492, loss_sub_cls: 20.3594, loss_obj_cls: 19.2550, loss_match: 4.6339, loss: 48.4974, grad_norm: 209.9018
2025-06-22 23:27:57,964 - mmdet - INFO - Epoch [1][11850/45697]	lr: 1.250e-05, eta: 2 days, 6:17:24, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 5.1330, loss_sub_cls: 20.3487, loss_obj_cls: 19.7490, loss_match: 5.1366, loss: 50.3673, grad_norm: 213.3561
2025-06-22 23:28:12,383 - mmdet - INFO - Epoch [1][11900/45697]	lr: 1.250e-05, eta: 2 days, 6:17:05, time: 0.288, data_time: 0.014, memory: 3006, loss_r_cls: 4.7030, loss_sub_cls: 20.2795, loss_obj_cls: 19.7200, loss_match: 4.0051, loss: 48.7075, grad_norm: 199.7822
2025-06-22 23:28:26,898 - mmdet - INFO - Epoch [1][11950/45697]	lr: 1.250e-05, eta: 2 days, 6:16:51, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.7959, loss_sub_cls: 19.6298, loss_obj_cls: 19.4550, loss_match: 3.6948, loss: 47.5755, grad_norm: 220.0097
2025-06-22 23:28:41,286 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:28:41,287 - mmdet - INFO - Epoch [1][12000/45697]	lr: 1.250e-05, eta: 2 days, 6:16:29, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.4423, loss_sub_cls: 19.8861, loss_obj_cls: 19.2219, loss_match: 4.1100, loss: 47.6603, grad_norm: 215.3774
2025-06-22 23:28:55,935 - mmdet - INFO - Epoch [1][12050/45697]	lr: 1.250e-05, eta: 2 days, 6:16:23, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.9778, loss_sub_cls: 20.3672, loss_obj_cls: 19.2095, loss_match: 4.9499, loss: 49.5045, grad_norm: 253.2901
2025-06-22 23:29:10,633 - mmdet - INFO - Epoch [1][12100/45697]	lr: 1.250e-05, eta: 2 days, 6:16:19, time: 0.294, data_time: 0.014, memory: 3006, loss_r_cls: 5.2045, loss_sub_cls: 20.5989, loss_obj_cls: 19.5319, loss_match: 4.3613, loss: 49.6966, grad_norm: 287.9036
2025-06-22 23:29:25,187 - mmdet - INFO - Epoch [1][12150/45697]	lr: 1.250e-05, eta: 2 days, 6:16:07, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.8014, loss_sub_cls: 20.7015, loss_obj_cls: 19.5803, loss_match: 4.1977, loss: 49.2808, grad_norm: 213.1015
2025-06-22 23:29:39,425 - mmdet - INFO - Epoch [1][12200/45697]	lr: 1.250e-05, eta: 2 days, 6:15:38, time: 0.285, data_time: 0.013, memory: 3006, loss_r_cls: 4.9104, loss_sub_cls: 20.4193, loss_obj_cls: 19.7255, loss_match: 4.8227, loss: 49.8778, grad_norm: 243.7358
2025-06-22 23:29:53,769 - mmdet - INFO - Epoch [1][12250/45697]	lr: 1.250e-05, eta: 2 days, 6:15:14, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.7871, loss_sub_cls: 19.9383, loss_obj_cls: 19.1873, loss_match: 4.4352, loss: 48.3479, grad_norm: 191.8196
2025-06-22 23:30:08,357 - mmdet - INFO - Epoch [1][12300/45697]	lr: 1.250e-05, eta: 2 days, 6:15:04, time: 0.292, data_time: 0.013, memory: 3006, loss_r_cls: 5.0394, loss_sub_cls: 20.2266, loss_obj_cls: 19.5603, loss_match: 4.7264, loss: 49.5527, grad_norm: 224.3095
2025-06-22 23:30:22,849 - mmdet - INFO - Epoch [1][12350/45697]	lr: 1.250e-05, eta: 2 days, 6:14:49, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.8605, loss_sub_cls: 20.2989, loss_obj_cls: 19.3491, loss_match: 4.6479, loss: 49.1564, grad_norm: 251.2472
2025-06-22 23:30:37,322 - mmdet - INFO - Epoch [1][12400/45697]	lr: 1.250e-05, eta: 2 days, 6:14:32, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.7274, loss_sub_cls: 19.7563, loss_obj_cls: 19.1557, loss_match: 4.6202, loss: 48.2596, grad_norm: 291.2305
2025-06-22 23:30:51,884 - mmdet - INFO - Epoch [1][12450/45697]	lr: 1.250e-05, eta: 2 days, 6:14:21, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.9593, loss_sub_cls: 19.9941, loss_obj_cls: 19.3537, loss_match: 5.5188, loss: 49.8259, grad_norm: 256.6249
2025-06-22 23:31:06,330 - mmdet - INFO - Epoch [1][12500/45697]	lr: 1.250e-05, eta: 2 days, 6:14:03, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.5065, loss_sub_cls: 19.9467, loss_obj_cls: 19.5878, loss_match: 4.2386, loss: 48.2797, grad_norm: 202.3171
2025-06-22 23:31:20,862 - mmdet - INFO - Epoch [1][12550/45697]	lr: 1.250e-05, eta: 2 days, 6:13:50, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.8969, loss_sub_cls: 19.9178, loss_obj_cls: 19.4288, loss_match: 3.9015, loss: 48.1450, grad_norm: 246.0658
2025-06-22 23:31:35,564 - mmdet - INFO - Epoch [1][12600/45697]	lr: 1.250e-05, eta: 2 days, 6:13:46, time: 0.294, data_time: 0.016, memory: 3006, loss_r_cls: 4.6991, loss_sub_cls: 19.9364, loss_obj_cls: 19.2354, loss_match: 4.2693, loss: 48.1402, grad_norm: 210.6704
2025-06-22 23:31:50,194 - mmdet - INFO - Epoch [1][12650/45697]	lr: 1.250e-05, eta: 2 days, 6:13:38, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 4.9657, loss_sub_cls: 20.2815, loss_obj_cls: 19.3140, loss_match: 4.1605, loss: 48.7217, grad_norm: 213.5495
2025-06-22 23:32:04,637 - mmdet - INFO - Epoch [1][12700/45697]	lr: 1.250e-05, eta: 2 days, 6:13:20, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 5.1543, loss_sub_cls: 20.2737, loss_obj_cls: 19.1901, loss_match: 4.5076, loss: 49.1257, grad_norm: 210.9077
2025-06-22 23:32:19,003 - mmdet - INFO - Epoch [1][12750/45697]	lr: 1.250e-05, eta: 2 days, 6:12:58, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.6736, loss_sub_cls: 19.7240, loss_obj_cls: 18.8677, loss_match: 4.2598, loss: 47.5250, grad_norm: 216.0367
2025-06-22 23:32:33,465 - mmdet - INFO - Epoch [1][12800/45697]	lr: 1.250e-05, eta: 2 days, 6:12:41, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.7178, loss_sub_cls: 19.7922, loss_obj_cls: 19.2734, loss_match: 4.0867, loss: 47.8702, grad_norm: 207.7358
2025-06-22 23:32:47,929 - mmdet - INFO - Epoch [1][12850/45697]	lr: 1.250e-05, eta: 2 days, 6:12:24, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 4.9504, loss_sub_cls: 20.1495, loss_obj_cls: 19.2049, loss_match: 4.0106, loss: 48.3153, grad_norm: 231.5121
2025-06-22 23:33:02,491 - mmdet - INFO - Epoch [1][12900/45697]	lr: 1.250e-05, eta: 2 days, 6:12:13, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.6018, loss_sub_cls: 20.4370, loss_obj_cls: 19.4388, loss_match: 4.1059, loss: 48.5835, grad_norm: 208.1563
2025-06-22 23:33:16,939 - mmdet - INFO - Epoch [1][12950/45697]	lr: 1.250e-05, eta: 2 days, 6:11:55, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.6786, loss_sub_cls: 19.9725, loss_obj_cls: 18.9184, loss_match: 3.9727, loss: 47.5422, grad_norm: 235.1161
2025-06-22 23:33:31,332 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:33:31,332 - mmdet - INFO - Epoch [1][13000/45697]	lr: 1.250e-05, eta: 2 days, 6:11:35, time: 0.288, data_time: 0.014, memory: 3006, loss_r_cls: 5.3349, loss_sub_cls: 19.8538, loss_obj_cls: 19.2454, loss_match: 4.3307, loss: 48.7647, grad_norm: 222.9142
2025-06-22 23:33:45,904 - mmdet - INFO - Epoch [1][13050/45697]	lr: 1.250e-05, eta: 2 days, 6:11:23, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.7231, loss_sub_cls: 20.0825, loss_obj_cls: 19.4175, loss_match: 5.0620, loss: 49.2852, grad_norm: 259.0453
2025-06-22 23:34:00,644 - mmdet - INFO - Epoch [1][13100/45697]	lr: 1.250e-05, eta: 2 days, 6:11:21, time: 0.295, data_time: 0.014, memory: 3006, loss_r_cls: 4.7676, loss_sub_cls: 20.1605, loss_obj_cls: 19.1407, loss_match: 4.5433, loss: 48.6121, grad_norm: 228.1846
2025-06-22 23:34:15,167 - mmdet - INFO - Epoch [1][13150/45697]	lr: 1.250e-05, eta: 2 days, 6:11:07, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 5.0667, loss_sub_cls: 20.0857, loss_obj_cls: 19.3072, loss_match: 4.0231, loss: 48.4827, grad_norm: 215.3789
2025-06-22 23:34:29,821 - mmdet - INFO - Epoch [1][13200/45697]	lr: 1.250e-05, eta: 2 days, 6:11:00, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.8709, loss_sub_cls: 19.9973, loss_obj_cls: 19.3875, loss_match: 4.5832, loss: 48.8389, grad_norm: 244.6806
2025-06-22 23:34:44,228 - mmdet - INFO - Epoch [1][13250/45697]	lr: 1.250e-05, eta: 2 days, 6:10:41, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.6405, loss_sub_cls: 19.9151, loss_obj_cls: 19.5350, loss_match: 4.4098, loss: 48.5004, grad_norm: 194.5792
2025-06-22 23:34:58,671 - mmdet - INFO - Epoch [1][13300/45697]	lr: 1.250e-05, eta: 2 days, 6:10:23, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.6845, loss_sub_cls: 19.7379, loss_obj_cls: 19.2795, loss_match: 4.0185, loss: 47.7204, grad_norm: 214.2399
2025-06-22 23:35:13,117 - mmdet - INFO - Epoch [1][13350/45697]	lr: 1.250e-05, eta: 2 days, 6:10:05, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.7118, loss_sub_cls: 19.6926, loss_obj_cls: 18.8994, loss_match: 5.0171, loss: 48.3209, grad_norm: 221.9985
2025-06-22 23:35:27,596 - mmdet - INFO - Epoch [1][13400/45697]	lr: 1.250e-05, eta: 2 days, 6:09:49, time: 0.290, data_time: 0.012, memory: 3006, loss_r_cls: 4.7819, loss_sub_cls: 20.1102, loss_obj_cls: 19.1380, loss_match: 3.9487, loss: 47.9788, grad_norm: 196.5498
2025-06-22 23:35:41,999 - mmdet - INFO - Epoch [1][13450/45697]	lr: 1.250e-05, eta: 2 days, 6:09:29, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.7836, loss_sub_cls: 20.0425, loss_obj_cls: 19.5555, loss_match: 4.1073, loss: 48.4887, grad_norm: 206.7645
2025-06-22 23:35:56,446 - mmdet - INFO - Epoch [1][13500/45697]	lr: 1.250e-05, eta: 2 days, 6:09:12, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 5.0358, loss_sub_cls: 19.7805, loss_obj_cls: 19.3428, loss_match: 4.7426, loss: 48.9016, grad_norm: 253.6764
2025-06-22 23:36:10,687 - mmdet - INFO - Epoch [1][13550/45697]	lr: 1.250e-05, eta: 2 days, 6:08:44, time: 0.285, data_time: 0.012, memory: 3006, loss_r_cls: 5.0046, loss_sub_cls: 19.9693, loss_obj_cls: 19.2278, loss_match: 3.9027, loss: 48.1043, grad_norm: 187.8345
2025-06-22 23:36:25,154 - mmdet - INFO - Epoch [1][13600/45697]	lr: 1.250e-05, eta: 2 days, 6:08:28, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.6937, loss_sub_cls: 19.5798, loss_obj_cls: 19.4403, loss_match: 4.5799, loss: 48.2936, grad_norm: 214.6711
2025-06-22 23:36:39,636 - mmdet - INFO - Epoch [1][13650/45697]	lr: 1.250e-05, eta: 2 days, 6:08:12, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 5.1461, loss_sub_cls: 20.0544, loss_obj_cls: 19.2312, loss_match: 4.1063, loss: 48.5380, grad_norm: 204.9677
2025-06-22 23:36:54,303 - mmdet - INFO - Epoch [1][13700/45697]	lr: 1.250e-05, eta: 2 days, 6:08:06, time: 0.293, data_time: 0.013, memory: 3006, loss_r_cls: 4.4727, loss_sub_cls: 20.2137, loss_obj_cls: 19.2514, loss_match: 4.4001, loss: 48.3379, grad_norm: 229.9793
2025-06-22 23:37:08,584 - mmdet - INFO - Epoch [1][13750/45697]	lr: 1.250e-05, eta: 2 days, 6:07:40, time: 0.286, data_time: 0.012, memory: 3006, loss_r_cls: 5.2859, loss_sub_cls: 19.8467, loss_obj_cls: 18.9021, loss_match: 3.9467, loss: 47.9814, grad_norm: 203.5172
2025-06-22 23:37:23,156 - mmdet - INFO - Epoch [1][13800/45697]	lr: 1.250e-05, eta: 2 days, 6:07:29, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.7801, loss_sub_cls: 19.3257, loss_obj_cls: 19.3085, loss_match: 3.5459, loss: 46.9602, grad_norm: 230.2445
2025-06-22 23:37:37,414 - mmdet - INFO - Epoch [1][13850/45697]	lr: 1.250e-05, eta: 2 days, 6:07:02, time: 0.285, data_time: 0.013, memory: 3006, loss_r_cls: 4.7273, loss_sub_cls: 19.5036, loss_obj_cls: 18.8734, loss_match: 4.3602, loss: 47.4646, grad_norm: 342.4731
2025-06-22 23:37:51,867 - mmdet - INFO - Epoch [1][13900/45697]	lr: 1.250e-05, eta: 2 days, 6:06:45, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.5917, loss_sub_cls: 19.9750, loss_obj_cls: 19.3217, loss_match: 3.4424, loss: 47.3308, grad_norm: 218.8546
2025-06-22 23:38:06,604 - mmdet - INFO - Epoch [1][13950/45697]	lr: 1.250e-05, eta: 2 days, 6:06:42, time: 0.295, data_time: 0.012, memory: 3006, loss_r_cls: 5.1318, loss_sub_cls: 19.7742, loss_obj_cls: 19.1843, loss_match: 4.1388, loss: 48.2291, grad_norm: 285.8540
2025-06-22 23:38:21,213 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:38:21,214 - mmdet - INFO - Epoch [1][14000/45697]	lr: 1.250e-05, eta: 2 days, 6:06:32, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.6604, loss_sub_cls: 19.4725, loss_obj_cls: 19.3418, loss_match: 4.0737, loss: 47.5484, grad_norm: 298.3222
2025-06-22 23:38:35,445 - mmdet - INFO - Epoch [1][14050/45697]	lr: 1.250e-05, eta: 2 days, 6:06:05, time: 0.285, data_time: 0.013, memory: 3006, loss_r_cls: 4.8984, loss_sub_cls: 19.3877, loss_obj_cls: 18.7517, loss_match: 4.7339, loss: 47.7717, grad_norm: 284.9198
2025-06-22 23:38:49,811 - mmdet - INFO - Epoch [1][14100/45697]	lr: 1.250e-05, eta: 2 days, 6:05:44, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.8300, loss_sub_cls: 19.6881, loss_obj_cls: 19.1222, loss_match: 3.9310, loss: 47.5712, grad_norm: 237.4943
2025-06-22 23:39:04,497 - mmdet - INFO - Epoch [1][14150/45697]	lr: 1.250e-05, eta: 2 days, 6:05:38, time: 0.294, data_time: 0.013, memory: 3006, loss_r_cls: 4.5891, loss_sub_cls: 19.4848, loss_obj_cls: 19.1602, loss_match: 4.9075, loss: 48.1417, grad_norm: 265.8808
2025-06-22 23:39:18,945 - mmdet - INFO - Epoch [1][14200/45697]	lr: 1.250e-05, eta: 2 days, 6:05:21, time: 0.289, data_time: 0.012, memory: 3006, loss_r_cls: 5.0446, loss_sub_cls: 20.0504, loss_obj_cls: 19.0221, loss_match: 3.9263, loss: 48.0434, grad_norm: 228.0625
2025-06-22 23:39:33,571 - mmdet - INFO - Epoch [1][14250/45697]	lr: 1.250e-05, eta: 2 days, 6:05:12, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.7481, loss_sub_cls: 20.0323, loss_obj_cls: 18.9377, loss_match: 4.7275, loss: 48.4456, grad_norm: 278.0091
2025-06-22 23:39:47,989 - mmdet - INFO - Epoch [1][14300/45697]	lr: 1.250e-05, eta: 2 days, 6:04:53, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.9931, loss_sub_cls: 20.0058, loss_obj_cls: 18.9658, loss_match: 3.6505, loss: 47.6153, grad_norm: 198.4255
2025-06-22 23:40:02,669 - mmdet - INFO - Epoch [1][14350/45697]	lr: 1.250e-05, eta: 2 days, 6:04:47, time: 0.294, data_time: 0.014, memory: 3006, loss_r_cls: 4.8034, loss_sub_cls: 19.9075, loss_obj_cls: 19.3242, loss_match: 3.9189, loss: 47.9540, grad_norm: 260.8925
2025-06-22 23:40:17,209 - mmdet - INFO - Epoch [1][14400/45697]	lr: 1.250e-05, eta: 2 days, 6:04:34, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.5113, loss_sub_cls: 20.1206, loss_obj_cls: 19.2263, loss_match: 3.5943, loss: 47.4524, grad_norm: 217.1757
2025-06-22 23:40:31,716 - mmdet - INFO - Epoch [1][14450/45697]	lr: 1.250e-05, eta: 2 days, 6:04:20, time: 0.290, data_time: 0.012, memory: 3006, loss_r_cls: 5.4350, loss_sub_cls: 20.1223, loss_obj_cls: 19.2144, loss_match: 4.6247, loss: 49.3964, grad_norm: 237.6577
2025-06-22 23:40:46,087 - mmdet - INFO - Epoch [1][14500/45697]	lr: 1.250e-05, eta: 2 days, 6:03:59, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.6893, loss_sub_cls: 19.8539, loss_obj_cls: 18.7891, loss_match: 4.0849, loss: 47.4172, grad_norm: 244.7213
2025-06-22 23:41:00,424 - mmdet - INFO - Epoch [1][14550/45697]	lr: 1.250e-05, eta: 2 days, 6:03:37, time: 0.287, data_time: 0.012, memory: 3006, loss_r_cls: 4.9683, loss_sub_cls: 19.6183, loss_obj_cls: 19.3801, loss_match: 3.9817, loss: 47.9485, grad_norm: 240.9636
2025-06-22 23:41:14,896 - mmdet - INFO - Epoch [1][14600/45697]	lr: 1.250e-05, eta: 2 days, 6:03:21, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 5.0626, loss_sub_cls: 19.8767, loss_obj_cls: 18.5431, loss_match: 3.2687, loss: 46.7512, grad_norm: 233.2062
2025-06-22 23:41:29,281 - mmdet - INFO - Epoch [1][14650/45697]	lr: 1.250e-05, eta: 2 days, 6:03:01, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.9304, loss_sub_cls: 19.6290, loss_obj_cls: 19.1636, loss_match: 4.0549, loss: 47.7779, grad_norm: 222.2074
2025-06-22 23:41:43,702 - mmdet - INFO - Epoch [1][14700/45697]	lr: 1.250e-05, eta: 2 days, 6:02:43, time: 0.288, data_time: 0.014, memory: 3006, loss_r_cls: 4.5994, loss_sub_cls: 19.4983, loss_obj_cls: 19.1408, loss_match: 4.3844, loss: 47.6229, grad_norm: 196.5479
2025-06-22 23:41:58,264 - mmdet - INFO - Epoch [1][14750/45697]	lr: 1.250e-05, eta: 2 days, 6:02:31, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 5.2244, loss_sub_cls: 19.9168, loss_obj_cls: 19.0540, loss_match: 4.2911, loss: 48.4863, grad_norm: 214.3918
2025-06-22 23:42:12,809 - mmdet - INFO - Epoch [1][14800/45697]	lr: 1.250e-05, eta: 2 days, 6:02:18, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.6005, loss_sub_cls: 19.8917, loss_obj_cls: 19.0218, loss_match: 4.0355, loss: 47.5495, grad_norm: 197.7922
2025-06-22 23:42:27,418 - mmdet - INFO - Epoch [1][14850/45697]	lr: 1.250e-05, eta: 2 days, 6:02:08, time: 0.292, data_time: 0.015, memory: 3006, loss_r_cls: 4.7921, loss_sub_cls: 19.5056, loss_obj_cls: 19.0219, loss_match: 5.2005, loss: 48.5201, grad_norm: 218.4797
2025-06-22 23:42:41,970 - mmdet - INFO - Epoch [1][14900/45697]	lr: 1.250e-05, eta: 2 days, 6:01:56, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 5.1722, loss_sub_cls: 19.8288, loss_obj_cls: 19.3588, loss_match: 3.8084, loss: 48.1681, grad_norm: 194.6087
2025-06-22 23:42:56,498 - mmdet - INFO - Epoch [1][14950/45697]	lr: 1.250e-05, eta: 2 days, 6:01:42, time: 0.291, data_time: 0.012, memory: 3006, loss_r_cls: 4.8147, loss_sub_cls: 20.1640, loss_obj_cls: 19.8250, loss_match: 3.9834, loss: 48.7871, grad_norm: 235.3920
2025-06-22 23:43:10,960 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:43:10,960 - mmdet - INFO - Epoch [1][15000/45697]	lr: 1.250e-05, eta: 2 days, 6:01:26, time: 0.289, data_time: 0.013, memory: 3006, loss_r_cls: 4.9274, loss_sub_cls: 20.1462, loss_obj_cls: 19.0782, loss_match: 3.7844, loss: 47.9361, grad_norm: 232.7548
2025-06-22 23:43:25,336 - mmdet - INFO - Epoch [1][15050/45697]	lr: 1.250e-05, eta: 2 days, 6:01:06, time: 0.288, data_time: 0.013, memory: 3006, loss_r_cls: 4.6226, loss_sub_cls: 19.6786, loss_obj_cls: 19.1636, loss_match: 3.1685, loss: 46.6333, grad_norm: 202.9833
2025-06-22 23:43:39,951 - mmdet - INFO - Epoch [1][15100/45697]	lr: 1.250e-05, eta: 2 days, 6:00:56, time: 0.292, data_time: 0.014, memory: 3006, loss_r_cls: 4.9244, loss_sub_cls: 19.7558, loss_obj_cls: 18.9310, loss_match: 4.6829, loss: 48.2941, grad_norm: 304.4587
2025-06-22 23:43:54,511 - mmdet - INFO - Epoch [1][15150/45697]	lr: 1.250e-05, eta: 2 days, 6:00:44, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.9208, loss_sub_cls: 19.5501, loss_obj_cls: 19.3828, loss_match: 4.1161, loss: 47.9698, grad_norm: 226.2288
2025-06-22 23:44:09,328 - mmdet - INFO - Epoch [1][15200/45697]	lr: 1.250e-05, eta: 2 days, 6:00:43, time: 0.296, data_time: 0.013, memory: 3006, loss_r_cls: 4.8306, loss_sub_cls: 19.7677, loss_obj_cls: 18.9707, loss_match: 4.2338, loss: 47.8028, grad_norm: 246.0834
2025-06-22 23:44:23,891 - mmdet - INFO - Epoch [1][15250/45697]	lr: 1.250e-05, eta: 2 days, 6:00:31, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.7030, loss_sub_cls: 20.0178, loss_obj_cls: 19.2267, loss_match: 4.8790, loss: 48.8265, grad_norm: 264.6239
2025-06-22 23:44:38,423 - mmdet - INFO - Epoch [1][15300/45697]	lr: 1.250e-05, eta: 2 days, 6:00:18, time: 0.291, data_time: 0.014, memory: 3006, loss_r_cls: 4.9171, loss_sub_cls: 20.1316, loss_obj_cls: 19.5507, loss_match: 4.4657, loss: 49.0651, grad_norm: 212.4322
2025-06-22 23:44:53,174 - mmdet - INFO - Epoch [1][15350/45697]	lr: 1.250e-05, eta: 2 days, 6:00:14, time: 0.295, data_time: 0.013, memory: 3006, loss_r_cls: 5.5794, loss_sub_cls: 20.1235, loss_obj_cls: 19.4678, loss_match: 4.1029, loss: 49.2735, grad_norm: 211.1911
2025-06-22 23:45:07,813 - mmdet - INFO - Epoch [1][15400/45697]	lr: 1.250e-05, eta: 2 days, 6:00:06, time: 0.293, data_time: 0.012, memory: 3006, loss_r_cls: 4.3699, loss_sub_cls: 19.8990, loss_obj_cls: 19.4190, loss_match: 3.3873, loss: 47.0752, grad_norm: 205.5542
2025-06-22 23:45:22,192 - mmdet - INFO - Epoch [1][15450/45697]	lr: 1.250e-05, eta: 2 days, 5:59:46, time: 0.288, data_time: 0.011, memory: 3006, loss_r_cls: 4.5878, loss_sub_cls: 20.1922, loss_obj_cls: 19.0567, loss_match: 3.7991, loss: 47.6358, grad_norm: 218.3516
2025-06-22 23:45:36,689 - mmdet - INFO - Epoch [1][15500/45697]	lr: 1.250e-05, eta: 2 days, 5:59:31, time: 0.290, data_time: 0.014, memory: 3006, loss_r_cls: 4.8951, loss_sub_cls: 20.4416, loss_obj_cls: 19.4237, loss_match: 4.3129, loss: 49.0733, grad_norm: 244.8180
2025-06-22 23:45:51,412 - mmdet - INFO - Epoch [1][15550/45697]	lr: 1.250e-05, eta: 2 days, 5:59:25, time: 0.294, data_time: 0.015, memory: 3006, loss_r_cls: 5.2115, loss_sub_cls: 19.9865, loss_obj_cls: 19.6114, loss_match: 3.5579, loss: 48.3673, grad_norm: 222.9027
2025-06-22 23:46:05,931 - mmdet - INFO - Epoch [1][15600/45697]	lr: 1.250e-05, eta: 2 days, 5:59:11, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 4.3951, loss_sub_cls: 19.8623, loss_obj_cls: 19.4152, loss_match: 3.4147, loss: 47.0874, grad_norm: 241.2432
2025-06-22 23:46:20,579 - mmdet - INFO - Epoch [1][15650/45697]	lr: 1.250e-05, eta: 2 days, 5:59:03, time: 0.293, data_time: 0.014, memory: 3006, loss_r_cls: 4.8406, loss_sub_cls: 20.2456, loss_obj_cls: 19.0213, loss_match: 4.2997, loss: 48.4072, grad_norm: 309.2186
2025-06-22 23:46:34,998 - mmdet - INFO - Epoch [1][15700/45697]	lr: 1.250e-05, eta: 2 days, 5:58:45, time: 0.288, data_time: 0.012, memory: 3006, loss_r_cls: 4.3866, loss_sub_cls: 20.0945, loss_obj_cls: 19.0100, loss_match: 3.4671, loss: 46.9583, grad_norm: 228.6461
2025-06-22 23:46:49,519 - mmdet - INFO - Epoch [1][15750/45697]	lr: 1.250e-05, eta: 2 days, 5:58:31, time: 0.290, data_time: 0.013, memory: 3006, loss_r_cls: 5.1540, loss_sub_cls: 20.2498, loss_obj_cls: 18.9451, loss_match: 4.4892, loss: 48.8381, grad_norm: 227.3680
2025-06-22 23:47:03,846 - mmdet - INFO - Epoch [1][15800/45697]	lr: 1.250e-05, eta: 2 days, 5:58:09, time: 0.287, data_time: 0.013, memory: 3006, loss_r_cls: 4.8542, loss_sub_cls: 20.2779, loss_obj_cls: 19.1638, loss_match: 4.6645, loss: 48.9604, grad_norm: 212.9684
2025-06-22 23:47:18,372 - mmdet - INFO - Epoch [1][15850/45697]	lr: 1.250e-05, eta: 2 days, 5:57:55, time: 0.291, data_time: 0.013, memory: 3006, loss_r_cls: 4.9637, loss_sub_cls: 19.8821, loss_obj_cls: 19.1346, loss_match: 4.1376, loss: 48.1180, grad_norm: 205.7715
2025-06-22 23:47:32,644 - mmdet - INFO - Epoch [1][15900/45697]	lr: 1.250e-05, eta: 2 days, 5:57:30, time: 0.285, data_time: 0.013, memory: 3006, loss_r_cls: 4.8176, loss_sub_cls: 19.8334, loss_obj_cls: 19.4932, loss_match: 3.7544, loss: 47.8986, grad_norm: 199.8552
2025-06-22 23:47:47,087 - mmdet - INFO - Epoch [1][15950/45697]	lr: 1.250e-05, eta: 2 days, 5:57:13, time: 0.289, data_time: 0.014, memory: 3006, loss_r_cls: 4.9490, loss_sub_cls: 20.3528, loss_obj_cls: 19.0540, loss_match: 4.0531, loss: 48.4089, grad_norm: 206.8631
2025-06-22 23:48:01,644 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:48:01,644 - mmdet - INFO - Epoch [1][16000/45697]	lr: 1.250e-05, eta: 2 days, 5:57:01, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.3636, loss_sub_cls: 20.4133, loss_obj_cls: 19.3592, loss_match: 4.1760, loss: 48.3122, grad_norm: 264.0857
2025-06-22 23:48:16,087 - mmdet - INFO - Epoch [1][16050/45697]	lr: 1.250e-05, eta: 2 days, 5:56:44, time: 0.289, data_time: 0.012, memory: 3008, loss_r_cls: 4.5029, loss_sub_cls: 20.7281, loss_obj_cls: 19.7743, loss_match: 3.4607, loss: 48.4660, grad_norm: 218.6768
2025-06-22 23:48:30,464 - mmdet - INFO - Epoch [1][16100/45697]	lr: 1.250e-05, eta: 2 days, 5:56:24, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 5.5150, loss_sub_cls: 20.0420, loss_obj_cls: 19.2424, loss_match: 4.3233, loss: 49.1227, grad_norm: 240.6263
2025-06-22 23:48:44,830 - mmdet - INFO - Epoch [1][16150/45697]	lr: 1.250e-05, eta: 2 days, 5:56:04, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.8448, loss_sub_cls: 19.8603, loss_obj_cls: 19.4715, loss_match: 4.2606, loss: 48.4373, grad_norm: 260.1604
2025-06-22 23:48:59,330 - mmdet - INFO - Epoch [1][16200/45697]	lr: 1.250e-05, eta: 2 days, 5:55:49, time: 0.290, data_time: 0.014, memory: 3008, loss_r_cls: 5.0042, loss_sub_cls: 19.3545, loss_obj_cls: 18.9630, loss_match: 3.4212, loss: 46.7429, grad_norm: 178.9089
2025-06-22 23:49:13,538 - mmdet - INFO - Epoch [1][16250/45697]	lr: 1.250e-05, eta: 2 days, 5:55:22, time: 0.284, data_time: 0.011, memory: 3008, loss_r_cls: 4.5021, loss_sub_cls: 20.2248, loss_obj_cls: 19.1471, loss_match: 4.5385, loss: 48.4125, grad_norm: 241.7275
2025-06-22 23:49:27,875 - mmdet - INFO - Epoch [1][16300/45697]	lr: 1.250e-05, eta: 2 days, 5:55:01, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.7068, loss_sub_cls: 19.6308, loss_obj_cls: 19.0975, loss_match: 4.4839, loss: 47.9190, grad_norm: 264.5946
2025-06-22 23:49:42,048 - mmdet - INFO - Epoch [1][16350/45697]	lr: 1.250e-05, eta: 2 days, 5:54:33, time: 0.283, data_time: 0.013, memory: 3008, loss_r_cls: 4.2851, loss_sub_cls: 20.1544, loss_obj_cls: 19.6310, loss_match: 4.6165, loss: 48.6870, grad_norm: 286.0841
2025-06-22 23:49:56,582 - mmdet - INFO - Epoch [1][16400/45697]	lr: 1.250e-05, eta: 2 days, 5:54:19, time: 0.291, data_time: 0.013, memory: 3008, loss_r_cls: 4.8178, loss_sub_cls: 20.3360, loss_obj_cls: 19.4960, loss_match: 3.9065, loss: 48.5563, grad_norm: 218.3081
2025-06-22 23:50:10,955 - mmdet - INFO - Epoch [1][16450/45697]	lr: 1.250e-05, eta: 2 days, 5:54:00, time: 0.288, data_time: 0.014, memory: 3008, loss_r_cls: 4.9904, loss_sub_cls: 20.2400, loss_obj_cls: 19.6135, loss_match: 3.6663, loss: 48.5103, grad_norm: 213.9579
2025-06-22 23:50:25,341 - mmdet - INFO - Epoch [1][16500/45697]	lr: 1.250e-05, eta: 2 days, 5:53:40, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 5.1645, loss_sub_cls: 20.4486, loss_obj_cls: 19.3341, loss_match: 3.5919, loss: 48.5392, grad_norm: 244.4349
2025-06-22 23:50:39,742 - mmdet - INFO - Epoch [1][16550/45697]	lr: 1.250e-05, eta: 2 days, 5:53:22, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.7561, loss_sub_cls: 20.5003, loss_obj_cls: 19.0596, loss_match: 4.0975, loss: 48.4135, grad_norm: 299.8375
2025-06-22 23:50:54,109 - mmdet - INFO - Epoch [1][16600/45697]	lr: 1.250e-05, eta: 2 days, 5:53:02, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.8412, loss_sub_cls: 20.1957, loss_obj_cls: 19.4319, loss_match: 4.3985, loss: 48.8673, grad_norm: 240.4137
2025-06-22 23:51:08,567 - mmdet - INFO - Epoch [1][16650/45697]	lr: 1.250e-05, eta: 2 days, 5:52:46, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.5426, loss_sub_cls: 20.2763, loss_obj_cls: 19.4294, loss_match: 3.6434, loss: 47.8917, grad_norm: 221.9266
2025-06-22 23:51:23,160 - mmdet - INFO - Epoch [1][16700/45697]	lr: 1.250e-05, eta: 2 days, 5:52:35, time: 0.292, data_time: 0.014, memory: 3008, loss_r_cls: 5.0135, loss_sub_cls: 19.7648, loss_obj_cls: 19.5174, loss_match: 3.9380, loss: 48.2336, grad_norm: 225.3206
2025-06-22 23:51:37,288 - mmdet - INFO - Epoch [1][16750/45697]	lr: 1.250e-05, eta: 2 days, 5:52:05, time: 0.283, data_time: 0.012, memory: 3008, loss_r_cls: 4.6961, loss_sub_cls: 20.2126, loss_obj_cls: 19.1551, loss_match: 3.8544, loss: 47.9181, grad_norm: 192.5991
2025-06-22 23:51:51,726 - mmdet - INFO - Epoch [1][16800/45697]	lr: 1.250e-05, eta: 2 days, 5:51:48, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 5.0942, loss_sub_cls: 19.9019, loss_obj_cls: 19.5344, loss_match: 4.9644, loss: 49.4949, grad_norm: 273.4880
2025-06-22 23:52:06,048 - mmdet - INFO - Epoch [1][16850/45697]	lr: 1.250e-05, eta: 2 days, 5:51:27, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.9902, loss_sub_cls: 19.7378, loss_obj_cls: 18.8470, loss_match: 3.9870, loss: 47.5620, grad_norm: 253.3458
2025-06-22 23:52:20,661 - mmdet - INFO - Epoch [1][16900/45697]	lr: 1.250e-05, eta: 2 days, 5:51:17, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 4.6094, loss_sub_cls: 20.0420, loss_obj_cls: 19.2220, loss_match: 4.1113, loss: 47.9846, grad_norm: 186.3234
2025-06-22 23:52:35,031 - mmdet - INFO - Epoch [1][16950/45697]	lr: 1.250e-05, eta: 2 days, 5:50:57, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 4.5747, loss_sub_cls: 20.0829, loss_obj_cls: 19.4533, loss_match: 3.8679, loss: 47.9788, grad_norm: 209.7189
2025-06-22 23:52:49,134 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:52:49,135 - mmdet - INFO - Epoch [1][17000/45697]	lr: 1.250e-05, eta: 2 days, 5:50:27, time: 0.282, data_time: 0.012, memory: 3008, loss_r_cls: 4.7270, loss_sub_cls: 20.0580, loss_obj_cls: 18.9018, loss_match: 4.4428, loss: 48.1295, grad_norm: 235.7973
2025-06-22 23:53:03,099 - mmdet - INFO - Epoch [1][17050/45697]	lr: 1.250e-05, eta: 2 days, 5:49:52, time: 0.279, data_time: 0.013, memory: 3008, loss_r_cls: 4.3931, loss_sub_cls: 19.6005, loss_obj_cls: 19.2056, loss_match: 3.9540, loss: 47.1532, grad_norm: 172.0387
2025-06-22 23:53:17,607 - mmdet - INFO - Epoch [1][17100/45697]	lr: 1.250e-05, eta: 2 days, 5:49:38, time: 0.290, data_time: 0.015, memory: 3008, loss_r_cls: 5.1093, loss_sub_cls: 19.7407, loss_obj_cls: 19.4786, loss_match: 3.7419, loss: 48.0705, grad_norm: 223.1054
2025-06-22 23:53:32,029 - mmdet - INFO - Epoch [1][17150/45697]	lr: 1.250e-05, eta: 2 days, 5:49:20, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.6769, loss_sub_cls: 19.7965, loss_obj_cls: 19.2971, loss_match: 3.9206, loss: 47.6911, grad_norm: 240.1155
2025-06-22 23:53:46,350 - mmdet - INFO - Epoch [1][17200/45697]	lr: 1.250e-05, eta: 2 days, 5:48:59, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.8443, loss_sub_cls: 20.0411, loss_obj_cls: 19.4439, loss_match: 3.8486, loss: 48.1779, grad_norm: 210.4854
2025-06-22 23:54:00,530 - mmdet - INFO - Epoch [1][17250/45697]	lr: 1.250e-05, eta: 2 days, 5:48:32, time: 0.284, data_time: 0.013, memory: 3008, loss_r_cls: 4.8397, loss_sub_cls: 19.6661, loss_obj_cls: 19.3765, loss_match: 4.2145, loss: 48.0967, grad_norm: 230.5211
2025-06-22 23:54:14,870 - mmdet - INFO - Epoch [1][17300/45697]	lr: 1.250e-05, eta: 2 days, 5:48:12, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.8358, loss_sub_cls: 20.3097, loss_obj_cls: 19.1810, loss_match: 4.1619, loss: 48.4885, grad_norm: 211.4755
2025-06-22 23:54:29,278 - mmdet - INFO - Epoch [1][17350/45697]	lr: 1.250e-05, eta: 2 days, 5:47:54, time: 0.288, data_time: 0.016, memory: 3008, loss_r_cls: 4.8119, loss_sub_cls: 19.5723, loss_obj_cls: 19.0930, loss_match: 4.7540, loss: 48.2313, grad_norm: 195.4681
2025-06-22 23:54:43,636 - mmdet - INFO - Epoch [1][17400/45697]	lr: 1.250e-05, eta: 2 days, 5:47:34, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 4.8635, loss_sub_cls: 19.8089, loss_obj_cls: 19.2647, loss_match: 3.8921, loss: 47.8292, grad_norm: 187.3259
2025-06-22 23:54:58,035 - mmdet - INFO - Epoch [1][17450/45697]	lr: 1.250e-05, eta: 2 days, 5:47:16, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.5826, loss_sub_cls: 19.8801, loss_obj_cls: 19.2557, loss_match: 4.1663, loss: 47.8846, grad_norm: 202.5454
2025-06-22 23:55:12,476 - mmdet - INFO - Epoch [1][17500/45697]	lr: 1.250e-05, eta: 2 days, 5:46:59, time: 0.289, data_time: 0.015, memory: 3008, loss_r_cls: 4.5306, loss_sub_cls: 20.0408, loss_obj_cls: 19.3242, loss_match: 4.6159, loss: 48.5114, grad_norm: 219.4687
2025-06-22 23:55:26,748 - mmdet - INFO - Epoch [1][17550/45697]	lr: 1.250e-05, eta: 2 days, 5:46:36, time: 0.285, data_time: 0.014, memory: 3008, loss_r_cls: 4.7717, loss_sub_cls: 19.5809, loss_obj_cls: 19.5820, loss_match: 4.1819, loss: 48.1166, grad_norm: 208.4616
2025-06-22 23:55:41,298 - mmdet - INFO - Epoch [1][17600/45697]	lr: 1.250e-05, eta: 2 days, 5:46:24, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.9439, loss_sub_cls: 19.6431, loss_obj_cls: 19.4341, loss_match: 4.5980, loss: 48.6190, grad_norm: 192.2157
2025-06-22 23:55:55,618 - mmdet - INFO - Epoch [1][17650/45697]	lr: 1.250e-05, eta: 2 days, 5:46:03, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.6310, loss_sub_cls: 19.9164, loss_obj_cls: 19.1633, loss_match: 3.9922, loss: 47.7028, grad_norm: 168.2918
2025-06-22 23:56:10,393 - mmdet - INFO - Epoch [1][17700/45697]	lr: 1.250e-05, eta: 2 days, 5:45:59, time: 0.295, data_time: 0.013, memory: 3008, loss_r_cls: 4.7533, loss_sub_cls: 20.0669, loss_obj_cls: 19.3564, loss_match: 3.1055, loss: 47.2821, grad_norm: 187.0140
2025-06-22 23:56:25,019 - mmdet - INFO - Epoch [1][17750/45697]	lr: 1.250e-05, eta: 2 days, 5:45:50, time: 0.293, data_time: 0.013, memory: 3008, loss_r_cls: 4.7410, loss_sub_cls: 19.9756, loss_obj_cls: 18.9594, loss_match: 3.6876, loss: 47.3636, grad_norm: 218.7073
2025-06-22 23:56:39,621 - mmdet - INFO - Epoch [1][17800/45697]	lr: 1.250e-05, eta: 2 days, 5:45:39, time: 0.292, data_time: 0.014, memory: 3008, loss_r_cls: 4.4793, loss_sub_cls: 19.6591, loss_obj_cls: 19.2703, loss_match: 3.8278, loss: 47.2364, grad_norm: 245.2528
2025-06-22 23:56:53,864 - mmdet - INFO - Epoch [1][17850/45697]	lr: 1.250e-05, eta: 2 days, 5:45:15, time: 0.285, data_time: 0.013, memory: 3008, loss_r_cls: 4.9959, loss_sub_cls: 19.7560, loss_obj_cls: 19.0923, loss_match: 3.4057, loss: 47.2499, grad_norm: 200.2210
2025-06-22 23:57:08,147 - mmdet - INFO - Epoch [1][17900/45697]	lr: 1.250e-05, eta: 2 days, 5:44:53, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.5456, loss_sub_cls: 19.9010, loss_obj_cls: 19.2286, loss_match: 3.7595, loss: 47.4347, grad_norm: 214.6449
2025-06-22 23:57:22,504 - mmdet - INFO - Epoch [1][17950/45697]	lr: 1.250e-05, eta: 2 days, 5:44:34, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.8083, loss_sub_cls: 19.8807, loss_obj_cls: 19.5793, loss_match: 3.5896, loss: 47.8579, grad_norm: 253.0648
2025-06-22 23:57:36,894 - mmdet - INFO - Exp name: pairnet.py
2025-06-22 23:57:36,894 - mmdet - INFO - Epoch [1][18000/45697]	lr: 1.250e-05, eta: 2 days, 5:44:15, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.9834, loss_sub_cls: 19.6770, loss_obj_cls: 19.0935, loss_match: 3.8330, loss: 47.5869, grad_norm: 220.4380
2025-06-22 23:57:51,463 - mmdet - INFO - Epoch [1][18050/45697]	lr: 1.250e-05, eta: 2 days, 5:44:04, time: 0.291, data_time: 0.015, memory: 3008, loss_r_cls: 4.9424, loss_sub_cls: 19.8807, loss_obj_cls: 19.5963, loss_match: 3.8821, loss: 48.3015, grad_norm: 182.6496
2025-06-22 23:58:05,872 - mmdet - INFO - Epoch [1][18100/45697]	lr: 1.250e-05, eta: 2 days, 5:43:46, time: 0.288, data_time: 0.014, memory: 3008, loss_r_cls: 4.6919, loss_sub_cls: 20.1351, loss_obj_cls: 19.2694, loss_match: 4.8773, loss: 48.9737, grad_norm: 223.0437
2025-06-22 23:58:20,254 - mmdet - INFO - Epoch [1][18150/45697]	lr: 1.250e-05, eta: 2 days, 5:43:27, time: 0.288, data_time: 0.014, memory: 3008, loss_r_cls: 4.8742, loss_sub_cls: 19.7912, loss_obj_cls: 19.0975, loss_match: 3.3373, loss: 47.1002, grad_norm: 189.8198
2025-06-22 23:58:34,702 - mmdet - INFO - Epoch [1][18200/45697]	lr: 1.250e-05, eta: 2 days, 5:43:11, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.6662, loss_sub_cls: 19.5269, loss_obj_cls: 18.9668, loss_match: 4.1861, loss: 47.3460, grad_norm: 272.4672
2025-06-22 23:58:49,354 - mmdet - INFO - Epoch [1][18250/45697]	lr: 1.250e-05, eta: 2 days, 5:43:03, time: 0.293, data_time: 0.014, memory: 3008, loss_r_cls: 4.7015, loss_sub_cls: 19.4329, loss_obj_cls: 19.2973, loss_match: 3.8560, loss: 47.2876, grad_norm: 254.5706
2025-06-22 23:59:03,716 - mmdet - INFO - Epoch [1][18300/45697]	lr: 1.250e-05, eta: 2 days, 5:42:43, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.9894, loss_sub_cls: 19.7569, loss_obj_cls: 19.4931, loss_match: 3.9077, loss: 48.1471, grad_norm: 245.7284
2025-06-22 23:59:18,152 - mmdet - INFO - Epoch [1][18350/45697]	lr: 1.250e-05, eta: 2 days, 5:42:27, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.9788, loss_sub_cls: 19.7165, loss_obj_cls: 19.1024, loss_match: 3.8544, loss: 47.6521, grad_norm: 223.0398
2025-06-22 23:59:32,232 - mmdet - INFO - Epoch [1][18400/45697]	lr: 1.250e-05, eta: 2 days, 5:41:58, time: 0.282, data_time: 0.014, memory: 3008, loss_r_cls: 4.8388, loss_sub_cls: 19.8046, loss_obj_cls: 19.1099, loss_match: 3.9800, loss: 47.7333, grad_norm: 270.5064
2025-06-22 23:59:46,676 - mmdet - INFO - Epoch [1][18450/45697]	lr: 1.250e-05, eta: 2 days, 5:41:41, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.7279, loss_sub_cls: 19.6865, loss_obj_cls: 19.1516, loss_match: 3.6953, loss: 47.2613, grad_norm: 199.3362
2025-06-23 00:00:01,053 - mmdet - INFO - Epoch [1][18500/45697]	lr: 1.250e-05, eta: 2 days, 5:41:23, time: 0.288, data_time: 0.016, memory: 3008, loss_r_cls: 4.7919, loss_sub_cls: 19.6694, loss_obj_cls: 19.0354, loss_match: 4.2141, loss: 47.7108, grad_norm: 217.3712
2025-06-23 00:00:15,515 - mmdet - INFO - Epoch [1][18550/45697]	lr: 1.250e-05, eta: 2 days, 5:41:07, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.6529, loss_sub_cls: 19.8589, loss_obj_cls: 19.1732, loss_match: 3.4815, loss: 47.1665, grad_norm: 180.6373
2025-06-23 00:00:29,718 - mmdet - INFO - Epoch [1][18600/45697]	lr: 1.250e-05, eta: 2 days, 5:40:42, time: 0.284, data_time: 0.013, memory: 3008, loss_r_cls: 5.3257, loss_sub_cls: 19.5736, loss_obj_cls: 18.9806, loss_match: 4.1121, loss: 47.9920, grad_norm: 225.9524
2025-06-23 00:00:44,409 - mmdet - INFO - Epoch [1][18650/45697]	lr: 1.250e-05, eta: 2 days, 5:40:35, time: 0.294, data_time: 0.013, memory: 3008, loss_r_cls: 4.9623, loss_sub_cls: 19.9193, loss_obj_cls: 19.5871, loss_match: 3.4384, loss: 47.9072, grad_norm: 184.5436
2025-06-23 00:00:58,900 - mmdet - INFO - Epoch [1][18700/45697]	lr: 1.250e-05, eta: 2 days, 5:40:21, time: 0.290, data_time: 0.011, memory: 3008, loss_r_cls: 4.5115, loss_sub_cls: 19.6767, loss_obj_cls: 19.0693, loss_match: 3.6892, loss: 46.9467, grad_norm: 207.9857
2025-06-23 00:01:13,228 - mmdet - INFO - Epoch [1][18750/45697]	lr: 1.250e-05, eta: 2 days, 5:40:00, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.6795, loss_sub_cls: 19.7300, loss_obj_cls: 19.1842, loss_match: 3.3177, loss: 46.9114, grad_norm: 235.0442
2025-06-23 00:01:27,455 - mmdet - INFO - Epoch [1][18800/45697]	lr: 1.250e-05, eta: 2 days, 5:39:37, time: 0.285, data_time: 0.013, memory: 3008, loss_r_cls: 4.5181, loss_sub_cls: 19.9425, loss_obj_cls: 19.2584, loss_match: 3.6859, loss: 47.4049, grad_norm: 226.7985
2025-06-23 00:01:41,837 - mmdet - INFO - Epoch [1][18850/45697]	lr: 1.250e-05, eta: 2 days, 5:39:18, time: 0.288, data_time: 0.012, memory: 3008, loss_r_cls: 4.2415, loss_sub_cls: 19.8865, loss_obj_cls: 19.1321, loss_match: 3.5768, loss: 46.8369, grad_norm: 242.0656
2025-06-23 00:01:56,318 - mmdet - INFO - Epoch [1][18900/45697]	lr: 1.250e-05, eta: 2 days, 5:39:04, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 5.1221, loss_sub_cls: 19.5981, loss_obj_cls: 19.5300, loss_match: 4.3980, loss: 48.6483, grad_norm: 252.0342
2025-06-23 00:02:10,939 - mmdet - INFO - Epoch [1][18950/45697]	lr: 1.250e-05, eta: 2 days, 5:38:54, time: 0.292, data_time: 0.014, memory: 3008, loss_r_cls: 4.5625, loss_sub_cls: 19.6424, loss_obj_cls: 19.2422, loss_match: 4.5785, loss: 48.0257, grad_norm: 264.2125
2025-06-23 00:02:25,346 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:02:25,346 - mmdet - INFO - Epoch [1][19000/45697]	lr: 1.250e-05, eta: 2 days, 5:38:36, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.8295, loss_sub_cls: 19.9041, loss_obj_cls: 19.3806, loss_match: 3.5057, loss: 47.6199, grad_norm: 225.3553
2025-06-23 00:02:39,545 - mmdet - INFO - Epoch [1][19050/45697]	lr: 1.250e-05, eta: 2 days, 5:38:12, time: 0.284, data_time: 0.012, memory: 3008, loss_r_cls: 4.9932, loss_sub_cls: 20.0683, loss_obj_cls: 19.1551, loss_match: 3.6272, loss: 47.8438, grad_norm: 255.1269
2025-06-23 00:02:53,905 - mmdet - INFO - Epoch [1][19100/45697]	lr: 1.250e-05, eta: 2 days, 5:37:53, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 4.8515, loss_sub_cls: 19.5497, loss_obj_cls: 19.2472, loss_match: 4.4941, loss: 48.1425, grad_norm: 226.7658
2025-06-23 00:03:08,180 - mmdet - INFO - Epoch [1][19150/45697]	lr: 1.250e-05, eta: 2 days, 5:37:31, time: 0.285, data_time: 0.014, memory: 3008, loss_r_cls: 4.8028, loss_sub_cls: 19.9763, loss_obj_cls: 18.9953, loss_match: 3.6974, loss: 47.4719, grad_norm: 189.9020
2025-06-23 00:03:22,573 - mmdet - INFO - Epoch [1][19200/45697]	lr: 1.250e-05, eta: 2 days, 5:37:13, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.6634, loss_sub_cls: 19.8083, loss_obj_cls: 19.3007, loss_match: 4.2186, loss: 47.9910, grad_norm: 199.4084
2025-06-23 00:03:36,767 - mmdet - INFO - Epoch [1][19250/45697]	lr: 1.250e-05, eta: 2 days, 5:36:49, time: 0.284, data_time: 0.012, memory: 3008, loss_r_cls: 5.2157, loss_sub_cls: 19.6344, loss_obj_cls: 19.1932, loss_match: 3.6492, loss: 47.6925, grad_norm: 214.1149
2025-06-23 00:03:50,872 - mmdet - INFO - Epoch [1][19300/45697]	lr: 1.250e-05, eta: 2 days, 5:36:21, time: 0.282, data_time: 0.012, memory: 3008, loss_r_cls: 4.5200, loss_sub_cls: 19.6677, loss_obj_cls: 19.1157, loss_match: 4.0956, loss: 47.3989, grad_norm: 195.3706
2025-06-23 00:04:05,170 - mmdet - INFO - Epoch [1][19350/45697]	lr: 1.250e-05, eta: 2 days, 5:36:00, time: 0.286, data_time: 0.012, memory: 3008, loss_r_cls: 5.1224, loss_sub_cls: 19.6345, loss_obj_cls: 19.0422, loss_match: 4.0500, loss: 47.8492, grad_norm: 236.0480
2025-06-23 00:04:19,586 - mmdet - INFO - Epoch [1][19400/45697]	lr: 1.250e-05, eta: 2 days, 5:35:43, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.8204, loss_sub_cls: 19.6900, loss_obj_cls: 19.0075, loss_match: 3.6383, loss: 47.1562, grad_norm: 263.0952
2025-06-23 00:04:33,814 - mmdet - INFO - Epoch [1][19450/45697]	lr: 1.250e-05, eta: 2 days, 5:35:20, time: 0.285, data_time: 0.013, memory: 3008, loss_r_cls: 4.8385, loss_sub_cls: 20.0523, loss_obj_cls: 19.1307, loss_match: 4.0866, loss: 48.1081, grad_norm: 243.6362
2025-06-23 00:04:48,080 - mmdet - INFO - Epoch [1][19500/45697]	lr: 1.250e-05, eta: 2 days, 5:34:58, time: 0.285, data_time: 0.014, memory: 3008, loss_r_cls: 4.6106, loss_sub_cls: 19.9508, loss_obj_cls: 19.4351, loss_match: 4.4281, loss: 48.4246, grad_norm: 216.5050
2025-06-23 00:05:02,514 - mmdet - INFO - Epoch [1][19550/45697]	lr: 1.250e-05, eta: 2 days, 5:34:42, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.7042, loss_sub_cls: 19.6413, loss_obj_cls: 19.6525, loss_match: 4.4706, loss: 48.4687, grad_norm: 201.6730
2025-06-23 00:05:16,724 - mmdet - INFO - Epoch [1][19600/45697]	lr: 1.250e-05, eta: 2 days, 5:34:18, time: 0.284, data_time: 0.011, memory: 3008, loss_r_cls: 4.0894, loss_sub_cls: 19.9411, loss_obj_cls: 19.2594, loss_match: 3.9760, loss: 47.2659, grad_norm: 199.0667
2025-06-23 00:05:31,015 - mmdet - INFO - Epoch [1][19650/45697]	lr: 1.250e-05, eta: 2 days, 5:33:57, time: 0.286, data_time: 0.012, memory: 3008, loss_r_cls: 5.2004, loss_sub_cls: 20.0565, loss_obj_cls: 19.7857, loss_match: 4.2410, loss: 49.2835, grad_norm: 209.1975
2025-06-23 00:05:45,563 - mmdet - INFO - Epoch [1][19700/45697]	lr: 1.250e-05, eta: 2 days, 5:33:45, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.9707, loss_sub_cls: 19.7525, loss_obj_cls: 19.3873, loss_match: 4.0227, loss: 48.1333, grad_norm: 184.3663
2025-06-23 00:05:59,866 - mmdet - INFO - Epoch [1][19750/45697]	lr: 1.250e-05, eta: 2 days, 5:33:24, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.6441, loss_sub_cls: 19.5617, loss_obj_cls: 19.1745, loss_match: 4.4137, loss: 47.7940, grad_norm: 233.8062
2025-06-23 00:06:13,967 - mmdet - INFO - Epoch [1][19800/45697]	lr: 1.250e-05, eta: 2 days, 5:32:57, time: 0.282, data_time: 0.012, memory: 3008, loss_r_cls: 4.5753, loss_sub_cls: 19.4379, loss_obj_cls: 19.0053, loss_match: 3.9237, loss: 46.9422, grad_norm: 210.8632
2025-06-23 00:06:28,497 - mmdet - INFO - Epoch [1][19850/45697]	lr: 1.250e-05, eta: 2 days, 5:32:44, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.2786, loss_sub_cls: 20.0536, loss_obj_cls: 19.4540, loss_match: 3.1764, loss: 46.9625, grad_norm: 229.9504
2025-06-23 00:06:42,914 - mmdet - INFO - Epoch [1][19900/45697]	lr: 1.250e-05, eta: 2 days, 5:32:28, time: 0.288, data_time: 0.015, memory: 3008, loss_r_cls: 4.8954, loss_sub_cls: 19.9727, loss_obj_cls: 19.3994, loss_match: 3.7000, loss: 47.9674, grad_norm: 234.8446
2025-06-23 00:06:57,320 - mmdet - INFO - Epoch [1][19950/45697]	lr: 1.250e-05, eta: 2 days, 5:32:11, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.4390, loss_sub_cls: 20.1688, loss_obj_cls: 19.7154, loss_match: 3.3948, loss: 47.7181, grad_norm: 248.9799
2025-06-23 00:07:11,782 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:07:11,782 - mmdet - INFO - Epoch [1][20000/45697]	lr: 1.250e-05, eta: 2 days, 5:31:56, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.9397, loss_sub_cls: 20.0993, loss_obj_cls: 19.3878, loss_match: 3.5758, loss: 48.0026, grad_norm: 256.3424
2025-06-23 00:07:26,385 - mmdet - INFO - Epoch [1][20050/45697]	lr: 1.250e-05, eta: 2 days, 5:31:45, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 4.7219, loss_sub_cls: 19.8979, loss_obj_cls: 18.8029, loss_match: 4.4715, loss: 47.8941, grad_norm: 211.9086
2025-06-23 00:07:40,784 - mmdet - INFO - Epoch [1][20100/45697]	lr: 1.250e-05, eta: 2 days, 5:31:28, time: 0.288, data_time: 0.012, memory: 3008, loss_r_cls: 4.7932, loss_sub_cls: 19.7432, loss_obj_cls: 19.3004, loss_match: 3.6521, loss: 47.4889, grad_norm: 218.2106
2025-06-23 00:07:55,380 - mmdet - INFO - Epoch [1][20150/45697]	lr: 1.250e-05, eta: 2 days, 5:31:17, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 4.7931, loss_sub_cls: 19.6102, loss_obj_cls: 18.6962, loss_match: 3.3657, loss: 46.4653, grad_norm: 242.1295
2025-06-23 00:08:09,706 - mmdet - INFO - Epoch [1][20200/45697]	lr: 1.250e-05, eta: 2 days, 5:30:58, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.8255, loss_sub_cls: 19.9327, loss_obj_cls: 19.1643, loss_match: 4.0094, loss: 47.9319, grad_norm: 248.8006
2025-06-23 00:08:24,243 - mmdet - INFO - Epoch [1][20250/45697]	lr: 1.250e-05, eta: 2 days, 5:30:45, time: 0.291, data_time: 0.015, memory: 3008, loss_r_cls: 4.4916, loss_sub_cls: 20.0568, loss_obj_cls: 18.9917, loss_match: 3.6022, loss: 47.1423, grad_norm: 219.3918
2025-06-23 00:08:38,821 - mmdet - INFO - Epoch [1][20300/45697]	lr: 1.250e-05, eta: 2 days, 5:30:34, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 4.7936, loss_sub_cls: 20.0311, loss_obj_cls: 19.5131, loss_match: 3.2860, loss: 47.6238, grad_norm: 267.9511
2025-06-23 00:08:53,328 - mmdet - INFO - Epoch [1][20350/45697]	lr: 1.250e-05, eta: 2 days, 5:30:20, time: 0.290, data_time: 0.012, memory: 3008, loss_r_cls: 4.7134, loss_sub_cls: 20.2051, loss_obj_cls: 19.2774, loss_match: 4.0085, loss: 48.2043, grad_norm: 282.1004
2025-06-23 00:09:07,624 - mmdet - INFO - Epoch [1][20400/45697]	lr: 1.250e-05, eta: 2 days, 5:30:00, time: 0.286, data_time: 0.012, memory: 3008, loss_r_cls: 4.4885, loss_sub_cls: 20.3909, loss_obj_cls: 19.2872, loss_match: 3.8863, loss: 48.0529, grad_norm: 230.9180
2025-06-23 00:09:21,957 - mmdet - INFO - Epoch [1][20450/45697]	lr: 1.250e-05, eta: 2 days, 5:29:40, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.6475, loss_sub_cls: 19.7584, loss_obj_cls: 19.3781, loss_match: 3.9441, loss: 47.7281, grad_norm: 234.3290
2025-06-23 00:09:36,074 - mmdet - INFO - Epoch [1][20500/45697]	lr: 1.250e-05, eta: 2 days, 5:29:14, time: 0.282, data_time: 0.011, memory: 3008, loss_r_cls: 4.6111, loss_sub_cls: 20.1141, loss_obj_cls: 19.2663, loss_match: 3.4232, loss: 47.4146, grad_norm: 208.2563
2025-06-23 00:09:50,481 - mmdet - INFO - Epoch [1][20550/45697]	lr: 1.250e-05, eta: 2 days, 5:28:57, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.8552, loss_sub_cls: 19.9847, loss_obj_cls: 19.1997, loss_match: 3.6722, loss: 47.7118, grad_norm: 213.3273
2025-06-23 00:10:04,825 - mmdet - INFO - Epoch [1][20600/45697]	lr: 1.250e-05, eta: 2 days, 5:28:39, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 5.3590, loss_sub_cls: 19.5628, loss_obj_cls: 19.0316, loss_match: 4.1987, loss: 48.1521, grad_norm: 272.4374
2025-06-23 00:10:19,269 - mmdet - INFO - Epoch [1][20650/45697]	lr: 1.250e-05, eta: 2 days, 5:28:23, time: 0.289, data_time: 0.012, memory: 3008, loss_r_cls: 4.3455, loss_sub_cls: 19.6636, loss_obj_cls: 19.2105, loss_match: 4.4376, loss: 47.6573, grad_norm: 190.9930
2025-06-23 00:10:33,759 - mmdet - INFO - Epoch [1][20700/45697]	lr: 1.250e-05, eta: 2 days, 5:28:09, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.9346, loss_sub_cls: 19.4122, loss_obj_cls: 19.4589, loss_match: 4.4611, loss: 48.2669, grad_norm: 248.8132
2025-06-23 00:10:48,079 - mmdet - INFO - Epoch [1][20750/45697]	lr: 1.250e-05, eta: 2 days, 5:27:49, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.1903, loss_sub_cls: 19.4853, loss_obj_cls: 19.3043, loss_match: 4.8269, loss: 47.8067, grad_norm: 167.9829
2025-06-23 00:11:02,773 - mmdet - INFO - Epoch [1][20800/45697]	lr: 1.250e-05, eta: 2 days, 5:27:42, time: 0.294, data_time: 0.015, memory: 3008, loss_r_cls: 4.4236, loss_sub_cls: 19.4961, loss_obj_cls: 19.2620, loss_match: 4.8042, loss: 47.9859, grad_norm: 153.9701
2025-06-23 00:11:17,135 - mmdet - INFO - Epoch [1][20850/45697]	lr: 1.250e-05, eta: 2 days, 5:27:24, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 4.5679, loss_sub_cls: 19.6362, loss_obj_cls: 18.9591, loss_match: 5.1684, loss: 48.3316, grad_norm: 174.6205
2025-06-23 00:11:31,677 - mmdet - INFO - Epoch [1][20900/45697]	lr: 1.250e-05, eta: 2 days, 5:27:11, time: 0.291, data_time: 0.012, memory: 3008, loss_r_cls: 4.5883, loss_sub_cls: 19.6592, loss_obj_cls: 18.9338, loss_match: 4.0684, loss: 47.2497, grad_norm: 144.1962
2025-06-23 00:11:46,471 - mmdet - INFO - Epoch [1][20950/45697]	lr: 1.250e-05, eta: 2 days, 5:27:07, time: 0.296, data_time: 0.013, memory: 3008, loss_r_cls: 4.9576, loss_sub_cls: 19.8474, loss_obj_cls: 19.0542, loss_match: 3.8122, loss: 47.6715, grad_norm: 185.6431
2025-06-23 00:12:01,236 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:12:01,237 - mmdet - INFO - Epoch [1][21000/45697]	lr: 1.250e-05, eta: 2 days, 5:27:01, time: 0.295, data_time: 0.013, memory: 3008, loss_r_cls: 4.5782, loss_sub_cls: 19.6806, loss_obj_cls: 19.4722, loss_match: 4.5543, loss: 48.2853, grad_norm: 247.7105
2025-06-23 00:12:15,702 - mmdet - INFO - Epoch [1][21050/45697]	lr: 1.250e-05, eta: 2 days, 5:26:46, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.9900, loss_sub_cls: 19.9971, loss_obj_cls: 19.2143, loss_match: 4.4268, loss: 48.6282, grad_norm: 229.2748
2025-06-23 00:12:30,195 - mmdet - INFO - Epoch [1][21100/45697]	lr: 1.250e-05, eta: 2 days, 5:26:32, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.9510, loss_sub_cls: 20.3057, loss_obj_cls: 19.4783, loss_match: 4.1003, loss: 48.8353, grad_norm: 199.4417
2025-06-23 00:12:44,553 - mmdet - INFO - Epoch [1][21150/45697]	lr: 1.250e-05, eta: 2 days, 5:26:14, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.9466, loss_sub_cls: 19.5797, loss_obj_cls: 19.1735, loss_match: 4.3641, loss: 48.0640, grad_norm: 247.3376
2025-06-23 00:12:58,868 - mmdet - INFO - Epoch [1][21200/45697]	lr: 1.250e-05, eta: 2 days, 5:25:54, time: 0.286, data_time: 0.013, memory: 3008, loss_r_cls: 4.7117, loss_sub_cls: 19.8249, loss_obj_cls: 19.4571, loss_match: 3.3699, loss: 47.3636, grad_norm: 175.5691
2025-06-23 00:13:13,198 - mmdet - INFO - Epoch [1][21250/45697]	lr: 1.250e-05, eta: 2 days, 5:25:35, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 5.0150, loss_sub_cls: 19.9092, loss_obj_cls: 19.4043, loss_match: 3.9816, loss: 48.3101, grad_norm: 231.6257
2025-06-23 00:13:27,533 - mmdet - INFO - Epoch [1][21300/45697]	lr: 1.250e-05, eta: 2 days, 5:25:16, time: 0.287, data_time: 0.014, memory: 3008, loss_r_cls: 4.9758, loss_sub_cls: 19.6256, loss_obj_cls: 19.2635, loss_match: 4.1627, loss: 48.0276, grad_norm: 209.3203
2025-06-23 00:13:42,024 - mmdet - INFO - Epoch [1][21350/45697]	lr: 1.250e-05, eta: 2 days, 5:25:02, time: 0.290, data_time: 0.014, memory: 3008, loss_r_cls: 4.6835, loss_sub_cls: 19.9401, loss_obj_cls: 19.2427, loss_match: 4.5850, loss: 48.4513, grad_norm: 223.7309
2025-06-23 00:13:56,170 - mmdet - INFO - Epoch [1][21400/45697]	lr: 1.250e-05, eta: 2 days, 5:24:37, time: 0.283, data_time: 0.013, memory: 3008, loss_r_cls: 4.6048, loss_sub_cls: 19.7184, loss_obj_cls: 19.5536, loss_match: 3.8551, loss: 47.7318, grad_norm: 236.0328
2025-06-23 00:14:10,567 - mmdet - INFO - Epoch [1][21450/45697]	lr: 1.250e-05, eta: 2 days, 5:24:20, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.8751, loss_sub_cls: 19.8237, loss_obj_cls: 19.1737, loss_match: 3.4812, loss: 47.3536, grad_norm: 208.4061
2025-06-23 00:14:24,959 - mmdet - INFO - Epoch [1][21500/45697]	lr: 1.250e-05, eta: 2 days, 5:24:03, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 5.0177, loss_sub_cls: 20.1278, loss_obj_cls: 19.2350, loss_match: 3.8844, loss: 48.2649, grad_norm: 219.6151
2025-06-23 00:14:39,529 - mmdet - INFO - Epoch [1][21550/45697]	lr: 1.250e-05, eta: 2 days, 5:23:51, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.1632, loss_sub_cls: 20.1584, loss_obj_cls: 19.2220, loss_match: 4.3641, loss: 47.9077, grad_norm: 199.7265
2025-06-23 00:14:53,569 - mmdet - INFO - Epoch [1][21600/45697]	lr: 1.250e-05, eta: 2 days, 5:23:24, time: 0.281, data_time: 0.012, memory: 3008, loss_r_cls: 4.6729, loss_sub_cls: 20.4870, loss_obj_cls: 19.5130, loss_match: 4.2372, loss: 48.9101, grad_norm: 213.9105
2025-06-23 00:15:07,959 - mmdet - INFO - Epoch [1][21650/45697]	lr: 1.250e-05, eta: 2 days, 5:23:06, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.5022, loss_sub_cls: 20.0876, loss_obj_cls: 19.5125, loss_match: 3.9987, loss: 48.1011, grad_norm: 188.1739
2025-06-23 00:15:22,330 - mmdet - INFO - Epoch [1][21700/45697]	lr: 1.250e-05, eta: 2 days, 5:22:49, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.7121, loss_sub_cls: 20.4802, loss_obj_cls: 19.0271, loss_match: 4.9696, loss: 49.1888, grad_norm: 227.5579
2025-06-23 00:15:36,502 - mmdet - INFO - Epoch [1][21750/45697]	lr: 1.250e-05, eta: 2 days, 5:22:25, time: 0.283, data_time: 0.013, memory: 3008, loss_r_cls: 5.0338, loss_sub_cls: 20.1888, loss_obj_cls: 19.1773, loss_match: 4.1725, loss: 48.5724, grad_norm: 191.1631
2025-06-23 00:15:51,004 - mmdet - INFO - Epoch [1][21800/45697]	lr: 1.250e-05, eta: 2 days, 5:22:11, time: 0.290, data_time: 0.014, memory: 3008, loss_r_cls: 4.7391, loss_sub_cls: 20.3823, loss_obj_cls: 19.3174, loss_match: 4.2898, loss: 48.7286, grad_norm: 186.9705
2025-06-23 00:16:05,555 - mmdet - INFO - Epoch [1][21850/45697]	lr: 1.250e-05, eta: 2 days, 5:21:59, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.7649, loss_sub_cls: 19.7754, loss_obj_cls: 19.4011, loss_match: 4.0050, loss: 47.9464, grad_norm: 184.6098
2025-06-23 00:16:19,884 - mmdet - INFO - Epoch [1][21900/45697]	lr: 1.250e-05, eta: 2 days, 5:21:40, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.4036, loss_sub_cls: 20.0455, loss_obj_cls: 19.2685, loss_match: 4.1552, loss: 47.8728, grad_norm: 182.1296
2025-06-23 00:16:34,220 - mmdet - INFO - Epoch [1][21950/45697]	lr: 1.250e-05, eta: 2 days, 5:21:21, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 5.0928, loss_sub_cls: 19.5166, loss_obj_cls: 18.8772, loss_match: 4.0412, loss: 47.5278, grad_norm: 217.4346
2025-06-23 00:16:48,798 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:16:48,798 - mmdet - INFO - Epoch [1][22000/45697]	lr: 1.250e-05, eta: 2 days, 5:21:10, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 5.1543, loss_sub_cls: 19.6322, loss_obj_cls: 19.0700, loss_match: 4.4512, loss: 48.3076, grad_norm: 218.5926
2025-06-23 00:17:03,268 - mmdet - INFO - Epoch [1][22050/45697]	lr: 1.250e-05, eta: 2 days, 5:20:55, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 5.0236, loss_sub_cls: 19.6802, loss_obj_cls: 19.0423, loss_match: 3.8699, loss: 47.6160, grad_norm: 202.3471
2025-06-23 00:17:17,786 - mmdet - INFO - Epoch [1][22100/45697]	lr: 1.250e-05, eta: 2 days, 5:20:42, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.7968, loss_sub_cls: 20.1168, loss_obj_cls: 19.4303, loss_match: 4.2168, loss: 48.5607, grad_norm: 210.5186
2025-06-23 00:17:32,419 - mmdet - INFO - Epoch [1][22150/45697]	lr: 1.250e-05, eta: 2 days, 5:20:33, time: 0.293, data_time: 0.014, memory: 3008, loss_r_cls: 4.6540, loss_sub_cls: 19.8474, loss_obj_cls: 19.1848, loss_match: 4.0079, loss: 47.6940, grad_norm: 212.3811
2025-06-23 00:17:46,810 - mmdet - INFO - Epoch [1][22200/45697]	lr: 1.250e-05, eta: 2 days, 5:20:15, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.5559, loss_sub_cls: 20.0819, loss_obj_cls: 19.1948, loss_match: 3.0431, loss: 46.8757, grad_norm: 169.7441
2025-06-23 00:18:01,256 - mmdet - INFO - Epoch [1][22250/45697]	lr: 1.250e-05, eta: 2 days, 5:20:00, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.7598, loss_sub_cls: 20.4135, loss_obj_cls: 19.5654, loss_match: 3.7532, loss: 48.4919, grad_norm: 244.7167
2025-06-23 00:18:15,752 - mmdet - INFO - Epoch [1][22300/45697]	lr: 1.250e-05, eta: 2 days, 5:19:46, time: 0.290, data_time: 0.014, memory: 3008, loss_r_cls: 4.8740, loss_sub_cls: 20.2591, loss_obj_cls: 19.1432, loss_match: 3.6702, loss: 47.9465, grad_norm: 220.0937
2025-06-23 00:18:29,924 - mmdet - INFO - Epoch [1][22350/45697]	lr: 1.250e-05, eta: 2 days, 5:19:23, time: 0.283, data_time: 0.012, memory: 3008, loss_r_cls: 4.4804, loss_sub_cls: 20.4096, loss_obj_cls: 19.3884, loss_match: 4.0619, loss: 48.3403, grad_norm: 201.9956
2025-06-23 00:18:44,440 - mmdet - INFO - Epoch [1][22400/45697]	lr: 1.250e-05, eta: 2 days, 5:19:10, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.8992, loss_sub_cls: 20.2003, loss_obj_cls: 19.4116, loss_match: 3.8486, loss: 48.3597, grad_norm: 212.6392
2025-06-23 00:18:58,805 - mmdet - INFO - Epoch [1][22450/45697]	lr: 1.250e-05, eta: 2 days, 5:18:52, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.9663, loss_sub_cls: 20.7499, loss_obj_cls: 19.1898, loss_match: 4.2145, loss: 49.1205, grad_norm: 262.7846
2025-06-23 00:19:13,380 - mmdet - INFO - Epoch [1][22500/45697]	lr: 1.250e-05, eta: 2 days, 5:18:40, time: 0.291, data_time: 0.013, memory: 3008, loss_r_cls: 5.6544, loss_sub_cls: 20.3205, loss_obj_cls: 19.2518, loss_match: 4.0960, loss: 49.3226, grad_norm: 205.2037
2025-06-23 00:19:28,112 - mmdet - INFO - Epoch [1][22550/45697]	lr: 1.250e-05, eta: 2 days, 5:18:33, time: 0.295, data_time: 0.013, memory: 3008, loss_r_cls: 4.5086, loss_sub_cls: 20.5439, loss_obj_cls: 19.5342, loss_match: 3.2720, loss: 47.8587, grad_norm: 211.5388
2025-06-23 00:19:42,931 - mmdet - INFO - Epoch [1][22600/45697]	lr: 1.250e-05, eta: 2 days, 5:18:29, time: 0.296, data_time: 0.014, memory: 3008, loss_r_cls: 4.4786, loss_sub_cls: 20.3181, loss_obj_cls: 19.4247, loss_match: 3.9278, loss: 48.1492, grad_norm: 224.9615
2025-06-23 00:19:57,446 - mmdet - INFO - Epoch [1][22650/45697]	lr: 1.250e-05, eta: 2 days, 5:18:16, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.8285, loss_sub_cls: 19.8354, loss_obj_cls: 18.9527, loss_match: 3.8651, loss: 47.4817, grad_norm: 229.3039
2025-06-23 00:20:12,109 - mmdet - INFO - Epoch [1][22700/45697]	lr: 1.250e-05, eta: 2 days, 5:18:07, time: 0.293, data_time: 0.014, memory: 3008, loss_r_cls: 4.7509, loss_sub_cls: 19.7922, loss_obj_cls: 19.4302, loss_match: 3.1094, loss: 47.0827, grad_norm: 182.9297
2025-06-23 00:20:26,271 - mmdet - INFO - Epoch [1][22750/45697]	lr: 1.250e-05, eta: 2 days, 5:17:43, time: 0.283, data_time: 0.012, memory: 3008, loss_r_cls: 5.0393, loss_sub_cls: 20.0235, loss_obj_cls: 19.3379, loss_match: 3.3964, loss: 47.7971, grad_norm: 237.6384
2025-06-23 00:20:40,558 - mmdet - INFO - Epoch [1][22800/45697]	lr: 1.250e-05, eta: 2 days, 5:17:23, time: 0.286, data_time: 0.012, memory: 3008, loss_r_cls: 5.0925, loss_sub_cls: 20.2334, loss_obj_cls: 19.1498, loss_match: 3.6062, loss: 48.0819, grad_norm: 248.8158
2025-06-23 00:20:54,800 - mmdet - INFO - Epoch [1][22850/45697]	lr: 1.250e-05, eta: 2 days, 5:17:02, time: 0.285, data_time: 0.012, memory: 3008, loss_r_cls: 4.7387, loss_sub_cls: 19.9762, loss_obj_cls: 19.0778, loss_match: 2.7546, loss: 46.5472, grad_norm: 223.3290
2025-06-23 00:21:09,172 - mmdet - INFO - Epoch [1][22900/45697]	lr: 1.250e-05, eta: 2 days, 5:16:44, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 5.0298, loss_sub_cls: 19.5863, loss_obj_cls: 18.9099, loss_match: 3.6212, loss: 47.1473, grad_norm: 225.4376
2025-06-23 00:21:23,532 - mmdet - INFO - Epoch [1][22950/45697]	lr: 1.250e-05, eta: 2 days, 5:16:26, time: 0.287, data_time: 0.011, memory: 3008, loss_r_cls: 4.2278, loss_sub_cls: 20.5912, loss_obj_cls: 19.4872, loss_match: 4.4212, loss: 48.7273, grad_norm: 258.5791
2025-06-23 00:21:37,999 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:21:37,999 - mmdet - INFO - Epoch [1][23000/45697]	lr: 1.250e-05, eta: 2 days, 5:16:12, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.1503, loss_sub_cls: 20.1321, loss_obj_cls: 19.1703, loss_match: 3.8506, loss: 47.3033, grad_norm: 235.1602
2025-06-23 00:21:52,340 - mmdet - INFO - Epoch [1][23050/45697]	lr: 1.250e-05, eta: 2 days, 5:15:53, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.8123, loss_sub_cls: 20.3040, loss_obj_cls: 19.3378, loss_match: 3.4507, loss: 47.9048, grad_norm: 241.1122
2025-06-23 00:22:06,785 - mmdet - INFO - Epoch [1][23100/45697]	lr: 1.250e-05, eta: 2 days, 5:15:38, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 4.8075, loss_sub_cls: 19.9400, loss_obj_cls: 19.0920, loss_match: 4.0513, loss: 47.8908, grad_norm: 274.6300
2025-06-23 00:22:21,011 - mmdet - INFO - Epoch [1][23150/45697]	lr: 1.250e-05, eta: 2 days, 5:15:17, time: 0.285, data_time: 0.012, memory: 3008, loss_r_cls: 4.7181, loss_sub_cls: 20.0398, loss_obj_cls: 19.6146, loss_match: 3.0012, loss: 47.3737, grad_norm: 223.8511
2025-06-23 00:22:35,402 - mmdet - INFO - Epoch [1][23200/45697]	lr: 1.250e-05, eta: 2 days, 5:15:00, time: 0.288, data_time: 0.013, memory: 3008, loss_r_cls: 4.6539, loss_sub_cls: 19.5558, loss_obj_cls: 19.3148, loss_match: 3.7273, loss: 47.2517, grad_norm: 225.2435
2025-06-23 00:22:49,902 - mmdet - INFO - Epoch [1][23250/45697]	lr: 1.250e-05, eta: 2 days, 5:14:46, time: 0.290, data_time: 0.015, memory: 3008, loss_r_cls: 5.0291, loss_sub_cls: 19.7624, loss_obj_cls: 19.3688, loss_match: 3.8359, loss: 47.9962, grad_norm: 211.1104
2025-06-23 00:23:04,264 - mmdet - INFO - Epoch [1][23300/45697]	lr: 1.250e-05, eta: 2 days, 5:14:28, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.9678, loss_sub_cls: 19.9204, loss_obj_cls: 19.1568, loss_match: 3.6046, loss: 47.6496, grad_norm: 201.5170
2025-06-23 00:23:18,908 - mmdet - INFO - Epoch [1][23350/45697]	lr: 1.250e-05, eta: 2 days, 5:14:19, time: 0.293, data_time: 0.015, memory: 3008, loss_r_cls: 4.7778, loss_sub_cls: 20.1423, loss_obj_cls: 19.3916, loss_match: 3.5974, loss: 47.9092, grad_norm: 213.4301
2025-06-23 00:23:33,264 - mmdet - INFO - Epoch [1][23400/45697]	lr: 1.250e-05, eta: 2 days, 5:14:01, time: 0.287, data_time: 0.012, memory: 3008, loss_r_cls: 4.6302, loss_sub_cls: 19.9895, loss_obj_cls: 19.5414, loss_match: 3.1750, loss: 47.3361, grad_norm: 222.7544
2025-06-23 00:23:47,978 - mmdet - INFO - Epoch [1][23450/45697]	lr: 1.250e-05, eta: 2 days, 5:13:53, time: 0.294, data_time: 0.015, memory: 3008, loss_r_cls: 4.9845, loss_sub_cls: 19.9685, loss_obj_cls: 19.0551, loss_match: 4.0440, loss: 48.0520, grad_norm: 219.7223
2025-06-23 00:24:02,421 - mmdet - INFO - Epoch [1][23500/45697]	lr: 1.250e-05, eta: 2 days, 5:13:38, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.2592, loss_sub_cls: 20.2508, loss_obj_cls: 19.1191, loss_match: 3.9384, loss: 47.5675, grad_norm: 238.4481
2025-06-23 00:24:16,861 - mmdet - INFO - Epoch [1][23550/45697]	lr: 1.250e-05, eta: 2 days, 5:13:22, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 5.3538, loss_sub_cls: 20.2685, loss_obj_cls: 18.9817, loss_match: 3.4382, loss: 48.0422, grad_norm: 256.6915
2025-06-23 00:24:31,314 - mmdet - INFO - Epoch [1][23600/45697]	lr: 1.250e-05, eta: 2 days, 5:13:07, time: 0.289, data_time: 0.014, memory: 3008, loss_r_cls: 4.8891, loss_sub_cls: 20.1826, loss_obj_cls: 19.1710, loss_match: 3.7798, loss: 48.0225, grad_norm: 262.4972
2025-06-23 00:24:45,512 - mmdet - INFO - Epoch [1][23650/45697]	lr: 1.250e-05, eta: 2 days, 5:12:45, time: 0.284, data_time: 0.011, memory: 3008, loss_r_cls: 4.5627, loss_sub_cls: 20.1265, loss_obj_cls: 19.4277, loss_match: 3.1207, loss: 47.2376, grad_norm: 226.3261
2025-06-23 00:24:59,939 - mmdet - INFO - Epoch [1][23700/45697]	lr: 1.250e-05, eta: 2 days, 5:12:29, time: 0.289, data_time: 0.013, memory: 3008, loss_r_cls: 5.1206, loss_sub_cls: 19.6839, loss_obj_cls: 19.1056, loss_match: 4.0445, loss: 47.9546, grad_norm: 278.4283
2025-06-23 00:25:14,439 - mmdet - INFO - Epoch [1][23750/45697]	lr: 1.250e-05, eta: 2 days, 5:12:16, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.5699, loss_sub_cls: 19.8834, loss_obj_cls: 18.8555, loss_match: 3.9157, loss: 47.2245, grad_norm: 210.1739
2025-06-23 00:25:28,969 - mmdet - INFO - Epoch [1][23800/45697]	lr: 1.250e-05, eta: 2 days, 5:12:03, time: 0.291, data_time: 0.013, memory: 3008, loss_r_cls: 4.9396, loss_sub_cls: 20.1902, loss_obj_cls: 19.2294, loss_match: 3.6918, loss: 48.0511, grad_norm: 241.3553
2025-06-23 00:25:43,549 - mmdet - INFO - Epoch [1][23850/45697]	lr: 1.250e-05, eta: 2 days, 5:11:51, time: 0.292, data_time: 0.014, memory: 3008, loss_r_cls: 4.4140, loss_sub_cls: 19.9694, loss_obj_cls: 19.4711, loss_match: 3.2456, loss: 47.1000, grad_norm: 237.2905
2025-06-23 00:25:58,152 - mmdet - INFO - Epoch [1][23900/45697]	lr: 1.250e-05, eta: 2 days, 5:11:40, time: 0.292, data_time: 0.013, memory: 3008, loss_r_cls: 5.1335, loss_sub_cls: 20.2332, loss_obj_cls: 19.5524, loss_match: 3.7871, loss: 48.7063, grad_norm: 249.0036
2025-06-23 00:26:12,347 - mmdet - INFO - Epoch [1][23950/45697]	lr: 1.250e-05, eta: 2 days, 5:11:18, time: 0.284, data_time: 0.012, memory: 3008, loss_r_cls: 4.9892, loss_sub_cls: 19.4417, loss_obj_cls: 19.4717, loss_match: 3.3435, loss: 47.2461, grad_norm: 213.2652
2025-06-23 00:26:26,713 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:26:26,713 - mmdet - INFO - Epoch [1][24000/45697]	lr: 1.250e-05, eta: 2 days, 5:11:01, time: 0.287, data_time: 0.013, memory: 3008, loss_r_cls: 4.9754, loss_sub_cls: 19.7129, loss_obj_cls: 19.5553, loss_match: 3.3090, loss: 47.5526, grad_norm: 246.2264
2025-06-23 00:26:41,309 - mmdet - INFO - Epoch [1][24050/45697]	lr: 1.250e-05, eta: 2 days, 5:10:50, time: 0.292, data_time: 0.015, memory: 3008, loss_r_cls: 5.1187, loss_sub_cls: 19.7581, loss_obj_cls: 19.6174, loss_match: 3.6497, loss: 48.1439, grad_norm: 236.6955
2025-06-23 00:26:55,861 - mmdet - INFO - Epoch [1][24100/45697]	lr: 1.250e-05, eta: 2 days, 5:10:37, time: 0.291, data_time: 0.014, memory: 3008, loss_r_cls: 4.9333, loss_sub_cls: 19.9713, loss_obj_cls: 19.4575, loss_match: 3.2075, loss: 47.5696, grad_norm: 234.0460
2025-06-23 00:27:10,342 - mmdet - INFO - Epoch [1][24150/45697]	lr: 1.250e-05, eta: 2 days, 5:10:23, time: 0.290, data_time: 0.013, memory: 3008, loss_r_cls: 4.8721, loss_sub_cls: 20.2023, loss_obj_cls: 19.1539, loss_match: 3.5219, loss: 47.7502, grad_norm: 252.8493
2025-06-23 00:27:25,064 - mmdet - INFO - Epoch [1][24200/45697]	lr: 1.250e-05, eta: 2 days, 5:10:15, time: 0.294, data_time: 0.016, memory: 3036, loss_r_cls: 4.7859, loss_sub_cls: 19.7784, loss_obj_cls: 19.5973, loss_match: 3.3567, loss: 47.5184, grad_norm: 226.8895
2025-06-23 00:27:39,502 - mmdet - INFO - Epoch [1][24250/45697]	lr: 1.250e-05, eta: 2 days, 5:10:00, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.2287, loss_sub_cls: 20.2493, loss_obj_cls: 19.5182, loss_match: 3.9390, loss: 47.9352, grad_norm: 254.9117
2025-06-23 00:27:53,764 - mmdet - INFO - Epoch [1][24300/45697]	lr: 1.250e-05, eta: 2 days, 5:09:40, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.9032, loss_sub_cls: 20.0140, loss_obj_cls: 19.0163, loss_match: 3.6877, loss: 47.6213, grad_norm: 256.3604
2025-06-23 00:28:07,934 - mmdet - INFO - Epoch [1][24350/45697]	lr: 1.250e-05, eta: 2 days, 5:09:17, time: 0.283, data_time: 0.014, memory: 3036, loss_r_cls: 4.8567, loss_sub_cls: 19.3899, loss_obj_cls: 19.1151, loss_match: 3.6670, loss: 47.0286, grad_norm: 248.4648
2025-06-23 00:28:22,485 - mmdet - INFO - Epoch [1][24400/45697]	lr: 1.250e-05, eta: 2 days, 5:09:05, time: 0.291, data_time: 0.013, memory: 3036, loss_r_cls: 4.5545, loss_sub_cls: 19.6343, loss_obj_cls: 19.0095, loss_match: 4.4404, loss: 47.6387, grad_norm: 287.5996
2025-06-23 00:28:37,008 - mmdet - INFO - Epoch [1][24450/45697]	lr: 1.250e-05, eta: 2 days, 5:08:52, time: 0.290, data_time: 0.013, memory: 3036, loss_r_cls: 4.7525, loss_sub_cls: 19.9200, loss_obj_cls: 19.4953, loss_match: 4.2133, loss: 48.3811, grad_norm: 284.1604
2025-06-23 00:28:51,318 - mmdet - INFO - Epoch [1][24500/45697]	lr: 1.250e-05, eta: 2 days, 5:08:33, time: 0.286, data_time: 0.012, memory: 3036, loss_r_cls: 4.2050, loss_sub_cls: 20.0073, loss_obj_cls: 19.0448, loss_match: 3.7939, loss: 47.0510, grad_norm: 213.9470
2025-06-23 00:29:05,916 - mmdet - INFO - Epoch [1][24550/45697]	lr: 1.250e-05, eta: 2 days, 5:08:22, time: 0.292, data_time: 0.013, memory: 3036, loss_r_cls: 4.5550, loss_sub_cls: 20.0822, loss_obj_cls: 18.9993, loss_match: 3.5236, loss: 47.1602, grad_norm: 230.6310
2025-06-23 00:29:20,416 - mmdet - INFO - Epoch [1][24600/45697]	lr: 1.250e-05, eta: 2 days, 5:08:08, time: 0.290, data_time: 0.013, memory: 3036, loss_r_cls: 4.9248, loss_sub_cls: 19.9189, loss_obj_cls: 19.6248, loss_match: 3.7321, loss: 48.2006, grad_norm: 221.3133
2025-06-23 00:29:34,882 - mmdet - INFO - Epoch [1][24650/45697]	lr: 1.250e-05, eta: 2 days, 5:07:53, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.6858, loss_sub_cls: 19.8414, loss_obj_cls: 19.1613, loss_match: 3.7029, loss: 47.3913, grad_norm: 223.5997
2025-06-23 00:29:49,305 - mmdet - INFO - Epoch [1][24700/45697]	lr: 1.250e-05, eta: 2 days, 5:07:37, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.4715, loss_sub_cls: 20.0799, loss_obj_cls: 19.5546, loss_match: 3.2499, loss: 47.3559, grad_norm: 196.2276
2025-06-23 00:30:03,934 - mmdet - INFO - Epoch [1][24750/45697]	lr: 1.250e-05, eta: 2 days, 5:07:27, time: 0.293, data_time: 0.013, memory: 3036, loss_r_cls: 5.3704, loss_sub_cls: 20.0071, loss_obj_cls: 19.4667, loss_match: 3.6567, loss: 48.5009, grad_norm: 267.2855
2025-06-23 00:30:18,423 - mmdet - INFO - Epoch [1][24800/45697]	lr: 1.250e-05, eta: 2 days, 5:07:13, time: 0.290, data_time: 0.013, memory: 3036, loss_r_cls: 4.8709, loss_sub_cls: 20.0703, loss_obj_cls: 19.5187, loss_match: 3.7566, loss: 48.2166, grad_norm: 228.0753
2025-06-23 00:30:32,759 - mmdet - INFO - Epoch [1][24850/45697]	lr: 1.250e-05, eta: 2 days, 5:06:55, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.7958, loss_sub_cls: 19.8656, loss_obj_cls: 19.0140, loss_match: 3.4936, loss: 47.1690, grad_norm: 255.9033
2025-06-23 00:30:47,216 - mmdet - INFO - Epoch [1][24900/45697]	lr: 1.250e-05, eta: 2 days, 5:06:40, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.5859, loss_sub_cls: 19.4392, loss_obj_cls: 19.1911, loss_match: 3.6005, loss: 46.8166, grad_norm: 214.5503
2025-06-23 00:31:01,669 - mmdet - INFO - Epoch [1][24950/45697]	lr: 1.250e-05, eta: 2 days, 5:06:25, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.7260, loss_sub_cls: 20.2295, loss_obj_cls: 19.3243, loss_match: 2.9584, loss: 47.2383, grad_norm: 230.7829
2025-06-23 00:31:16,035 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:31:16,035 - mmdet - INFO - Epoch [1][25000/45697]	lr: 1.250e-05, eta: 2 days, 5:06:08, time: 0.287, data_time: 0.012, memory: 3036, loss_r_cls: 4.6194, loss_sub_cls: 20.2207, loss_obj_cls: 19.4266, loss_match: 3.4235, loss: 47.6902, grad_norm: 223.1389
2025-06-23 00:31:30,430 - mmdet - INFO - Epoch [1][25050/45697]	lr: 1.250e-05, eta: 2 days, 5:05:51, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.8425, loss_sub_cls: 20.2113, loss_obj_cls: 19.4474, loss_match: 4.4289, loss: 48.9301, grad_norm: 249.0441
2025-06-23 00:31:44,961 - mmdet - INFO - Epoch [1][25100/45697]	lr: 1.250e-05, eta: 2 days, 5:05:38, time: 0.291, data_time: 0.014, memory: 3036, loss_r_cls: 4.7077, loss_sub_cls: 19.7529, loss_obj_cls: 19.3886, loss_match: 3.8829, loss: 47.7321, grad_norm: 195.1040
2025-06-23 00:31:59,477 - mmdet - INFO - Epoch [1][25150/45697]	lr: 1.250e-05, eta: 2 days, 5:05:25, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 4.8519, loss_sub_cls: 19.4933, loss_obj_cls: 19.2754, loss_match: 3.9302, loss: 47.5507, grad_norm: 244.6053
2025-06-23 00:32:13,799 - mmdet - INFO - Epoch [1][25200/45697]	lr: 1.250e-05, eta: 2 days, 5:05:06, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 5.0310, loss_sub_cls: 19.8328, loss_obj_cls: 19.1684, loss_match: 3.1783, loss: 47.2104, grad_norm: 217.2564
2025-06-23 00:32:28,290 - mmdet - INFO - Epoch [1][25250/45697]	lr: 1.250e-05, eta: 2 days, 5:04:52, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 4.9162, loss_sub_cls: 19.8101, loss_obj_cls: 19.3475, loss_match: 3.4024, loss: 47.4762, grad_norm: 206.9912
2025-06-23 00:32:42,826 - mmdet - INFO - Epoch [1][25300/45697]	lr: 1.250e-05, eta: 2 days, 5:04:40, time: 0.291, data_time: 0.015, memory: 3036, loss_r_cls: 4.7842, loss_sub_cls: 19.8551, loss_obj_cls: 19.3975, loss_match: 4.3731, loss: 48.4100, grad_norm: 173.5394
2025-06-23 00:32:57,404 - mmdet - INFO - Epoch [1][25350/45697]	lr: 1.250e-05, eta: 2 days, 5:04:28, time: 0.292, data_time: 0.012, memory: 3036, loss_r_cls: 4.3639, loss_sub_cls: 19.9306, loss_obj_cls: 19.3174, loss_match: 3.4236, loss: 47.0354, grad_norm: 233.0599
2025-06-23 00:33:11,839 - mmdet - INFO - Epoch [1][25400/45697]	lr: 1.250e-05, eta: 2 days, 5:04:12, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 5.1644, loss_sub_cls: 20.0386, loss_obj_cls: 19.1933, loss_match: 3.9477, loss: 48.3440, grad_norm: 204.7999
2025-06-23 00:33:26,520 - mmdet - INFO - Epoch [1][25450/45697]	lr: 1.250e-05, eta: 2 days, 5:04:03, time: 0.294, data_time: 0.015, memory: 3036, loss_r_cls: 4.4983, loss_sub_cls: 19.8903, loss_obj_cls: 19.1960, loss_match: 4.5992, loss: 48.1839, grad_norm: 246.3557
2025-06-23 00:33:40,889 - mmdet - INFO - Epoch [1][25500/45697]	lr: 1.250e-05, eta: 2 days, 5:03:46, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 4.5188, loss_sub_cls: 20.1606, loss_obj_cls: 18.9324, loss_match: 4.0040, loss: 47.6158, grad_norm: 205.4652
2025-06-23 00:33:55,481 - mmdet - INFO - Epoch [1][25550/45697]	lr: 1.250e-05, eta: 2 days, 5:03:35, time: 0.292, data_time: 0.013, memory: 3036, loss_r_cls: 4.5928, loss_sub_cls: 20.0345, loss_obj_cls: 19.3746, loss_match: 3.7539, loss: 47.7559, grad_norm: 204.2683
2025-06-23 00:34:09,754 - mmdet - INFO - Epoch [1][25600/45697]	lr: 1.250e-05, eta: 2 days, 5:03:15, time: 0.285, data_time: 0.012, memory: 3036, loss_r_cls: 4.4873, loss_sub_cls: 19.8897, loss_obj_cls: 19.1035, loss_match: 3.4070, loss: 46.8875, grad_norm: 212.3318
2025-06-23 00:34:24,353 - mmdet - INFO - Epoch [1][25650/45697]	lr: 1.250e-05, eta: 2 days, 5:03:04, time: 0.292, data_time: 0.013, memory: 3036, loss_r_cls: 4.6024, loss_sub_cls: 20.3135, loss_obj_cls: 19.2772, loss_match: 3.9073, loss: 48.1005, grad_norm: 207.2709
2025-06-23 00:34:39,085 - mmdet - INFO - Epoch [1][25700/45697]	lr: 1.250e-05, eta: 2 days, 5:02:56, time: 0.295, data_time: 0.013, memory: 3036, loss_r_cls: 4.8252, loss_sub_cls: 19.7417, loss_obj_cls: 19.1568, loss_match: 4.2326, loss: 47.9563, grad_norm: 217.0915
2025-06-23 00:34:53,654 - mmdet - INFO - Epoch [1][25750/45697]	lr: 1.250e-05, eta: 2 days, 5:02:44, time: 0.291, data_time: 0.013, memory: 3036, loss_r_cls: 5.1964, loss_sub_cls: 19.8233, loss_obj_cls: 19.6628, loss_match: 4.2526, loss: 48.9350, grad_norm: 232.1701
2025-06-23 00:35:08,245 - mmdet - INFO - Epoch [1][25800/45697]	lr: 1.250e-05, eta: 2 days, 5:02:33, time: 0.292, data_time: 0.012, memory: 3036, loss_r_cls: 4.4651, loss_sub_cls: 20.1626, loss_obj_cls: 19.6722, loss_match: 4.0814, loss: 48.3813, grad_norm: 252.3425
2025-06-23 00:35:22,821 - mmdet - INFO - Epoch [1][25850/45697]	lr: 1.250e-05, eta: 2 days, 5:02:21, time: 0.292, data_time: 0.014, memory: 3036, loss_r_cls: 4.5190, loss_sub_cls: 19.6410, loss_obj_cls: 19.1522, loss_match: 3.5598, loss: 46.8720, grad_norm: 187.6578
2025-06-23 00:35:37,150 - mmdet - INFO - Epoch [1][25900/45697]	lr: 1.250e-05, eta: 2 days, 5:02:03, time: 0.287, data_time: 0.011, memory: 3036, loss_r_cls: 4.6168, loss_sub_cls: 20.4138, loss_obj_cls: 19.6114, loss_match: 3.6089, loss: 48.2509, grad_norm: 234.5955
2025-06-23 00:35:51,635 - mmdet - INFO - Epoch [1][25950/45697]	lr: 1.250e-05, eta: 2 days, 5:01:48, time: 0.290, data_time: 0.015, memory: 3036, loss_r_cls: 4.6442, loss_sub_cls: 19.6998, loss_obj_cls: 19.1705, loss_match: 4.6195, loss: 48.1340, grad_norm: 271.9056
2025-06-23 00:36:05,980 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:36:05,980 - mmdet - INFO - Epoch [1][26000/45697]	lr: 1.250e-05, eta: 2 days, 5:01:31, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.8931, loss_sub_cls: 19.9677, loss_obj_cls: 19.0517, loss_match: 3.8783, loss: 47.7908, grad_norm: 187.0573
2025-06-23 00:36:20,354 - mmdet - INFO - Epoch [1][26050/45697]	lr: 1.250e-05, eta: 2 days, 5:01:14, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.7587, loss_sub_cls: 20.0391, loss_obj_cls: 19.4860, loss_match: 3.0235, loss: 47.3073, grad_norm: 181.0210
2025-06-23 00:36:34,803 - mmdet - INFO - Epoch [1][26100/45697]	lr: 1.250e-05, eta: 2 days, 5:00:59, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.7898, loss_sub_cls: 19.8290, loss_obj_cls: 19.4773, loss_match: 4.5016, loss: 48.5977, grad_norm: 252.3593
2025-06-23 00:36:49,043 - mmdet - INFO - Epoch [1][26150/45697]	lr: 1.250e-05, eta: 2 days, 5:00:38, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.7621, loss_sub_cls: 19.4373, loss_obj_cls: 19.7884, loss_match: 3.5765, loss: 47.5644, grad_norm: 214.7513
2025-06-23 00:37:03,747 - mmdet - INFO - Epoch [1][26200/45697]	lr: 1.250e-05, eta: 2 days, 5:00:30, time: 0.294, data_time: 0.015, memory: 3036, loss_r_cls: 4.5205, loss_sub_cls: 19.6551, loss_obj_cls: 19.0487, loss_match: 3.6096, loss: 46.8338, grad_norm: 196.5442
2025-06-23 00:37:17,927 - mmdet - INFO - Epoch [1][26250/45697]	lr: 1.250e-05, eta: 2 days, 5:00:08, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.6928, loss_sub_cls: 19.4862, loss_obj_cls: 19.0286, loss_match: 3.6930, loss: 46.9006, grad_norm: 203.9938
2025-06-23 00:37:32,424 - mmdet - INFO - Epoch [1][26300/45697]	lr: 1.250e-05, eta: 2 days, 4:59:54, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 4.8482, loss_sub_cls: 19.4399, loss_obj_cls: 19.1429, loss_match: 4.7388, loss: 48.1698, grad_norm: 256.4321
2025-06-23 00:37:46,829 - mmdet - INFO - Epoch [1][26350/45697]	lr: 1.250e-05, eta: 2 days, 4:59:38, time: 0.288, data_time: 0.012, memory: 3036, loss_r_cls: 4.7291, loss_sub_cls: 19.6581, loss_obj_cls: 19.0271, loss_match: 4.2056, loss: 47.6199, grad_norm: 232.9846
2025-06-23 00:38:01,290 - mmdet - INFO - Epoch [1][26400/45697]	lr: 1.250e-05, eta: 2 days, 4:59:23, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 4.2636, loss_sub_cls: 19.4351, loss_obj_cls: 18.6943, loss_match: 3.7460, loss: 46.1390, grad_norm: 188.5726
2025-06-23 00:38:15,747 - mmdet - INFO - Epoch [1][26450/45697]	lr: 1.250e-05, eta: 2 days, 4:59:08, time: 0.289, data_time: 0.012, memory: 3036, loss_r_cls: 4.6137, loss_sub_cls: 19.8613, loss_obj_cls: 18.7056, loss_match: 4.2796, loss: 47.4601, grad_norm: 204.8918
2025-06-23 00:38:29,982 - mmdet - INFO - Epoch [1][26500/45697]	lr: 1.250e-05, eta: 2 days, 4:58:48, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 5.0295, loss_sub_cls: 19.7495, loss_obj_cls: 19.2526, loss_match: 3.4076, loss: 47.4392, grad_norm: 188.8498
2025-06-23 00:38:44,295 - mmdet - INFO - Epoch [1][26550/45697]	lr: 1.250e-05, eta: 2 days, 4:58:29, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 4.8261, loss_sub_cls: 19.4354, loss_obj_cls: 18.6543, loss_match: 4.7102, loss: 47.6259, grad_norm: 215.2109
2025-06-23 00:38:58,621 - mmdet - INFO - Epoch [1][26600/45697]	lr: 1.250e-05, eta: 2 days, 4:58:11, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 4.6071, loss_sub_cls: 19.9023, loss_obj_cls: 19.0059, loss_match: 4.2333, loss: 47.7486, grad_norm: 187.8466
2025-06-23 00:39:12,760 - mmdet - INFO - Epoch [1][26650/45697]	lr: 1.250e-05, eta: 2 days, 4:57:48, time: 0.283, data_time: 0.012, memory: 3036, loss_r_cls: 5.0090, loss_sub_cls: 19.5750, loss_obj_cls: 19.2272, loss_match: 3.6260, loss: 47.4373, grad_norm: 207.3043
2025-06-23 00:39:26,869 - mmdet - INFO - Epoch [1][26700/45697]	lr: 1.250e-05, eta: 2 days, 4:57:25, time: 0.282, data_time: 0.012, memory: 3036, loss_r_cls: 4.4262, loss_sub_cls: 19.5893, loss_obj_cls: 19.2336, loss_match: 3.7098, loss: 46.9589, grad_norm: 235.1699
2025-06-23 00:39:41,102 - mmdet - INFO - Epoch [1][26750/45697]	lr: 1.250e-05, eta: 2 days, 4:57:05, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.3826, loss_sub_cls: 19.7473, loss_obj_cls: 19.2647, loss_match: 4.0342, loss: 47.4289, grad_norm: 256.5385
2025-06-23 00:39:55,504 - mmdet - INFO - Epoch [1][26800/45697]	lr: 1.250e-05, eta: 2 days, 4:56:49, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.9756, loss_sub_cls: 20.0794, loss_obj_cls: 18.8293, loss_match: 3.0246, loss: 46.9088, grad_norm: 171.3305
2025-06-23 00:40:09,895 - mmdet - INFO - Epoch [1][26850/45697]	lr: 1.250e-05, eta: 2 days, 4:56:32, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 5.0410, loss_sub_cls: 19.7830, loss_obj_cls: 19.2987, loss_match: 3.9307, loss: 48.0534, grad_norm: 206.5867
2025-06-23 00:40:24,422 - mmdet - INFO - Epoch [1][26900/45697]	lr: 1.250e-05, eta: 2 days, 4:56:19, time: 0.291, data_time: 0.014, memory: 3036, loss_r_cls: 4.4922, loss_sub_cls: 20.0935, loss_obj_cls: 19.4082, loss_match: 3.5979, loss: 47.5919, grad_norm: 210.0771
2025-06-23 00:40:38,755 - mmdet - INFO - Epoch [1][26950/45697]	lr: 1.250e-05, eta: 2 days, 4:56:01, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.8137, loss_sub_cls: 19.4568, loss_obj_cls: 19.0454, loss_match: 3.8075, loss: 47.1235, grad_norm: 196.1938
2025-06-23 00:40:53,160 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:40:53,160 - mmdet - INFO - Epoch [1][27000/45697]	lr: 1.250e-05, eta: 2 days, 4:55:45, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.5934, loss_sub_cls: 19.5783, loss_obj_cls: 19.0615, loss_match: 3.5298, loss: 46.7630, grad_norm: 163.6024
2025-06-23 00:41:07,313 - mmdet - INFO - Epoch [1][27050/45697]	lr: 1.250e-05, eta: 2 days, 4:55:23, time: 0.283, data_time: 0.013, memory: 3036, loss_r_cls: 4.5611, loss_sub_cls: 19.7766, loss_obj_cls: 19.5760, loss_match: 3.8744, loss: 47.7881, grad_norm: 223.7594
2025-06-23 00:41:21,517 - mmdet - INFO - Epoch [1][27100/45697]	lr: 1.250e-05, eta: 2 days, 4:55:02, time: 0.284, data_time: 0.012, memory: 3036, loss_r_cls: 4.5609, loss_sub_cls: 19.7872, loss_obj_cls: 19.1307, loss_match: 3.0606, loss: 46.5393, grad_norm: 187.5735
2025-06-23 00:41:35,884 - mmdet - INFO - Epoch [1][27150/45697]	lr: 1.250e-05, eta: 2 days, 4:54:45, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 5.0006, loss_sub_cls: 20.1752, loss_obj_cls: 19.1521, loss_match: 3.0757, loss: 47.4036, grad_norm: 250.6898
2025-06-23 00:41:50,030 - mmdet - INFO - Epoch [1][27200/45697]	lr: 1.250e-05, eta: 2 days, 4:54:23, time: 0.283, data_time: 0.013, memory: 3036, loss_r_cls: 4.8926, loss_sub_cls: 20.1120, loss_obj_cls: 19.5067, loss_match: 3.7131, loss: 48.2244, grad_norm: 250.4056
2025-06-23 00:42:04,272 - mmdet - INFO - Epoch [1][27250/45697]	lr: 1.250e-05, eta: 2 days, 4:54:03, time: 0.285, data_time: 0.012, memory: 3036, loss_r_cls: 4.7046, loss_sub_cls: 19.5855, loss_obj_cls: 19.0891, loss_match: 3.8984, loss: 47.2777, grad_norm: 252.0560
2025-06-23 00:42:18,552 - mmdet - INFO - Epoch [1][27300/45697]	lr: 1.250e-05, eta: 2 days, 4:53:44, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 4.6589, loss_sub_cls: 19.6123, loss_obj_cls: 19.2102, loss_match: 5.3476, loss: 48.8290, grad_norm: 269.8457
2025-06-23 00:42:32,581 - mmdet - INFO - Epoch [1][27350/45697]	lr: 1.250e-05, eta: 2 days, 4:53:19, time: 0.281, data_time: 0.012, memory: 3036, loss_r_cls: 4.9292, loss_sub_cls: 19.6882, loss_obj_cls: 18.8113, loss_match: 3.8474, loss: 47.2762, grad_norm: 179.3446
2025-06-23 00:42:46,852 - mmdet - INFO - Epoch [1][27400/45697]	lr: 1.250e-05, eta: 2 days, 4:53:00, time: 0.285, data_time: 0.012, memory: 3036, loss_r_cls: 4.9937, loss_sub_cls: 19.7192, loss_obj_cls: 19.1069, loss_match: 3.8806, loss: 47.7003, grad_norm: 206.0671
2025-06-23 00:43:01,253 - mmdet - INFO - Epoch [1][27450/45697]	lr: 1.250e-05, eta: 2 days, 4:52:44, time: 0.288, data_time: 0.014, memory: 3036, loss_r_cls: 5.0157, loss_sub_cls: 19.7137, loss_obj_cls: 19.4288, loss_match: 4.0019, loss: 48.1601, grad_norm: 221.2377
2025-06-23 00:43:15,612 - mmdet - INFO - Epoch [1][27500/45697]	lr: 1.250e-05, eta: 2 days, 4:52:27, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 5.1925, loss_sub_cls: 19.3027, loss_obj_cls: 19.0695, loss_match: 3.5416, loss: 47.1063, grad_norm: 209.8075
2025-06-23 00:43:29,765 - mmdet - INFO - Epoch [1][27550/45697]	lr: 1.250e-05, eta: 2 days, 4:52:05, time: 0.283, data_time: 0.014, memory: 3036, loss_r_cls: 4.7666, loss_sub_cls: 19.5247, loss_obj_cls: 18.5557, loss_match: 3.8112, loss: 46.6582, grad_norm: 198.8630
2025-06-23 00:43:44,212 - mmdet - INFO - Epoch [1][27600/45697]	lr: 1.250e-05, eta: 2 days, 4:51:50, time: 0.289, data_time: 0.015, memory: 3036, loss_r_cls: 4.7106, loss_sub_cls: 19.6707, loss_obj_cls: 19.2318, loss_match: 3.7950, loss: 47.4081, grad_norm: 228.6773
2025-06-23 00:43:58,816 - mmdet - INFO - Epoch [1][27650/45697]	lr: 1.250e-05, eta: 2 days, 4:51:39, time: 0.292, data_time: 0.015, memory: 3036, loss_r_cls: 4.6567, loss_sub_cls: 19.8898, loss_obj_cls: 19.2276, loss_match: 3.9085, loss: 47.6827, grad_norm: 204.4663
2025-06-23 00:44:13,263 - mmdet - INFO - Epoch [1][27700/45697]	lr: 1.250e-05, eta: 2 days, 4:51:24, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 4.8401, loss_sub_cls: 19.4941, loss_obj_cls: 19.2584, loss_match: 3.6547, loss: 47.2473, grad_norm: 166.6380
2025-06-23 00:44:28,922 - mmdet - INFO - Epoch [1][27750/45697]	lr: 1.250e-05, eta: 2 days, 4:51:38, time: 0.313, data_time: 0.032, memory: 3036, loss_r_cls: 5.2760, loss_sub_cls: 19.8384, loss_obj_cls: 19.0714, loss_match: 4.1110, loss: 48.2968, grad_norm: 231.2031
2025-06-23 00:44:43,266 - mmdet - INFO - Epoch [1][27800/45697]	lr: 1.250e-05, eta: 2 days, 4:51:20, time: 0.287, data_time: 0.011, memory: 3036, loss_r_cls: 5.1113, loss_sub_cls: 19.9203, loss_obj_cls: 19.0999, loss_match: 4.0137, loss: 48.1451, grad_norm: 228.1922
2025-06-23 00:44:57,584 - mmdet - INFO - Epoch [1][27850/45697]	lr: 1.250e-05, eta: 2 days, 4:51:02, time: 0.286, data_time: 0.014, memory: 3036, loss_r_cls: 4.6639, loss_sub_cls: 19.7747, loss_obj_cls: 19.3439, loss_match: 3.5688, loss: 47.3513, grad_norm: 177.9383
2025-06-23 00:45:12,004 - mmdet - INFO - Epoch [1][27900/45697]	lr: 1.250e-05, eta: 2 days, 4:50:47, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.6463, loss_sub_cls: 20.0168, loss_obj_cls: 19.7030, loss_match: 3.3421, loss: 47.7081, grad_norm: 247.6069
2025-06-23 00:45:26,493 - mmdet - INFO - Epoch [1][27950/45697]	lr: 1.250e-05, eta: 2 days, 4:50:33, time: 0.290, data_time: 0.012, memory: 3036, loss_r_cls: 5.0685, loss_sub_cls: 19.8821, loss_obj_cls: 19.3431, loss_match: 3.2784, loss: 47.5721, grad_norm: 218.7275
2025-06-23 00:45:40,669 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:45:40,670 - mmdet - INFO - Epoch [1][28000/45697]	lr: 1.250e-05, eta: 2 days, 4:50:12, time: 0.284, data_time: 0.012, memory: 3036, loss_r_cls: 4.5585, loss_sub_cls: 19.7732, loss_obj_cls: 19.5630, loss_match: 3.5171, loss: 47.4119, grad_norm: 232.1409
2025-06-23 00:45:54,932 - mmdet - INFO - Epoch [1][28050/45697]	lr: 1.250e-05, eta: 2 days, 4:49:52, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.8897, loss_sub_cls: 19.1893, loss_obj_cls: 19.1370, loss_match: 4.2458, loss: 47.4618, grad_norm: 259.8714
2025-06-23 00:46:09,041 - mmdet - INFO - Epoch [1][28100/45697]	lr: 1.250e-05, eta: 2 days, 4:49:30, time: 0.282, data_time: 0.012, memory: 3036, loss_r_cls: 4.5722, loss_sub_cls: 19.1564, loss_obj_cls: 19.4113, loss_match: 3.9637, loss: 47.1035, grad_norm: 215.7687
2025-06-23 00:46:23,318 - mmdet - INFO - Epoch [1][28150/45697]	lr: 1.250e-05, eta: 2 days, 4:49:11, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 4.7129, loss_sub_cls: 20.0105, loss_obj_cls: 19.4351, loss_match: 3.5118, loss: 47.6703, grad_norm: 210.1903
2025-06-23 00:46:37,513 - mmdet - INFO - Epoch [1][28200/45697]	lr: 1.250e-05, eta: 2 days, 4:48:50, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.6257, loss_sub_cls: 19.9856, loss_obj_cls: 19.4358, loss_match: 3.8835, loss: 47.9307, grad_norm: 271.2010
2025-06-23 00:46:52,005 - mmdet - INFO - Epoch [1][28250/45697]	lr: 1.250e-05, eta: 2 days, 4:48:36, time: 0.290, data_time: 0.013, memory: 3036, loss_r_cls: 4.6761, loss_sub_cls: 19.9598, loss_obj_cls: 19.2428, loss_match: 4.5042, loss: 48.3829, grad_norm: 247.0526
2025-06-23 00:47:06,388 - mmdet - INFO - Epoch [1][28300/45697]	lr: 1.250e-05, eta: 2 days, 4:48:20, time: 0.288, data_time: 0.015, memory: 3036, loss_r_cls: 4.7519, loss_sub_cls: 20.2049, loss_obj_cls: 19.1457, loss_match: 3.6307, loss: 47.7332, grad_norm: 239.6986
2025-06-23 00:47:20,764 - mmdet - INFO - Epoch [1][28350/45697]	lr: 1.250e-05, eta: 2 days, 4:48:03, time: 0.288, data_time: 0.015, memory: 3036, loss_r_cls: 4.4615, loss_sub_cls: 19.6338, loss_obj_cls: 19.0541, loss_match: 4.0892, loss: 47.2386, grad_norm: 219.2879
2025-06-23 00:47:35,214 - mmdet - INFO - Epoch [1][28400/45697]	lr: 1.250e-05, eta: 2 days, 4:47:49, time: 0.289, data_time: 0.016, memory: 3036, loss_r_cls: 4.9168, loss_sub_cls: 19.7198, loss_obj_cls: 19.0275, loss_match: 4.1416, loss: 47.8058, grad_norm: 170.8275
2025-06-23 00:47:49,991 - mmdet - INFO - Epoch [1][28450/45697]	lr: 1.250e-05, eta: 2 days, 4:47:41, time: 0.296, data_time: 0.015, memory: 3036, loss_r_cls: 4.9413, loss_sub_cls: 19.4771, loss_obj_cls: 19.3996, loss_match: 3.7228, loss: 47.5407, grad_norm: 187.4275
2025-06-23 00:48:04,393 - mmdet - INFO - Epoch [1][28500/45697]	lr: 1.250e-05, eta: 2 days, 4:47:25, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.9463, loss_sub_cls: 19.4330, loss_obj_cls: 19.3403, loss_match: 5.1679, loss: 48.8874, grad_norm: 256.9169
2025-06-23 00:48:18,495 - mmdet - INFO - Epoch [1][28550/45697]	lr: 1.250e-05, eta: 2 days, 4:47:03, time: 0.282, data_time: 0.013, memory: 3036, loss_r_cls: 4.7109, loss_sub_cls: 19.8673, loss_obj_cls: 18.8774, loss_match: 3.9332, loss: 47.3888, grad_norm: 231.9175
2025-06-23 00:48:32,460 - mmdet - INFO - Epoch [1][28600/45697]	lr: 1.250e-05, eta: 2 days, 4:46:37, time: 0.279, data_time: 0.012, memory: 3036, loss_r_cls: 4.8302, loss_sub_cls: 19.7768, loss_obj_cls: 18.7827, loss_match: 3.6681, loss: 47.0577, grad_norm: 194.0506
2025-06-23 00:48:46,934 - mmdet - INFO - Epoch [1][28650/45697]	lr: 1.250e-05, eta: 2 days, 4:46:22, time: 0.289, data_time: 0.015, memory: 3036, loss_r_cls: 4.7337, loss_sub_cls: 19.8745, loss_obj_cls: 19.1060, loss_match: 4.1002, loss: 47.8144, grad_norm: 216.9027
2025-06-23 00:49:01,186 - mmdet - INFO - Epoch [1][28700/45697]	lr: 1.250e-05, eta: 2 days, 4:46:03, time: 0.285, data_time: 0.012, memory: 3036, loss_r_cls: 5.0649, loss_sub_cls: 19.3070, loss_obj_cls: 19.0534, loss_match: 3.8557, loss: 47.2810, grad_norm: 229.9695
2025-06-23 00:49:15,283 - mmdet - INFO - Epoch [1][28750/45697]	lr: 1.250e-05, eta: 2 days, 4:45:40, time: 0.282, data_time: 0.012, memory: 3036, loss_r_cls: 4.6460, loss_sub_cls: 19.6139, loss_obj_cls: 19.2178, loss_match: 4.0332, loss: 47.5109, grad_norm: 203.8512
2025-06-23 00:49:29,497 - mmdet - INFO - Epoch [1][28800/45697]	lr: 1.250e-05, eta: 2 days, 4:45:20, time: 0.284, data_time: 0.012, memory: 3036, loss_r_cls: 4.5059, loss_sub_cls: 19.2328, loss_obj_cls: 18.9416, loss_match: 4.2217, loss: 46.9020, grad_norm: 198.8254
2025-06-23 00:49:43,654 - mmdet - INFO - Epoch [1][28850/45697]	lr: 1.250e-05, eta: 2 days, 4:44:59, time: 0.283, data_time: 0.013, memory: 3036, loss_r_cls: 4.5104, loss_sub_cls: 19.5315, loss_obj_cls: 19.3922, loss_match: 3.8705, loss: 47.3047, grad_norm: 169.8138
2025-06-23 00:49:58,089 - mmdet - INFO - Epoch [1][28900/45697]	lr: 1.250e-05, eta: 2 days, 4:44:44, time: 0.289, data_time: 0.016, memory: 3036, loss_r_cls: 4.4539, loss_sub_cls: 19.6362, loss_obj_cls: 19.1057, loss_match: 3.9580, loss: 47.1539, grad_norm: 164.3649
2025-06-23 00:50:12,718 - mmdet - INFO - Epoch [1][28950/45697]	lr: 1.250e-05, eta: 2 days, 4:44:33, time: 0.293, data_time: 0.016, memory: 3036, loss_r_cls: 5.4668, loss_sub_cls: 19.5068, loss_obj_cls: 18.8309, loss_match: 3.6536, loss: 47.4582, grad_norm: 209.4273
2025-06-23 00:50:27,174 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:50:27,174 - mmdet - INFO - Epoch [1][29000/45697]	lr: 1.250e-05, eta: 2 days, 4:44:19, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 4.7640, loss_sub_cls: 19.2627, loss_obj_cls: 19.3792, loss_match: 4.1487, loss: 47.5547, grad_norm: 186.4471
2025-06-23 00:50:41,505 - mmdet - INFO - Epoch [1][29050/45697]	lr: 1.250e-05, eta: 2 days, 4:44:01, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 5.2144, loss_sub_cls: 19.2381, loss_obj_cls: 19.1211, loss_match: 4.7137, loss: 48.2874, grad_norm: 219.0799
2025-06-23 00:50:55,744 - mmdet - INFO - Epoch [1][29100/45697]	lr: 1.250e-05, eta: 2 days, 4:43:42, time: 0.285, data_time: 0.012, memory: 3036, loss_r_cls: 4.8228, loss_sub_cls: 20.0509, loss_obj_cls: 19.5645, loss_match: 4.0927, loss: 48.5308, grad_norm: 204.8345
2025-06-23 00:51:10,267 - mmdet - INFO - Epoch [1][29150/45697]	lr: 1.250e-05, eta: 2 days, 4:43:29, time: 0.290, data_time: 0.013, memory: 3036, loss_r_cls: 4.5147, loss_sub_cls: 19.9325, loss_obj_cls: 19.4867, loss_match: 3.7114, loss: 47.6453, grad_norm: 188.1943
2025-06-23 00:51:24,688 - mmdet - INFO - Epoch [1][29200/45697]	lr: 1.250e-05, eta: 2 days, 4:43:13, time: 0.288, data_time: 0.015, memory: 3036, loss_r_cls: 4.2616, loss_sub_cls: 19.5005, loss_obj_cls: 19.1242, loss_match: 4.5895, loss: 47.4758, grad_norm: 228.3050
2025-06-23 00:51:39,053 - mmdet - INFO - Epoch [1][29250/45697]	lr: 1.250e-05, eta: 2 days, 4:42:57, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.8008, loss_sub_cls: 19.5066, loss_obj_cls: 19.3297, loss_match: 4.4498, loss: 48.0869, grad_norm: 198.9050
2025-06-23 00:51:53,478 - mmdet - INFO - Epoch [1][29300/45697]	lr: 1.250e-05, eta: 2 days, 4:42:41, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 5.0394, loss_sub_cls: 19.7015, loss_obj_cls: 19.1777, loss_match: 4.2275, loss: 48.1462, grad_norm: 193.8438
2025-06-23 00:52:07,843 - mmdet - INFO - Epoch [1][29350/45697]	lr: 1.250e-05, eta: 2 days, 4:42:25, time: 0.287, data_time: 0.012, memory: 3036, loss_r_cls: 4.8070, loss_sub_cls: 19.7202, loss_obj_cls: 19.2990, loss_match: 3.7112, loss: 47.5374, grad_norm: 212.0757
2025-06-23 00:52:22,154 - mmdet - INFO - Epoch [1][29400/45697]	lr: 1.250e-05, eta: 2 days, 4:42:07, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 4.5975, loss_sub_cls: 19.9015, loss_obj_cls: 18.9495, loss_match: 3.6423, loss: 47.0908, grad_norm: 175.6443
2025-06-23 00:52:36,532 - mmdet - INFO - Epoch [1][29450/45697]	lr: 1.250e-05, eta: 2 days, 4:41:51, time: 0.288, data_time: 0.014, memory: 3036, loss_r_cls: 4.6366, loss_sub_cls: 19.9543, loss_obj_cls: 19.3712, loss_match: 4.5386, loss: 48.5008, grad_norm: 182.3391
2025-06-23 00:52:50,906 - mmdet - INFO - Epoch [1][29500/45697]	lr: 1.250e-05, eta: 2 days, 4:41:34, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 4.5628, loss_sub_cls: 19.8714, loss_obj_cls: 19.3334, loss_match: 4.2199, loss: 47.9876, grad_norm: 215.1818
2025-06-23 00:53:05,283 - mmdet - INFO - Epoch [1][29550/45697]	lr: 1.250e-05, eta: 2 days, 4:41:18, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 5.0372, loss_sub_cls: 19.6930, loss_obj_cls: 19.4025, loss_match: 3.1690, loss: 47.3016, grad_norm: 194.8934
2025-06-23 00:53:19,606 - mmdet - INFO - Epoch [1][29600/45697]	lr: 1.250e-05, eta: 2 days, 4:41:01, time: 0.286, data_time: 0.012, memory: 3036, loss_r_cls: 4.8884, loss_sub_cls: 19.3169, loss_obj_cls: 19.2274, loss_match: 3.7236, loss: 47.1563, grad_norm: 229.2145
2025-06-23 00:53:34,134 - mmdet - INFO - Epoch [1][29650/45697]	lr: 1.250e-05, eta: 2 days, 4:40:48, time: 0.291, data_time: 0.013, memory: 3036, loss_r_cls: 4.4596, loss_sub_cls: 19.7332, loss_obj_cls: 19.3701, loss_match: 4.1387, loss: 47.7017, grad_norm: 258.8916
2025-06-23 00:53:48,495 - mmdet - INFO - Epoch [1][29700/45697]	lr: 1.250e-05, eta: 2 days, 4:40:31, time: 0.287, data_time: 0.015, memory: 3036, loss_r_cls: 4.4642, loss_sub_cls: 19.3321, loss_obj_cls: 18.9093, loss_match: 5.0760, loss: 47.7816, grad_norm: 217.5140
2025-06-23 00:54:02,655 - mmdet - INFO - Epoch [1][29750/45697]	lr: 1.250e-05, eta: 2 days, 4:40:10, time: 0.283, data_time: 0.014, memory: 3036, loss_r_cls: 4.6577, loss_sub_cls: 19.7105, loss_obj_cls: 19.2049, loss_match: 4.3427, loss: 47.9159, grad_norm: 182.4819
2025-06-23 00:54:17,096 - mmdet - INFO - Epoch [1][29800/45697]	lr: 1.250e-05, eta: 2 days, 4:39:55, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.9651, loss_sub_cls: 19.9042, loss_obj_cls: 19.2604, loss_match: 4.1523, loss: 48.2819, grad_norm: 178.7874
2025-06-23 00:54:31,604 - mmdet - INFO - Epoch [1][29850/45697]	lr: 1.250e-05, eta: 2 days, 4:39:42, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 4.5303, loss_sub_cls: 19.6198, loss_obj_cls: 18.9305, loss_match: 4.1479, loss: 47.2285, grad_norm: 190.9570
2025-06-23 00:54:46,089 - mmdet - INFO - Epoch [1][29900/45697]	lr: 1.250e-05, eta: 2 days, 4:39:28, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 5.0596, loss_sub_cls: 19.1332, loss_obj_cls: 19.4006, loss_match: 3.7216, loss: 47.3149, grad_norm: 228.6525
2025-06-23 00:55:00,464 - mmdet - INFO - Epoch [1][29950/45697]	lr: 1.250e-05, eta: 2 days, 4:39:12, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 5.0130, loss_sub_cls: 19.6236, loss_obj_cls: 19.0270, loss_match: 3.6282, loss: 47.2917, grad_norm: 239.1458
2025-06-23 00:55:14,881 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 00:55:14,882 - mmdet - INFO - Epoch [1][30000/45697]	lr: 1.250e-05, eta: 2 days, 4:38:56, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.8258, loss_sub_cls: 19.5749, loss_obj_cls: 19.3835, loss_match: 4.2541, loss: 48.0382, grad_norm: 207.4900
2025-06-23 00:55:29,156 - mmdet - INFO - Epoch [1][30050/45697]	lr: 1.250e-05, eta: 2 days, 4:38:38, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.5284, loss_sub_cls: 19.2594, loss_obj_cls: 19.2965, loss_match: 3.5899, loss: 46.6742, grad_norm: 191.5437
2025-06-23 00:55:43,471 - mmdet - INFO - Epoch [1][30100/45697]	lr: 1.250e-05, eta: 2 days, 4:38:20, time: 0.286, data_time: 0.013, memory: 3036, loss_r_cls: 4.1856, loss_sub_cls: 19.5310, loss_obj_cls: 19.1170, loss_match: 4.0040, loss: 46.8376, grad_norm: 181.2578
2025-06-23 00:55:57,831 - mmdet - INFO - Epoch [1][30150/45697]	lr: 1.250e-05, eta: 2 days, 4:38:04, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 5.2746, loss_sub_cls: 19.5594, loss_obj_cls: 18.9500, loss_match: 4.5617, loss: 48.3457, grad_norm: 202.3031
2025-06-23 00:56:11,920 - mmdet - INFO - Epoch [1][30200/45697]	lr: 1.250e-05, eta: 2 days, 4:37:41, time: 0.282, data_time: 0.014, memory: 3036, loss_r_cls: 4.9917, loss_sub_cls: 19.4087, loss_obj_cls: 18.9618, loss_match: 3.9755, loss: 47.3376, grad_norm: 183.3608
2025-06-23 00:56:26,158 - mmdet - INFO - Epoch [1][30250/45697]	lr: 1.250e-05, eta: 2 days, 4:37:22, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 4.5219, loss_sub_cls: 19.4957, loss_obj_cls: 18.8852, loss_match: 3.6851, loss: 46.5879, grad_norm: 172.6082
2025-06-23 00:56:40,454 - mmdet - INFO - Epoch [1][30300/45697]	lr: 1.250e-05, eta: 2 days, 4:37:04, time: 0.286, data_time: 0.014, memory: 3036, loss_r_cls: 4.6416, loss_sub_cls: 19.7380, loss_obj_cls: 19.3454, loss_match: 3.7668, loss: 47.4919, grad_norm: 258.7448
2025-06-23 00:56:54,928 - mmdet - INFO - Epoch [1][30350/45697]	lr: 1.250e-05, eta: 2 days, 4:36:50, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.5490, loss_sub_cls: 19.7996, loss_obj_cls: 19.1883, loss_match: 4.4480, loss: 47.9849, grad_norm: 195.0172
2025-06-23 00:57:09,142 - mmdet - INFO - Epoch [1][30400/45697]	lr: 1.250e-05, eta: 2 days, 4:36:30, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.6364, loss_sub_cls: 19.6219, loss_obj_cls: 19.1074, loss_match: 4.0851, loss: 47.4508, grad_norm: 225.3906
2025-06-23 00:57:23,459 - mmdet - INFO - Epoch [1][30450/45697]	lr: 1.250e-05, eta: 2 days, 4:36:13, time: 0.286, data_time: 0.011, memory: 3036, loss_r_cls: 4.9362, loss_sub_cls: 19.7170, loss_obj_cls: 19.3428, loss_match: 3.5482, loss: 47.5442, grad_norm: 252.7883
2025-06-23 00:57:37,901 - mmdet - INFO - Epoch [1][30500/45697]	lr: 1.250e-05, eta: 2 days, 4:35:58, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 5.0739, loss_sub_cls: 19.8832, loss_obj_cls: 19.3432, loss_match: 3.8409, loss: 48.1412, grad_norm: 274.2617
2025-06-23 00:57:52,376 - mmdet - INFO - Epoch [1][30550/45697]	lr: 1.250e-05, eta: 2 days, 4:35:44, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.2987, loss_sub_cls: 19.4803, loss_obj_cls: 19.0111, loss_match: 3.5733, loss: 46.3634, grad_norm: 232.2198
2025-06-23 00:58:06,749 - mmdet - INFO - Epoch [1][30600/45697]	lr: 1.250e-05, eta: 2 days, 4:35:28, time: 0.287, data_time: 0.012, memory: 3036, loss_r_cls: 4.5118, loss_sub_cls: 19.6486, loss_obj_cls: 19.0559, loss_match: 3.9494, loss: 47.1657, grad_norm: 196.0351
2025-06-23 00:58:20,924 - mmdet - INFO - Epoch [1][30650/45697]	lr: 1.250e-05, eta: 2 days, 4:35:07, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.4640, loss_sub_cls: 19.7048, loss_obj_cls: 19.4308, loss_match: 3.3213, loss: 46.9210, grad_norm: 198.7497
2025-06-23 00:58:35,169 - mmdet - INFO - Epoch [1][30700/45697]	lr: 1.250e-05, eta: 2 days, 4:34:48, time: 0.285, data_time: 0.011, memory: 3036, loss_r_cls: 4.6639, loss_sub_cls: 19.3107, loss_obj_cls: 18.7564, loss_match: 4.3456, loss: 47.0767, grad_norm: 239.9555
2025-06-23 00:58:49,384 - mmdet - INFO - Epoch [1][30750/45697]	lr: 1.250e-05, eta: 2 days, 4:34:29, time: 0.284, data_time: 0.012, memory: 3036, loss_r_cls: 5.0964, loss_sub_cls: 19.7048, loss_obj_cls: 19.0598, loss_match: 3.3332, loss: 47.1942, grad_norm: 216.1199
2025-06-23 00:59:03,785 - mmdet - INFO - Epoch [1][30800/45697]	lr: 1.250e-05, eta: 2 days, 4:34:13, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.5620, loss_sub_cls: 20.2876, loss_obj_cls: 19.2616, loss_match: 5.0433, loss: 49.1544, grad_norm: 257.3596
2025-06-23 00:59:18,185 - mmdet - INFO - Epoch [1][30850/45697]	lr: 1.250e-05, eta: 2 days, 4:33:57, time: 0.288, data_time: 0.015, memory: 3036, loss_r_cls: 4.5559, loss_sub_cls: 19.7076, loss_obj_cls: 19.3968, loss_match: 4.1265, loss: 47.7868, grad_norm: 228.3959
2025-06-23 00:59:32,551 - mmdet - INFO - Epoch [1][30900/45697]	lr: 1.250e-05, eta: 2 days, 4:33:41, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 5.2411, loss_sub_cls: 19.9805, loss_obj_cls: 19.4221, loss_match: 3.5877, loss: 48.2315, grad_norm: 181.9014
2025-06-23 00:59:47,088 - mmdet - INFO - Epoch [1][30950/45697]	lr: 1.250e-05, eta: 2 days, 4:33:28, time: 0.291, data_time: 0.014, memory: 3036, loss_r_cls: 5.0398, loss_sub_cls: 19.8947, loss_obj_cls: 19.1646, loss_match: 4.2305, loss: 48.3296, grad_norm: 200.4295
2025-06-23 01:00:01,288 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:00:01,289 - mmdet - INFO - Epoch [1][31000/45697]	lr: 1.250e-05, eta: 2 days, 4:33:09, time: 0.284, data_time: 0.012, memory: 3036, loss_r_cls: 4.3603, loss_sub_cls: 19.6225, loss_obj_cls: 19.1720, loss_match: 4.0001, loss: 47.1549, grad_norm: 235.7632
2025-06-23 01:00:15,680 - mmdet - INFO - Epoch [1][31050/45697]	lr: 1.250e-05, eta: 2 days, 4:32:53, time: 0.288, data_time: 0.012, memory: 3036, loss_r_cls: 4.9009, loss_sub_cls: 19.4149, loss_obj_cls: 19.1920, loss_match: 3.4128, loss: 46.9206, grad_norm: 210.6488
2025-06-23 01:00:30,045 - mmdet - INFO - Epoch [1][31100/45697]	lr: 1.250e-05, eta: 2 days, 4:32:36, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 4.3998, loss_sub_cls: 19.8264, loss_obj_cls: 19.2871, loss_match: 4.3690, loss: 47.8823, grad_norm: 220.6810
2025-06-23 01:00:44,311 - mmdet - INFO - Epoch [1][31150/45697]	lr: 1.250e-05, eta: 2 days, 4:32:18, time: 0.285, data_time: 0.013, memory: 3036, loss_r_cls: 5.3173, loss_sub_cls: 19.6018, loss_obj_cls: 19.0807, loss_match: 3.6820, loss: 47.6818, grad_norm: 204.7024
2025-06-23 01:00:58,808 - mmdet - INFO - Epoch [1][31200/45697]	lr: 1.250e-05, eta: 2 days, 4:32:05, time: 0.290, data_time: 0.014, memory: 3036, loss_r_cls: 4.6158, loss_sub_cls: 19.5433, loss_obj_cls: 19.0676, loss_match: 3.7082, loss: 46.9348, grad_norm: 185.0417
2025-06-23 01:01:13,241 - mmdet - INFO - Epoch [1][31250/45697]	lr: 1.250e-05, eta: 2 days, 4:31:50, time: 0.289, data_time: 0.015, memory: 3036, loss_r_cls: 5.0368, loss_sub_cls: 19.3012, loss_obj_cls: 19.5280, loss_match: 3.9142, loss: 47.7802, grad_norm: 216.9003
2025-06-23 01:01:27,319 - mmdet - INFO - Epoch [1][31300/45697]	lr: 1.250e-05, eta: 2 days, 4:31:27, time: 0.282, data_time: 0.012, memory: 3036, loss_r_cls: 4.4767, loss_sub_cls: 19.6906, loss_obj_cls: 19.1148, loss_match: 4.3484, loss: 47.6306, grad_norm: 210.5158
2025-06-23 01:01:41,347 - mmdet - INFO - Epoch [1][31350/45697]	lr: 1.250e-05, eta: 2 days, 4:31:04, time: 0.281, data_time: 0.012, memory: 3036, loss_r_cls: 4.9961, loss_sub_cls: 19.6424, loss_obj_cls: 19.3072, loss_match: 4.4546, loss: 48.4003, grad_norm: 242.0525
2025-06-23 01:01:55,425 - mmdet - INFO - Epoch [1][31400/45697]	lr: 1.250e-05, eta: 2 days, 4:30:42, time: 0.282, data_time: 0.012, memory: 3036, loss_r_cls: 4.8007, loss_sub_cls: 19.9478, loss_obj_cls: 19.3607, loss_match: 4.0850, loss: 48.1942, grad_norm: 239.7412
2025-06-23 01:02:09,618 - mmdet - INFO - Epoch [1][31450/45697]	lr: 1.250e-05, eta: 2 days, 4:30:22, time: 0.284, data_time: 0.014, memory: 3036, loss_r_cls: 4.3835, loss_sub_cls: 19.9150, loss_obj_cls: 19.5391, loss_match: 3.8389, loss: 47.6766, grad_norm: 182.6047
2025-06-23 01:02:23,746 - mmdet - INFO - Epoch [1][31500/45697]	lr: 1.250e-05, eta: 2 days, 4:30:01, time: 0.283, data_time: 0.013, memory: 3036, loss_r_cls: 4.7690, loss_sub_cls: 19.8237, loss_obj_cls: 19.1814, loss_match: 3.7753, loss: 47.5494, grad_norm: 182.8602
2025-06-23 01:02:37,933 - mmdet - INFO - Epoch [1][31550/45697]	lr: 1.250e-05, eta: 2 days, 4:29:41, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.9400, loss_sub_cls: 19.5181, loss_obj_cls: 19.4260, loss_match: 3.7666, loss: 47.6507, grad_norm: 223.9326
2025-06-23 01:02:52,762 - mmdet - INFO - Epoch [1][31600/45697]	lr: 1.250e-05, eta: 2 days, 4:29:34, time: 0.297, data_time: 0.015, memory: 3036, loss_r_cls: 4.4956, loss_sub_cls: 19.8257, loss_obj_cls: 18.9467, loss_match: 4.1477, loss: 47.4156, grad_norm: 209.4860
2025-06-23 01:03:07,636 - mmdet - INFO - Epoch [1][31650/45697]	lr: 1.250e-05, eta: 2 days, 4:29:29, time: 0.297, data_time: 0.013, memory: 3036, loss_r_cls: 4.4449, loss_sub_cls: 19.2336, loss_obj_cls: 19.0913, loss_match: 3.8291, loss: 46.5988, grad_norm: 185.5898
2025-06-23 01:03:22,683 - mmdet - INFO - Epoch [1][31700/45697]	lr: 1.250e-05, eta: 2 days, 4:29:26, time: 0.301, data_time: 0.015, memory: 3036, loss_r_cls: 5.2986, loss_sub_cls: 19.3630, loss_obj_cls: 19.0221, loss_match: 4.2231, loss: 47.9069, grad_norm: 239.2398
2025-06-23 01:03:37,239 - mmdet - INFO - Epoch [1][31750/45697]	lr: 1.250e-05, eta: 2 days, 4:29:14, time: 0.291, data_time: 0.013, memory: 3036, loss_r_cls: 4.9896, loss_sub_cls: 19.4138, loss_obj_cls: 19.3207, loss_match: 3.7764, loss: 47.5005, grad_norm: 232.0408
2025-06-23 01:03:51,692 - mmdet - INFO - Epoch [1][31800/45697]	lr: 1.250e-05, eta: 2 days, 4:29:00, time: 0.289, data_time: 0.014, memory: 3036, loss_r_cls: 4.6615, loss_sub_cls: 19.1017, loss_obj_cls: 18.6943, loss_match: 3.9583, loss: 46.4158, grad_norm: 239.6492
2025-06-23 01:04:06,428 - mmdet - INFO - Epoch [1][31850/45697]	lr: 1.250e-05, eta: 2 days, 4:28:51, time: 0.295, data_time: 0.014, memory: 3036, loss_r_cls: 5.0823, loss_sub_cls: 19.4485, loss_obj_cls: 18.6448, loss_match: 3.6667, loss: 46.8423, grad_norm: 171.8954
2025-06-23 01:04:21,846 - mmdet - INFO - Epoch [1][31900/45697]	lr: 1.250e-05, eta: 2 days, 4:28:56, time: 0.308, data_time: 0.014, memory: 3036, loss_r_cls: 4.6033, loss_sub_cls: 19.8999, loss_obj_cls: 19.0532, loss_match: 4.2651, loss: 47.8215, grad_norm: 248.3238
2025-06-23 01:04:36,916 - mmdet - INFO - Epoch [1][31950/45697]	lr: 1.250e-05, eta: 2 days, 4:28:54, time: 0.301, data_time: 0.015, memory: 3036, loss_r_cls: 4.3611, loss_sub_cls: 19.8969, loss_obj_cls: 19.1901, loss_match: 3.8422, loss: 47.2903, grad_norm: 195.6161
2025-06-23 01:04:51,814 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:04:51,815 - mmdet - INFO - Epoch [1][32000/45697]	lr: 1.250e-05, eta: 2 days, 4:28:49, time: 0.298, data_time: 0.014, memory: 3036, loss_r_cls: 4.9515, loss_sub_cls: 19.5519, loss_obj_cls: 19.3048, loss_match: 4.4222, loss: 48.2304, grad_norm: 210.2078
2025-06-23 01:05:06,012 - mmdet - INFO - Epoch [1][32050/45697]	lr: 1.250e-05, eta: 2 days, 4:28:29, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.2293, loss_sub_cls: 20.1850, loss_obj_cls: 19.0446, loss_match: 3.5669, loss: 47.0257, grad_norm: 205.0594
2025-06-23 01:05:20,759 - mmdet - INFO - Epoch [1][32100/45697]	lr: 1.250e-05, eta: 2 days, 4:28:21, time: 0.295, data_time: 0.014, memory: 3036, loss_r_cls: 5.3859, loss_sub_cls: 20.3220, loss_obj_cls: 19.1613, loss_match: 3.6924, loss: 48.5615, grad_norm: 246.5651
2025-06-23 01:05:35,789 - mmdet - INFO - Epoch [1][32150/45697]	lr: 1.250e-05, eta: 2 days, 4:28:18, time: 0.301, data_time: 0.013, memory: 3036, loss_r_cls: 4.4269, loss_sub_cls: 19.9923, loss_obj_cls: 19.1926, loss_match: 2.9947, loss: 46.6063, grad_norm: 193.6386
2025-06-23 01:05:51,083 - mmdet - INFO - Epoch [1][32200/45697]	lr: 1.250e-05, eta: 2 days, 4:28:20, time: 0.306, data_time: 0.015, memory: 3036, loss_r_cls: 4.6522, loss_sub_cls: 19.4961, loss_obj_cls: 19.3940, loss_match: 3.8905, loss: 47.4328, grad_norm: 232.8884
2025-06-23 01:06:06,218 - mmdet - INFO - Epoch [1][32250/45697]	lr: 1.250e-05, eta: 2 days, 4:28:20, time: 0.303, data_time: 0.015, memory: 3036, loss_r_cls: 4.8558, loss_sub_cls: 19.7751, loss_obj_cls: 19.1881, loss_match: 4.0291, loss: 47.8481, grad_norm: 217.5252
2025-06-23 01:06:20,945 - mmdet - INFO - Epoch [1][32300/45697]	lr: 1.250e-05, eta: 2 days, 4:28:11, time: 0.295, data_time: 0.014, memory: 3036, loss_r_cls: 4.5197, loss_sub_cls: 19.6263, loss_obj_cls: 19.1261, loss_match: 4.4049, loss: 47.6769, grad_norm: 199.0935
2025-06-23 01:06:35,553 - mmdet - INFO - Epoch [1][32350/45697]	lr: 1.250e-05, eta: 2 days, 4:27:59, time: 0.292, data_time: 0.014, memory: 3036, loss_r_cls: 4.4152, loss_sub_cls: 19.6673, loss_obj_cls: 19.1130, loss_match: 3.9061, loss: 47.1016, grad_norm: 185.4014
2025-06-23 01:06:51,098 - mmdet - INFO - Epoch [1][32400/45697]	lr: 1.250e-05, eta: 2 days, 4:28:06, time: 0.311, data_time: 0.013, memory: 3036, loss_r_cls: 4.4802, loss_sub_cls: 19.5915, loss_obj_cls: 18.9206, loss_match: 3.9169, loss: 46.9093, grad_norm: 215.5256
2025-06-23 01:07:05,804 - mmdet - INFO - Epoch [1][32450/45697]	lr: 1.250e-05, eta: 2 days, 4:27:57, time: 0.294, data_time: 0.014, memory: 3036, loss_r_cls: 5.2545, loss_sub_cls: 19.7500, loss_obj_cls: 19.3108, loss_match: 4.0680, loss: 48.3833, grad_norm: 191.2817
2025-06-23 01:07:20,514 - mmdet - INFO - Epoch [1][32500/45697]	lr: 1.250e-05, eta: 2 days, 4:27:47, time: 0.294, data_time: 0.014, memory: 3036, loss_r_cls: 4.4129, loss_sub_cls: 19.8887, loss_obj_cls: 18.8588, loss_match: 3.7151, loss: 46.8754, grad_norm: 196.7490
2025-06-23 01:07:35,503 - mmdet - INFO - Epoch [1][32550/45697]	lr: 1.250e-05, eta: 2 days, 4:27:44, time: 0.300, data_time: 0.015, memory: 3036, loss_r_cls: 4.1825, loss_sub_cls: 19.7360, loss_obj_cls: 19.0285, loss_match: 4.6001, loss: 47.5471, grad_norm: 205.6679
2025-06-23 01:07:50,514 - mmdet - INFO - Epoch [1][32600/45697]	lr: 1.250e-05, eta: 2 days, 4:27:40, time: 0.300, data_time: 0.014, memory: 3036, loss_r_cls: 4.9703, loss_sub_cls: 19.7310, loss_obj_cls: 19.6009, loss_match: 4.6097, loss: 48.9119, grad_norm: 184.9132
2025-06-23 01:08:04,459 - mmdet - INFO - Epoch [1][32650/45697]	lr: 1.250e-05, eta: 2 days, 4:27:15, time: 0.279, data_time: 0.013, memory: 3036, loss_r_cls: 4.5143, loss_sub_cls: 20.1431, loss_obj_cls: 19.5249, loss_match: 4.9370, loss: 49.1193, grad_norm: 210.8020
2025-06-23 01:08:18,176 - mmdet - INFO - Epoch [1][32700/45697]	lr: 1.250e-05, eta: 2 days, 4:26:46, time: 0.274, data_time: 0.016, memory: 3036, loss_r_cls: 4.4743, loss_sub_cls: 19.9701, loss_obj_cls: 18.9812, loss_match: 4.4878, loss: 47.9133, grad_norm: 173.2108
2025-06-23 01:08:31,596 - mmdet - INFO - Epoch [1][32750/45697]	lr: 1.250e-05, eta: 2 days, 4:26:11, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 4.6775, loss_sub_cls: 20.1698, loss_obj_cls: 19.3559, loss_match: 3.8702, loss: 48.0733, grad_norm: 207.1371
2025-06-23 01:08:45,245 - mmdet - INFO - Epoch [1][32800/45697]	lr: 1.250e-05, eta: 2 days, 4:25:40, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 5.1188, loss_sub_cls: 19.7224, loss_obj_cls: 19.1012, loss_match: 5.1937, loss: 49.1361, grad_norm: 235.1985
2025-06-23 01:08:58,762 - mmdet - INFO - Epoch [1][32850/45697]	lr: 1.250e-05, eta: 2 days, 4:25:07, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 5.0314, loss_sub_cls: 19.5652, loss_obj_cls: 19.1376, loss_match: 4.3839, loss: 48.1180, grad_norm: 187.0771
2025-06-23 01:09:12,241 - mmdet - INFO - Epoch [1][32900/45697]	lr: 1.250e-05, eta: 2 days, 4:24:33, time: 0.270, data_time: 0.012, memory: 3036, loss_r_cls: 5.3428, loss_sub_cls: 19.9939, loss_obj_cls: 19.2294, loss_match: 3.7211, loss: 48.2871, grad_norm: 215.3053
2025-06-23 01:09:26,080 - mmdet - INFO - Epoch [1][32950/45697]	lr: 1.250e-05, eta: 2 days, 4:24:06, time: 0.277, data_time: 0.013, memory: 3036, loss_r_cls: 4.9631, loss_sub_cls: 19.4015, loss_obj_cls: 19.0158, loss_match: 3.6139, loss: 46.9944, grad_norm: 224.9633
2025-06-23 01:09:39,619 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:09:39,619 - mmdet - INFO - Epoch [1][33000/45697]	lr: 1.250e-05, eta: 2 days, 4:23:34, time: 0.271, data_time: 0.014, memory: 3036, loss_r_cls: 4.9143, loss_sub_cls: 19.6359, loss_obj_cls: 19.0506, loss_match: 4.5369, loss: 48.1378, grad_norm: 236.1139
2025-06-23 01:09:53,489 - mmdet - INFO - Epoch [1][33050/45697]	lr: 1.250e-05, eta: 2 days, 4:23:08, time: 0.277, data_time: 0.013, memory: 3036, loss_r_cls: 4.6553, loss_sub_cls: 19.5497, loss_obj_cls: 18.7505, loss_match: 4.0125, loss: 46.9680, grad_norm: 205.9553
2025-06-23 01:10:07,198 - mmdet - INFO - Epoch [1][33100/45697]	lr: 1.250e-05, eta: 2 days, 4:22:38, time: 0.274, data_time: 0.015, memory: 3036, loss_r_cls: 4.5322, loss_sub_cls: 19.7738, loss_obj_cls: 19.2240, loss_match: 4.0538, loss: 47.5838, grad_norm: 198.0657
2025-06-23 01:10:20,881 - mmdet - INFO - Epoch [1][33150/45697]	lr: 1.250e-05, eta: 2 days, 4:22:09, time: 0.274, data_time: 0.014, memory: 3036, loss_r_cls: 4.6262, loss_sub_cls: 19.4466, loss_obj_cls: 18.7479, loss_match: 3.5903, loss: 46.4111, grad_norm: 203.6568
2025-06-23 01:10:34,592 - mmdet - INFO - Epoch [1][33200/45697]	lr: 1.250e-05, eta: 2 days, 4:21:40, time: 0.274, data_time: 0.015, memory: 3036, loss_r_cls: 4.8481, loss_sub_cls: 19.9396, loss_obj_cls: 19.0558, loss_match: 3.7390, loss: 47.5825, grad_norm: 216.9205
2025-06-23 01:10:48,233 - mmdet - INFO - Epoch [1][33250/45697]	lr: 1.250e-05, eta: 2 days, 4:21:10, time: 0.273, data_time: 0.013, memory: 3036, loss_r_cls: 4.8768, loss_sub_cls: 19.6495, loss_obj_cls: 19.2397, loss_match: 4.1189, loss: 47.8849, grad_norm: 247.9563
2025-06-23 01:11:02,355 - mmdet - INFO - Epoch [1][33300/45697]	lr: 1.250e-05, eta: 2 days, 4:20:49, time: 0.282, data_time: 0.017, memory: 3036, loss_r_cls: 4.8281, loss_sub_cls: 19.4762, loss_obj_cls: 19.4739, loss_match: 4.3681, loss: 48.1464, grad_norm: 220.1809
2025-06-23 01:11:15,880 - mmdet - INFO - Epoch [1][33350/45697]	lr: 1.250e-05, eta: 2 days, 4:20:16, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.8822, loss_sub_cls: 19.7831, loss_obj_cls: 19.0020, loss_match: 4.1101, loss: 47.7774, grad_norm: 219.9640
2025-06-23 01:11:29,707 - mmdet - INFO - Epoch [1][33400/45697]	lr: 1.250e-05, eta: 2 days, 4:19:50, time: 0.277, data_time: 0.015, memory: 3036, loss_r_cls: 4.8307, loss_sub_cls: 19.5696, loss_obj_cls: 19.3026, loss_match: 3.8678, loss: 47.5708, grad_norm: 200.5700
2025-06-23 01:11:43,309 - mmdet - INFO - Epoch [1][33450/45697]	lr: 1.250e-05, eta: 2 days, 4:19:19, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.8286, loss_sub_cls: 19.5603, loss_obj_cls: 19.4129, loss_match: 4.2651, loss: 48.0669, grad_norm: 220.0594
2025-06-23 01:11:56,972 - mmdet - INFO - Epoch [1][33500/45697]	lr: 1.250e-05, eta: 2 days, 4:18:49, time: 0.273, data_time: 0.015, memory: 3036, loss_r_cls: 4.9566, loss_sub_cls: 19.5852, loss_obj_cls: 19.0133, loss_match: 4.4543, loss: 48.0095, grad_norm: 209.3956
2025-06-23 01:12:10,753 - mmdet - INFO - Epoch [1][33550/45697]	lr: 1.250e-05, eta: 2 days, 4:18:22, time: 0.276, data_time: 0.016, memory: 3036, loss_r_cls: 4.6634, loss_sub_cls: 19.3287, loss_obj_cls: 19.1858, loss_match: 4.0201, loss: 47.1980, grad_norm: 176.3258
2025-06-23 01:12:24,584 - mmdet - INFO - Epoch [1][33600/45697]	lr: 1.250e-05, eta: 2 days, 4:17:55, time: 0.277, data_time: 0.014, memory: 3036, loss_r_cls: 4.4790, loss_sub_cls: 20.0186, loss_obj_cls: 18.7927, loss_match: 4.4812, loss: 47.7715, grad_norm: 206.3211
2025-06-23 01:12:38,286 - mmdet - INFO - Epoch [1][33650/45697]	lr: 1.250e-05, eta: 2 days, 4:17:27, time: 0.274, data_time: 0.013, memory: 3036, loss_r_cls: 4.6336, loss_sub_cls: 19.3302, loss_obj_cls: 18.9564, loss_match: 5.4271, loss: 48.3473, grad_norm: 243.3086
2025-06-23 01:12:52,041 - mmdet - INFO - Epoch [1][33700/45697]	lr: 1.250e-05, eta: 2 days, 4:16:59, time: 0.275, data_time: 0.014, memory: 3036, loss_r_cls: 4.6703, loss_sub_cls: 19.6237, loss_obj_cls: 18.8278, loss_match: 4.6268, loss: 47.7485, grad_norm: 204.7010
2025-06-23 01:13:05,790 - mmdet - INFO - Epoch [1][33750/45697]	lr: 1.250e-05, eta: 2 days, 4:16:31, time: 0.275, data_time: 0.015, memory: 3036, loss_r_cls: 4.9633, loss_sub_cls: 19.7666, loss_obj_cls: 19.0703, loss_match: 3.8021, loss: 47.6024, grad_norm: 171.7617
2025-06-23 01:13:19,322 - mmdet - INFO - Epoch [1][33800/45697]	lr: 1.250e-05, eta: 2 days, 4:15:59, time: 0.271, data_time: 0.014, memory: 3036, loss_r_cls: 4.8510, loss_sub_cls: 19.9003, loss_obj_cls: 19.6011, loss_match: 4.7170, loss: 49.0694, grad_norm: 247.7461
2025-06-23 01:13:32,747 - mmdet - INFO - Epoch [1][33850/45697]	lr: 1.250e-05, eta: 2 days, 4:15:25, time: 0.268, data_time: 0.012, memory: 3036, loss_r_cls: 4.8938, loss_sub_cls: 19.9546, loss_obj_cls: 19.0180, loss_match: 5.5045, loss: 49.3710, grad_norm: 230.6294
2025-06-23 01:13:46,525 - mmdet - INFO - Epoch [1][33900/45697]	lr: 1.250e-05, eta: 2 days, 4:14:58, time: 0.276, data_time: 0.013, memory: 3036, loss_r_cls: 4.6204, loss_sub_cls: 19.7086, loss_obj_cls: 19.1714, loss_match: 5.1330, loss: 48.6334, grad_norm: 250.2657
2025-06-23 01:14:00,870 - mmdet - INFO - Epoch [1][33950/45697]	lr: 1.250e-05, eta: 2 days, 4:14:42, time: 0.287, data_time: 0.013, memory: 3036, loss_r_cls: 5.1530, loss_sub_cls: 19.2684, loss_obj_cls: 19.2833, loss_match: 4.4265, loss: 48.1311, grad_norm: 208.6161
2025-06-23 01:14:14,329 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:14:14,330 - mmdet - INFO - Epoch [1][34000/45697]	lr: 1.250e-05, eta: 2 days, 4:14:09, time: 0.269, data_time: 0.015, memory: 3036, loss_r_cls: 4.0934, loss_sub_cls: 19.5862, loss_obj_cls: 19.4480, loss_match: 5.3130, loss: 48.4406, grad_norm: 224.6883
2025-06-23 01:14:27,854 - mmdet - INFO - Epoch [1][34050/45697]	lr: 1.250e-05, eta: 2 days, 4:13:37, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 4.6586, loss_sub_cls: 19.6318, loss_obj_cls: 19.4140, loss_match: 4.4761, loss: 48.1805, grad_norm: 200.2968
2025-06-23 01:14:41,674 - mmdet - INFO - Epoch [1][34100/45697]	lr: 1.250e-05, eta: 2 days, 4:13:11, time: 0.276, data_time: 0.014, memory: 3036, loss_r_cls: 5.0140, loss_sub_cls: 19.7598, loss_obj_cls: 18.9931, loss_match: 4.5438, loss: 48.3107, grad_norm: 220.6780
2025-06-23 01:14:55,081 - mmdet - INFO - Epoch [1][34150/45697]	lr: 1.250e-05, eta: 2 days, 4:12:37, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 5.1087, loss_sub_cls: 19.6019, loss_obj_cls: 19.0465, loss_match: 5.1156, loss: 48.8727, grad_norm: 238.3958
2025-06-23 01:15:08,474 - mmdet - INFO - Epoch [1][34200/45697]	lr: 1.250e-05, eta: 2 days, 4:12:03, time: 0.268, data_time: 0.014, memory: 3036, loss_r_cls: 4.9071, loss_sub_cls: 19.8419, loss_obj_cls: 19.3078, loss_match: 4.6513, loss: 48.7080, grad_norm: 212.2430
2025-06-23 01:15:22,131 - mmdet - INFO - Epoch [1][34250/45697]	lr: 1.250e-05, eta: 2 days, 4:11:34, time: 0.273, data_time: 0.012, memory: 3036, loss_r_cls: 4.6953, loss_sub_cls: 19.8934, loss_obj_cls: 19.1130, loss_match: 3.7592, loss: 47.4609, grad_norm: 192.0284
2025-06-23 01:15:35,647 - mmdet - INFO - Epoch [1][34300/45697]	lr: 1.250e-05, eta: 2 days, 4:11:02, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.7894, loss_sub_cls: 19.5374, loss_obj_cls: 18.8826, loss_match: 4.4112, loss: 47.6207, grad_norm: 214.1085
2025-06-23 01:15:49,187 - mmdet - INFO - Epoch [1][34350/45697]	lr: 1.250e-05, eta: 2 days, 4:10:31, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.8055, loss_sub_cls: 19.7030, loss_obj_cls: 19.0496, loss_match: 4.5554, loss: 48.1135, grad_norm: 239.6637
2025-06-23 01:16:03,002 - mmdet - INFO - Epoch [1][34400/45697]	lr: 1.250e-05, eta: 2 days, 4:10:05, time: 0.276, data_time: 0.015, memory: 3036, loss_r_cls: 4.6639, loss_sub_cls: 20.2180, loss_obj_cls: 19.3908, loss_match: 4.3986, loss: 48.6713, grad_norm: 217.4414
2025-06-23 01:16:16,495 - mmdet - INFO - Epoch [1][34450/45697]	lr: 1.250e-05, eta: 2 days, 4:09:33, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 4.8499, loss_sub_cls: 19.6219, loss_obj_cls: 18.7988, loss_match: 4.6930, loss: 47.9636, grad_norm: 253.1366
2025-06-23 01:16:30,063 - mmdet - INFO - Epoch [1][34500/45697]	lr: 1.250e-05, eta: 2 days, 4:09:02, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 5.3974, loss_sub_cls: 19.6915, loss_obj_cls: 18.8243, loss_match: 4.6601, loss: 48.5732, grad_norm: 245.1231
2025-06-23 01:16:44,964 - mmdet - INFO - Epoch [1][34550/45697]	lr: 1.250e-05, eta: 2 days, 4:08:57, time: 0.298, data_time: 0.014, memory: 3036, loss_r_cls: 4.6425, loss_sub_cls: 19.8239, loss_obj_cls: 19.1969, loss_match: 4.1829, loss: 47.8462, grad_norm: 219.5723
2025-06-23 01:16:59,325 - mmdet - INFO - Epoch [1][34600/45697]	lr: 1.250e-05, eta: 2 days, 4:08:42, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 5.3347, loss_sub_cls: 20.2950, loss_obj_cls: 18.8776, loss_match: 4.1008, loss: 48.6081, grad_norm: 237.2246
2025-06-23 01:17:12,969 - mmdet - INFO - Epoch [1][34650/45697]	lr: 1.250e-05, eta: 2 days, 4:08:12, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 4.7447, loss_sub_cls: 19.6294, loss_obj_cls: 19.1627, loss_match: 3.9656, loss: 47.5024, grad_norm: 224.8823
2025-06-23 01:17:26,417 - mmdet - INFO - Epoch [1][34700/45697]	lr: 1.250e-05, eta: 2 days, 4:07:40, time: 0.269, data_time: 0.012, memory: 3036, loss_r_cls: 4.8280, loss_sub_cls: 19.7094, loss_obj_cls: 19.0191, loss_match: 5.5962, loss: 49.1526, grad_norm: 317.3895
2025-06-23 01:17:40,158 - mmdet - INFO - Epoch [1][34750/45697]	lr: 1.250e-05, eta: 2 days, 4:07:13, time: 0.275, data_time: 0.015, memory: 3036, loss_r_cls: 4.3351, loss_sub_cls: 19.7598, loss_obj_cls: 19.2408, loss_match: 4.9440, loss: 48.2797, grad_norm: 238.3192
2025-06-23 01:17:53,894 - mmdet - INFO - Epoch [1][34800/45697]	lr: 1.250e-05, eta: 2 days, 4:06:46, time: 0.275, data_time: 0.015, memory: 3036, loss_r_cls: 5.0155, loss_sub_cls: 19.6440, loss_obj_cls: 19.2834, loss_match: 4.9440, loss: 48.8869, grad_norm: 210.9842
2025-06-23 01:18:07,464 - mmdet - INFO - Epoch [1][34850/45697]	lr: 1.250e-05, eta: 2 days, 4:06:15, time: 0.271, data_time: 0.015, memory: 3036, loss_r_cls: 4.9406, loss_sub_cls: 19.7579, loss_obj_cls: 19.2908, loss_match: 5.2082, loss: 49.1975, grad_norm: 224.7075
2025-06-23 01:18:21,042 - mmdet - INFO - Epoch [1][34900/45697]	lr: 1.250e-05, eta: 2 days, 4:05:45, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 4.4405, loss_sub_cls: 19.7046, loss_obj_cls: 19.2535, loss_match: 4.6709, loss: 48.0696, grad_norm: 226.7643
2025-06-23 01:18:35,219 - mmdet - INFO - Epoch [1][34950/45697]	lr: 1.250e-05, eta: 2 days, 4:05:26, time: 0.284, data_time: 0.013, memory: 3036, loss_r_cls: 4.8490, loss_sub_cls: 20.4346, loss_obj_cls: 19.4469, loss_match: 4.7119, loss: 49.4423, grad_norm: 220.1223
2025-06-23 01:18:48,674 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:18:48,674 - mmdet - INFO - Epoch [1][35000/45697]	lr: 1.250e-05, eta: 2 days, 4:04:54, time: 0.269, data_time: 0.014, memory: 3036, loss_r_cls: 4.8156, loss_sub_cls: 19.6486, loss_obj_cls: 19.1494, loss_match: 4.4980, loss: 48.1117, grad_norm: 208.7796
2025-06-23 01:19:02,475 - mmdet - INFO - Epoch [1][35050/45697]	lr: 1.250e-05, eta: 2 days, 4:04:28, time: 0.276, data_time: 0.014, memory: 3036, loss_r_cls: 4.7476, loss_sub_cls: 20.5130, loss_obj_cls: 19.0993, loss_match: 4.8873, loss: 49.2472, grad_norm: 252.0905
2025-06-23 01:19:16,098 - mmdet - INFO - Epoch [1][35100/45697]	lr: 1.250e-05, eta: 2 days, 4:03:59, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 5.1805, loss_sub_cls: 20.1913, loss_obj_cls: 19.1178, loss_match: 4.6792, loss: 49.1688, grad_norm: 214.4445
2025-06-23 01:19:30,212 - mmdet - INFO - Epoch [1][35150/45697]	lr: 1.250e-05, eta: 2 days, 4:03:39, time: 0.282, data_time: 0.014, memory: 3036, loss_r_cls: 4.6184, loss_sub_cls: 20.0621, loss_obj_cls: 19.1791, loss_match: 4.1818, loss: 48.0413, grad_norm: 221.4483
2025-06-23 01:19:43,932 - mmdet - INFO - Epoch [1][35200/45697]	lr: 1.250e-05, eta: 2 days, 4:03:12, time: 0.274, data_time: 0.014, memory: 3036, loss_r_cls: 4.1932, loss_sub_cls: 19.9456, loss_obj_cls: 19.6487, loss_match: 5.0336, loss: 48.8211, grad_norm: 259.7947
2025-06-23 01:19:57,462 - mmdet - INFO - Epoch [1][35250/45697]	lr: 1.250e-05, eta: 2 days, 4:02:42, time: 0.271, data_time: 0.014, memory: 3036, loss_r_cls: 4.7841, loss_sub_cls: 20.3644, loss_obj_cls: 19.3674, loss_match: 4.7318, loss: 49.2477, grad_norm: 230.5800
2025-06-23 01:20:11,181 - mmdet - INFO - Epoch [1][35300/45697]	lr: 1.250e-05, eta: 2 days, 4:02:15, time: 0.274, data_time: 0.015, memory: 3036, loss_r_cls: 4.5590, loss_sub_cls: 20.5119, loss_obj_cls: 19.4918, loss_match: 4.8042, loss: 49.3669, grad_norm: 244.4320
2025-06-23 01:20:24,813 - mmdet - INFO - Epoch [1][35350/45697]	lr: 1.250e-05, eta: 2 days, 4:01:46, time: 0.273, data_time: 0.015, memory: 3036, loss_r_cls: 4.5990, loss_sub_cls: 20.2852, loss_obj_cls: 20.0415, loss_match: 4.3962, loss: 49.3219, grad_norm: 231.4484
2025-06-23 01:20:38,524 - mmdet - INFO - Epoch [1][35400/45697]	lr: 1.250e-05, eta: 2 days, 4:01:19, time: 0.274, data_time: 0.015, memory: 3036, loss_r_cls: 4.4298, loss_sub_cls: 20.0939, loss_obj_cls: 19.5714, loss_match: 4.9239, loss: 49.0190, grad_norm: 252.6342
2025-06-23 01:20:52,832 - mmdet - INFO - Epoch [1][35450/45697]	lr: 1.250e-05, eta: 2 days, 4:01:03, time: 0.286, data_time: 0.014, memory: 3036, loss_r_cls: 4.7006, loss_sub_cls: 20.4652, loss_obj_cls: 19.5575, loss_match: 4.1373, loss: 48.8606, grad_norm: 220.7859
2025-06-23 01:21:07,262 - mmdet - INFO - Epoch [1][35500/45697]	lr: 1.250e-05, eta: 2 days, 4:00:49, time: 0.289, data_time: 0.013, memory: 3036, loss_r_cls: 4.6658, loss_sub_cls: 19.5829, loss_obj_cls: 19.4669, loss_match: 4.4729, loss: 48.1885, grad_norm: 237.8325
2025-06-23 01:21:21,152 - mmdet - INFO - Epoch [1][35550/45697]	lr: 1.250e-05, eta: 2 days, 4:00:25, time: 0.278, data_time: 0.014, memory: 3036, loss_r_cls: 4.5529, loss_sub_cls: 20.1862, loss_obj_cls: 19.1562, loss_match: 4.5611, loss: 48.4564, grad_norm: 212.7756
2025-06-23 01:21:34,773 - mmdet - INFO - Epoch [1][35600/45697]	lr: 1.250e-05, eta: 2 days, 3:59:56, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.4857, loss_sub_cls: 19.9196, loss_obj_cls: 19.2901, loss_match: 4.6360, loss: 48.3313, grad_norm: 231.3363
2025-06-23 01:21:48,346 - mmdet - INFO - Epoch [1][35650/45697]	lr: 1.250e-05, eta: 2 days, 3:59:27, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.4281, loss_sub_cls: 19.9148, loss_obj_cls: 19.5723, loss_match: 4.7504, loss: 48.6657, grad_norm: 228.3162
2025-06-23 01:22:01,793 - mmdet - INFO - Epoch [1][35700/45697]	lr: 1.250e-05, eta: 2 days, 3:58:55, time: 0.269, data_time: 0.013, memory: 3036, loss_r_cls: 4.3097, loss_sub_cls: 19.8568, loss_obj_cls: 19.5154, loss_match: 4.2261, loss: 47.9081, grad_norm: 243.1366
2025-06-23 01:22:15,406 - mmdet - INFO - Epoch [1][35750/45697]	lr: 1.250e-05, eta: 2 days, 3:58:26, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 4.8898, loss_sub_cls: 20.1562, loss_obj_cls: 19.3735, loss_match: 4.7131, loss: 49.1326, grad_norm: 262.3404
2025-06-23 01:22:28,982 - mmdet - INFO - Epoch [1][35800/45697]	lr: 1.250e-05, eta: 2 days, 3:57:57, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.6877, loss_sub_cls: 20.1036, loss_obj_cls: 19.7809, loss_match: 4.4746, loss: 49.0468, grad_norm: 233.3857
2025-06-23 01:22:42,628 - mmdet - INFO - Epoch [1][35850/45697]	lr: 1.250e-05, eta: 2 days, 3:57:29, time: 0.273, data_time: 0.016, memory: 3036, loss_r_cls: 5.0636, loss_sub_cls: 20.6114, loss_obj_cls: 19.2026, loss_match: 4.6503, loss: 49.5279, grad_norm: 221.2398
2025-06-23 01:22:56,431 - mmdet - INFO - Epoch [1][35900/45697]	lr: 1.250e-05, eta: 2 days, 3:57:04, time: 0.276, data_time: 0.014, memory: 3036, loss_r_cls: 4.3609, loss_sub_cls: 20.2825, loss_obj_cls: 19.7211, loss_match: 4.6104, loss: 48.9750, grad_norm: 261.4510
2025-06-23 01:23:09,867 - mmdet - INFO - Epoch [1][35950/45697]	lr: 1.250e-05, eta: 2 days, 3:56:32, time: 0.269, data_time: 0.014, memory: 3036, loss_r_cls: 4.4950, loss_sub_cls: 20.2118, loss_obj_cls: 19.5084, loss_match: 4.3014, loss: 48.5166, grad_norm: 240.3511
2025-06-23 01:23:23,919 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:23:23,919 - mmdet - INFO - Epoch [1][36000/45697]	lr: 1.250e-05, eta: 2 days, 3:56:11, time: 0.281, data_time: 0.013, memory: 3036, loss_r_cls: 4.7297, loss_sub_cls: 20.2586, loss_obj_cls: 19.4451, loss_match: 5.0968, loss: 49.5301, grad_norm: 265.9308
2025-06-23 01:23:37,706 - mmdet - INFO - Epoch [1][36050/45697]	lr: 1.250e-05, eta: 2 days, 3:55:46, time: 0.276, data_time: 0.014, memory: 3036, loss_r_cls: 4.4311, loss_sub_cls: 20.1393, loss_obj_cls: 19.2500, loss_match: 4.8060, loss: 48.6264, grad_norm: 230.5292
2025-06-23 01:23:51,349 - mmdet - INFO - Epoch [1][36100/45697]	lr: 1.250e-05, eta: 2 days, 3:55:18, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 4.9568, loss_sub_cls: 20.3720, loss_obj_cls: 19.5809, loss_match: 3.9672, loss: 48.8770, grad_norm: 238.1304
2025-06-23 01:24:04,812 - mmdet - INFO - Epoch [1][36150/45697]	lr: 1.250e-05, eta: 2 days, 3:54:47, time: 0.269, data_time: 0.013, memory: 3036, loss_r_cls: 5.1881, loss_sub_cls: 20.4518, loss_obj_cls: 19.3854, loss_match: 4.5604, loss: 49.5857, grad_norm: 275.1641
2025-06-23 01:24:19,051 - mmdet - INFO - Epoch [1][36200/45697]	lr: 1.250e-05, eta: 2 days, 3:54:30, time: 0.285, data_time: 0.016, memory: 3036, loss_r_cls: 5.0396, loss_sub_cls: 19.9979, loss_obj_cls: 19.5203, loss_match: 4.1127, loss: 48.6705, grad_norm: 222.3766
2025-06-23 01:24:32,582 - mmdet - INFO - Epoch [1][36250/45697]	lr: 1.250e-05, eta: 2 days, 3:54:00, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.4016, loss_sub_cls: 19.8928, loss_obj_cls: 19.5847, loss_match: 4.5268, loss: 48.4059, grad_norm: 240.4416
2025-06-23 01:24:46,076 - mmdet - INFO - Epoch [1][36300/45697]	lr: 1.250e-05, eta: 2 days, 3:53:30, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.7416, loss_sub_cls: 20.2329, loss_obj_cls: 19.1854, loss_match: 3.9231, loss: 48.0832, grad_norm: 258.0527
2025-06-23 01:24:59,653 - mmdet - INFO - Epoch [1][36350/45697]	lr: 1.250e-05, eta: 2 days, 3:53:01, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 5.1466, loss_sub_cls: 20.2027, loss_obj_cls: 19.2734, loss_match: 4.3636, loss: 48.9863, grad_norm: 266.0177
2025-06-23 01:25:14,724 - mmdet - INFO - Epoch [1][36400/45697]	lr: 1.250e-05, eta: 2 days, 3:52:58, time: 0.301, data_time: 0.014, memory: 3036, loss_r_cls: 4.5642, loss_sub_cls: 19.6370, loss_obj_cls: 19.7882, loss_match: 4.3001, loss: 48.2895, grad_norm: 240.3495
2025-06-23 01:25:29,303 - mmdet - INFO - Epoch [1][36450/45697]	lr: 1.250e-05, eta: 2 days, 3:52:47, time: 0.292, data_time: 0.015, memory: 3036, loss_r_cls: 4.7589, loss_sub_cls: 19.8356, loss_obj_cls: 19.4899, loss_match: 4.1826, loss: 48.2670, grad_norm: 233.0192
2025-06-23 01:25:42,961 - mmdet - INFO - Epoch [1][36500/45697]	lr: 1.250e-05, eta: 2 days, 3:52:20, time: 0.273, data_time: 0.012, memory: 3036, loss_r_cls: 4.4002, loss_sub_cls: 19.5892, loss_obj_cls: 19.1071, loss_match: 4.2298, loss: 47.3263, grad_norm: 239.0217
2025-06-23 01:25:56,804 - mmdet - INFO - Epoch [1][36550/45697]	lr: 1.250e-05, eta: 2 days, 3:51:56, time: 0.277, data_time: 0.013, memory: 3036, loss_r_cls: 4.6345, loss_sub_cls: 19.5788, loss_obj_cls: 19.5810, loss_match: 4.1005, loss: 47.8948, grad_norm: 217.3094
2025-06-23 01:26:10,063 - mmdet - INFO - Epoch [1][36600/45697]	lr: 1.250e-05, eta: 2 days, 3:51:22, time: 0.265, data_time: 0.011, memory: 3036, loss_r_cls: 4.7050, loss_sub_cls: 20.0644, loss_obj_cls: 19.4529, loss_match: 4.4647, loss: 48.6870, grad_norm: 259.1833
2025-06-23 01:26:23,723 - mmdet - INFO - Epoch [1][36650/45697]	lr: 1.250e-05, eta: 2 days, 3:50:54, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 4.8300, loss_sub_cls: 19.8543, loss_obj_cls: 19.6255, loss_match: 3.6266, loss: 47.9364, grad_norm: 223.6889
2025-06-23 01:26:37,288 - mmdet - INFO - Epoch [1][36700/45697]	lr: 1.250e-05, eta: 2 days, 3:50:25, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.6884, loss_sub_cls: 19.7350, loss_obj_cls: 19.2996, loss_match: 4.8725, loss: 48.5954, grad_norm: 239.5154
2025-06-23 01:26:50,747 - mmdet - INFO - Epoch [1][36750/45697]	lr: 1.250e-05, eta: 2 days, 3:49:55, time: 0.269, data_time: 0.013, memory: 3036, loss_r_cls: 4.8649, loss_sub_cls: 19.6992, loss_obj_cls: 19.2099, loss_match: 3.9266, loss: 47.7006, grad_norm: 210.8708
2025-06-23 01:27:04,626 - mmdet - INFO - Epoch [1][36800/45697]	lr: 1.250e-05, eta: 2 days, 3:49:32, time: 0.278, data_time: 0.013, memory: 3036, loss_r_cls: 4.7697, loss_sub_cls: 20.1334, loss_obj_cls: 19.6738, loss_match: 4.1122, loss: 48.6890, grad_norm: 222.6196
2025-06-23 01:27:18,470 - mmdet - INFO - Epoch [1][36850/45697]	lr: 1.250e-05, eta: 2 days, 3:49:08, time: 0.277, data_time: 0.015, memory: 3036, loss_r_cls: 4.8968, loss_sub_cls: 19.7615, loss_obj_cls: 19.2809, loss_match: 4.7511, loss: 48.6902, grad_norm: 233.8911
2025-06-23 01:27:32,195 - mmdet - INFO - Epoch [1][36900/45697]	lr: 1.250e-05, eta: 2 days, 3:48:42, time: 0.274, data_time: 0.014, memory: 3036, loss_r_cls: 4.6579, loss_sub_cls: 19.4848, loss_obj_cls: 19.4612, loss_match: 4.0127, loss: 47.6167, grad_norm: 193.6614
2025-06-23 01:27:45,983 - mmdet - INFO - Epoch [1][36950/45697]	lr: 1.250e-05, eta: 2 days, 3:48:17, time: 0.276, data_time: 0.015, memory: 3036, loss_r_cls: 4.9506, loss_sub_cls: 19.5670, loss_obj_cls: 19.5744, loss_match: 5.0965, loss: 49.1885, grad_norm: 265.2381
2025-06-23 01:27:59,476 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:27:59,476 - mmdet - INFO - Epoch [1][37000/45697]	lr: 1.250e-05, eta: 2 days, 3:47:47, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 4.6976, loss_sub_cls: 19.6061, loss_obj_cls: 19.2770, loss_match: 4.1493, loss: 47.7301, grad_norm: 197.0295
2025-06-23 01:28:12,846 - mmdet - INFO - Epoch [1][37050/45697]	lr: 1.250e-05, eta: 2 days, 3:47:15, time: 0.267, data_time: 0.013, memory: 3036, loss_r_cls: 4.9350, loss_sub_cls: 19.6202, loss_obj_cls: 19.3990, loss_match: 4.7027, loss: 48.6569, grad_norm: 270.7043
2025-06-23 01:28:26,283 - mmdet - INFO - Epoch [1][37100/45697]	lr: 1.250e-05, eta: 2 days, 3:46:44, time: 0.269, data_time: 0.014, memory: 3036, loss_r_cls: 4.3321, loss_sub_cls: 19.3736, loss_obj_cls: 18.9329, loss_match: 4.3873, loss: 47.0259, grad_norm: 246.7635
2025-06-23 01:28:39,758 - mmdet - INFO - Epoch [1][37150/45697]	lr: 1.250e-05, eta: 2 days, 3:46:14, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.7537, loss_sub_cls: 19.6205, loss_obj_cls: 19.7419, loss_match: 3.9031, loss: 48.0191, grad_norm: 254.9181
2025-06-23 01:28:53,377 - mmdet - INFO - Epoch [1][37200/45697]	lr: 1.250e-05, eta: 2 days, 3:45:47, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.5891, loss_sub_cls: 19.9695, loss_obj_cls: 19.5026, loss_match: 3.7032, loss: 47.7644, grad_norm: 231.1886
2025-06-23 01:29:06,865 - mmdet - INFO - Epoch [1][37250/45697]	lr: 1.250e-05, eta: 2 days, 3:45:17, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 4.7505, loss_sub_cls: 19.7772, loss_obj_cls: 19.5597, loss_match: 4.6809, loss: 48.7683, grad_norm: 276.4275
2025-06-23 01:29:20,069 - mmdet - INFO - Epoch [1][37300/45697]	lr: 1.250e-05, eta: 2 days, 3:44:42, time: 0.264, data_time: 0.011, memory: 3036, loss_r_cls: 4.8150, loss_sub_cls: 19.8836, loss_obj_cls: 19.6147, loss_match: 4.1645, loss: 48.4778, grad_norm: 253.0736
2025-06-23 01:29:33,592 - mmdet - INFO - Epoch [1][37350/45697]	lr: 1.250e-05, eta: 2 days, 3:44:13, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 5.0043, loss_sub_cls: 20.1520, loss_obj_cls: 19.1076, loss_match: 3.9997, loss: 48.2636, grad_norm: 227.9408
2025-06-23 01:29:47,515 - mmdet - INFO - Epoch [1][37400/45697]	lr: 1.250e-05, eta: 2 days, 3:43:51, time: 0.278, data_time: 0.016, memory: 3036, loss_r_cls: 4.6247, loss_sub_cls: 19.4077, loss_obj_cls: 19.2895, loss_match: 4.2369, loss: 47.5587, grad_norm: 259.6834
2025-06-23 01:30:00,975 - mmdet - INFO - Epoch [1][37450/45697]	lr: 1.250e-05, eta: 2 days, 3:43:21, time: 0.269, data_time: 0.014, memory: 3036, loss_r_cls: 4.8406, loss_sub_cls: 19.8764, loss_obj_cls: 19.5036, loss_match: 3.6932, loss: 47.9138, grad_norm: 225.0719
2025-06-23 01:30:14,354 - mmdet - INFO - Epoch [1][37500/45697]	lr: 1.250e-05, eta: 2 days, 3:42:49, time: 0.268, data_time: 0.014, memory: 3036, loss_r_cls: 4.6465, loss_sub_cls: 19.7180, loss_obj_cls: 18.8553, loss_match: 4.3377, loss: 47.5575, grad_norm: 256.8268
2025-06-23 01:30:28,060 - mmdet - INFO - Epoch [1][37550/45697]	lr: 1.250e-05, eta: 2 days, 3:42:24, time: 0.274, data_time: 0.013, memory: 3036, loss_r_cls: 4.7533, loss_sub_cls: 19.8422, loss_obj_cls: 19.5229, loss_match: 4.1943, loss: 48.3127, grad_norm: 269.9447
2025-06-23 01:30:41,588 - mmdet - INFO - Epoch [1][37600/45697]	lr: 1.250e-05, eta: 2 days, 3:41:55, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.8761, loss_sub_cls: 19.4880, loss_obj_cls: 19.2033, loss_match: 4.1388, loss: 47.7062, grad_norm: 200.9434
2025-06-23 01:30:54,887 - mmdet - INFO - Epoch [1][37650/45697]	lr: 1.250e-05, eta: 2 days, 3:41:22, time: 0.266, data_time: 0.013, memory: 3036, loss_r_cls: 4.6077, loss_sub_cls: 19.8178, loss_obj_cls: 19.7075, loss_match: 3.8924, loss: 48.0254, grad_norm: 236.8378
2025-06-23 01:31:08,472 - mmdet - INFO - Epoch [1][37700/45697]	lr: 1.250e-05, eta: 2 days, 3:40:55, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.7717, loss_sub_cls: 19.4945, loss_obj_cls: 18.9388, loss_match: 4.4565, loss: 47.6615, grad_norm: 283.7611
2025-06-23 01:31:22,453 - mmdet - INFO - Epoch [1][37750/45697]	lr: 1.250e-05, eta: 2 days, 3:40:34, time: 0.280, data_time: 0.016, memory: 3036, loss_r_cls: 4.8501, loss_sub_cls: 19.2034, loss_obj_cls: 19.4325, loss_match: 5.1780, loss: 48.6640, grad_norm: 312.1385
2025-06-23 01:31:36,043 - mmdet - INFO - Epoch [1][37800/45697]	lr: 1.250e-05, eta: 2 days, 3:40:06, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 4.8822, loss_sub_cls: 19.6391, loss_obj_cls: 19.3254, loss_match: 4.5215, loss: 48.3682, grad_norm: 249.9550
2025-06-23 01:31:49,426 - mmdet - INFO - Epoch [1][37850/45697]	lr: 1.250e-05, eta: 2 days, 3:39:35, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 4.8388, loss_sub_cls: 19.8390, loss_obj_cls: 19.2086, loss_match: 4.0237, loss: 47.9102, grad_norm: 245.6701
2025-06-23 01:32:02,910 - mmdet - INFO - Epoch [1][37900/45697]	lr: 1.250e-05, eta: 2 days, 3:39:06, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 5.0028, loss_sub_cls: 19.6789, loss_obj_cls: 19.2626, loss_match: 4.5578, loss: 48.5021, grad_norm: 258.3606
2025-06-23 01:32:16,614 - mmdet - INFO - Epoch [1][37950/45697]	lr: 1.250e-05, eta: 2 days, 3:38:40, time: 0.274, data_time: 0.015, memory: 3036, loss_r_cls: 4.7077, loss_sub_cls: 19.6382, loss_obj_cls: 19.1012, loss_match: 4.5977, loss: 48.0447, grad_norm: 258.9847
2025-06-23 01:32:30,500 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:32:30,501 - mmdet - INFO - Epoch [1][38000/45697]	lr: 1.250e-05, eta: 2 days, 3:38:18, time: 0.278, data_time: 0.012, memory: 3036, loss_r_cls: 4.7746, loss_sub_cls: 19.5461, loss_obj_cls: 19.1648, loss_match: 4.2475, loss: 47.7331, grad_norm: 271.7513
2025-06-23 01:32:44,515 - mmdet - INFO - Epoch [1][38050/45697]	lr: 1.250e-05, eta: 2 days, 3:37:58, time: 0.280, data_time: 0.015, memory: 3036, loss_r_cls: 4.1791, loss_sub_cls: 19.6293, loss_obj_cls: 19.7103, loss_match: 4.8719, loss: 48.3905, grad_norm: 268.1691
2025-06-23 01:32:58,063 - mmdet - INFO - Epoch [1][38100/45697]	lr: 1.250e-05, eta: 2 days, 3:37:30, time: 0.271, data_time: 0.014, memory: 3036, loss_r_cls: 5.0874, loss_sub_cls: 19.7543, loss_obj_cls: 19.2969, loss_match: 5.4593, loss: 49.5979, grad_norm: 291.7496
2025-06-23 01:33:11,606 - mmdet - INFO - Epoch [1][38150/45697]	lr: 1.250e-05, eta: 2 days, 3:37:01, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 4.4505, loss_sub_cls: 19.5632, loss_obj_cls: 19.4110, loss_match: 4.5590, loss: 47.9837, grad_norm: 250.8548
2025-06-23 01:33:25,480 - mmdet - INFO - Epoch [1][38200/45697]	lr: 1.250e-05, eta: 2 days, 3:36:39, time: 0.277, data_time: 0.014, memory: 3036, loss_r_cls: 4.5639, loss_sub_cls: 19.1625, loss_obj_cls: 18.9660, loss_match: 4.0992, loss: 46.7916, grad_norm: 256.9741
2025-06-23 01:33:38,731 - mmdet - INFO - Epoch [1][38250/45697]	lr: 1.250e-05, eta: 2 days, 3:36:06, time: 0.265, data_time: 0.013, memory: 3036, loss_r_cls: 4.3826, loss_sub_cls: 19.4035, loss_obj_cls: 19.1279, loss_match: 4.1780, loss: 47.0920, grad_norm: 220.5312
2025-06-23 01:33:52,071 - mmdet - INFO - Epoch [1][38300/45697]	lr: 1.250e-05, eta: 2 days, 3:35:34, time: 0.267, data_time: 0.013, memory: 3036, loss_r_cls: 4.8504, loss_sub_cls: 20.1151, loss_obj_cls: 19.4999, loss_match: 3.9745, loss: 48.4400, grad_norm: 221.1101
2025-06-23 01:34:05,602 - mmdet - INFO - Epoch [1][38350/45697]	lr: 1.250e-05, eta: 2 days, 3:35:06, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 5.1410, loss_sub_cls: 19.7720, loss_obj_cls: 19.3732, loss_match: 3.8445, loss: 48.1308, grad_norm: 231.8746
2025-06-23 01:34:19,212 - mmdet - INFO - Epoch [1][38400/45697]	lr: 1.250e-05, eta: 2 days, 3:34:39, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.7727, loss_sub_cls: 19.7136, loss_obj_cls: 19.1087, loss_match: 4.1587, loss: 47.7537, grad_norm: 259.7241
2025-06-23 01:34:33,061 - mmdet - INFO - Epoch [1][38450/45697]	lr: 1.250e-05, eta: 2 days, 3:34:17, time: 0.277, data_time: 0.016, memory: 3036, loss_r_cls: 5.2894, loss_sub_cls: 19.9800, loss_obj_cls: 19.3587, loss_match: 4.6742, loss: 49.3024, grad_norm: 243.5754
2025-06-23 01:34:47,183 - mmdet - INFO - Epoch [1][38500/45697]	lr: 1.250e-05, eta: 2 days, 3:33:59, time: 0.282, data_time: 0.013, memory: 3036, loss_r_cls: 5.0215, loss_sub_cls: 20.0252, loss_obj_cls: 19.0148, loss_match: 3.8659, loss: 47.9275, grad_norm: 209.9394
2025-06-23 01:35:00,601 - mmdet - INFO - Epoch [1][38550/45697]	lr: 1.250e-05, eta: 2 days, 3:33:29, time: 0.268, data_time: 0.014, memory: 3036, loss_r_cls: 5.5846, loss_sub_cls: 19.6202, loss_obj_cls: 18.8407, loss_match: 4.2425, loss: 48.2879, grad_norm: 222.6566
2025-06-23 01:35:14,620 - mmdet - INFO - Epoch [1][38600/45697]	lr: 1.250e-05, eta: 2 days, 3:33:09, time: 0.280, data_time: 0.016, memory: 3036, loss_r_cls: 4.7307, loss_sub_cls: 19.6414, loss_obj_cls: 19.2899, loss_match: 3.8661, loss: 47.5281, grad_norm: 227.2189
2025-06-23 01:35:28,126 - mmdet - INFO - Epoch [1][38650/45697]	lr: 1.250e-05, eta: 2 days, 3:32:40, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.5512, loss_sub_cls: 19.6486, loss_obj_cls: 19.2868, loss_match: 3.7400, loss: 47.2266, grad_norm: 214.4238
2025-06-23 01:35:41,734 - mmdet - INFO - Epoch [1][38700/45697]	lr: 1.250e-05, eta: 2 days, 3:32:14, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.7781, loss_sub_cls: 19.2547, loss_obj_cls: 19.1569, loss_match: 4.2229, loss: 47.4127, grad_norm: 235.5442
2025-06-23 01:35:55,373 - mmdet - INFO - Epoch [1][38750/45697]	lr: 1.250e-05, eta: 2 days, 3:31:48, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 4.5719, loss_sub_cls: 19.9585, loss_obj_cls: 19.5056, loss_match: 3.6840, loss: 47.7199, grad_norm: 208.9225
2025-06-23 01:36:09,043 - mmdet - INFO - Epoch [1][38800/45697]	lr: 1.250e-05, eta: 2 days, 3:31:22, time: 0.273, data_time: 0.015, memory: 3036, loss_r_cls: 5.0929, loss_sub_cls: 19.9072, loss_obj_cls: 19.5030, loss_match: 4.0292, loss: 48.5323, grad_norm: 241.0587
2025-06-23 01:36:22,597 - mmdet - INFO - Epoch [1][38850/45697]	lr: 1.250e-05, eta: 2 days, 3:30:55, time: 0.271, data_time: 0.014, memory: 3036, loss_r_cls: 4.6359, loss_sub_cls: 19.5457, loss_obj_cls: 19.0952, loss_match: 4.5247, loss: 47.8014, grad_norm: 245.1445
2025-06-23 01:36:37,421 - mmdet - INFO - Epoch [1][38900/45697]	lr: 1.250e-05, eta: 2 days, 3:30:48, time: 0.296, data_time: 0.013, memory: 3036, loss_r_cls: 4.7470, loss_sub_cls: 19.5919, loss_obj_cls: 19.3629, loss_match: 4.4737, loss: 48.1755, grad_norm: 221.3994
2025-06-23 01:36:51,391 - mmdet - INFO - Epoch [1][38950/45697]	lr: 1.250e-05, eta: 2 days, 3:30:28, time: 0.279, data_time: 0.012, memory: 3036, loss_r_cls: 4.1905, loss_sub_cls: 19.9511, loss_obj_cls: 19.2114, loss_match: 4.9399, loss: 48.2927, grad_norm: 271.8621
2025-06-23 01:37:05,010 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:37:05,010 - mmdet - INFO - Epoch [1][39000/45697]	lr: 1.250e-05, eta: 2 days, 3:30:02, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 5.0789, loss_sub_cls: 20.0616, loss_obj_cls: 19.4880, loss_match: 4.1948, loss: 48.8233, grad_norm: 236.1314
2025-06-23 01:37:19,216 - mmdet - INFO - Epoch [1][39050/45697]	lr: 1.250e-05, eta: 2 days, 3:29:45, time: 0.284, data_time: 0.016, memory: 3036, loss_r_cls: 4.7851, loss_sub_cls: 20.1477, loss_obj_cls: 19.3427, loss_match: 4.3417, loss: 48.6173, grad_norm: 271.2602
2025-06-23 01:37:33,035 - mmdet - INFO - Epoch [1][39100/45697]	lr: 1.250e-05, eta: 2 days, 3:29:22, time: 0.276, data_time: 0.016, memory: 3036, loss_r_cls: 5.0958, loss_sub_cls: 20.0279, loss_obj_cls: 19.6164, loss_match: 4.6696, loss: 49.4097, grad_norm: 240.2982
2025-06-23 01:37:46,437 - mmdet - INFO - Epoch [1][39150/45697]	lr: 1.250e-05, eta: 2 days, 3:28:52, time: 0.268, data_time: 0.011, memory: 3036, loss_r_cls: 4.4190, loss_sub_cls: 20.1511, loss_obj_cls: 19.1984, loss_match: 5.2619, loss: 49.0303, grad_norm: 272.5975
2025-06-23 01:38:00,785 - mmdet - INFO - Epoch [1][39200/45697]	lr: 1.250e-05, eta: 2 days, 3:28:38, time: 0.287, data_time: 0.014, memory: 3036, loss_r_cls: 4.9840, loss_sub_cls: 19.9173, loss_obj_cls: 19.7854, loss_match: 4.3623, loss: 49.0490, grad_norm: 218.3186
2025-06-23 01:38:14,344 - mmdet - INFO - Epoch [1][39250/45697]	lr: 1.250e-05, eta: 2 days, 3:28:11, time: 0.271, data_time: 0.013, memory: 3036, loss_r_cls: 5.1034, loss_sub_cls: 20.0905, loss_obj_cls: 19.8123, loss_match: 5.2764, loss: 50.2826, grad_norm: 243.8239
2025-06-23 01:38:28,139 - mmdet - INFO - Epoch [1][39300/45697]	lr: 1.250e-05, eta: 2 days, 3:27:48, time: 0.276, data_time: 0.016, memory: 3036, loss_r_cls: 4.4026, loss_sub_cls: 20.3717, loss_obj_cls: 19.7420, loss_match: 4.5730, loss: 49.0893, grad_norm: 229.8520
2025-06-23 01:38:41,787 - mmdet - INFO - Epoch [1][39350/45697]	lr: 1.250e-05, eta: 2 days, 3:27:22, time: 0.273, data_time: 0.015, memory: 3036, loss_r_cls: 4.9532, loss_sub_cls: 20.5552, loss_obj_cls: 19.1492, loss_match: 4.7154, loss: 49.3730, grad_norm: 257.4970
2025-06-23 01:38:55,166 - mmdet - INFO - Epoch [1][39400/45697]	lr: 1.250e-05, eta: 2 days, 3:26:52, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 4.9366, loss_sub_cls: 19.7899, loss_obj_cls: 19.1773, loss_match: 4.0451, loss: 47.9490, grad_norm: 215.1204
2025-06-23 01:39:09,337 - mmdet - INFO - Epoch [1][39450/45697]	lr: 1.250e-05, eta: 2 days, 3:26:35, time: 0.283, data_time: 0.014, memory: 3036, loss_r_cls: 5.1688, loss_sub_cls: 19.9301, loss_obj_cls: 19.3463, loss_match: 4.4353, loss: 48.8805, grad_norm: 210.2134
2025-06-23 01:39:23,742 - mmdet - INFO - Epoch [1][39500/45697]	lr: 1.250e-05, eta: 2 days, 3:26:22, time: 0.288, data_time: 0.013, memory: 3036, loss_r_cls: 4.8592, loss_sub_cls: 19.7511, loss_obj_cls: 19.4516, loss_match: 4.4081, loss: 48.4699, grad_norm: 271.4644
2025-06-23 01:39:37,343 - mmdet - INFO - Epoch [1][39550/45697]	lr: 1.250e-05, eta: 2 days, 3:25:55, time: 0.272, data_time: 0.014, memory: 3036, loss_r_cls: 4.2797, loss_sub_cls: 19.3396, loss_obj_cls: 19.3877, loss_match: 4.8637, loss: 47.8707, grad_norm: 248.5677
2025-06-23 01:39:51,267 - mmdet - INFO - Epoch [1][39600/45697]	lr: 1.250e-05, eta: 2 days, 3:25:34, time: 0.278, data_time: 0.015, memory: 3036, loss_r_cls: 5.1629, loss_sub_cls: 19.5125, loss_obj_cls: 19.7822, loss_match: 4.6308, loss: 49.0885, grad_norm: 256.1993
2025-06-23 01:40:05,790 - mmdet - INFO - Epoch [1][39650/45697]	lr: 1.250e-05, eta: 2 days, 3:25:23, time: 0.290, data_time: 0.012, memory: 3036, loss_r_cls: 4.5574, loss_sub_cls: 19.7983, loss_obj_cls: 19.1783, loss_match: 4.5465, loss: 48.0806, grad_norm: 235.2460
2025-06-23 01:40:19,346 - mmdet - INFO - Epoch [1][39700/45697]	lr: 1.250e-05, eta: 2 days, 3:24:56, time: 0.271, data_time: 0.015, memory: 3036, loss_r_cls: 4.4933, loss_sub_cls: 20.1054, loss_obj_cls: 19.5701, loss_match: 4.3108, loss: 48.4795, grad_norm: 211.9127
2025-06-23 01:40:33,038 - mmdet - INFO - Epoch [1][39750/45697]	lr: 1.250e-05, eta: 2 days, 3:24:31, time: 0.274, data_time: 0.014, memory: 3036, loss_r_cls: 5.1963, loss_sub_cls: 20.2840, loss_obj_cls: 19.2321, loss_match: 4.4674, loss: 49.1797, grad_norm: 225.5913
2025-06-23 01:40:46,678 - mmdet - INFO - Epoch [1][39800/45697]	lr: 1.250e-05, eta: 2 days, 3:24:06, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 5.3272, loss_sub_cls: 19.8043, loss_obj_cls: 19.2017, loss_match: 3.7515, loss: 48.0846, grad_norm: 232.0154
2025-06-23 01:41:00,086 - mmdet - INFO - Epoch [1][39850/45697]	lr: 1.250e-05, eta: 2 days, 3:23:37, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 4.5528, loss_sub_cls: 19.7686, loss_obj_cls: 19.7472, loss_match: 4.5109, loss: 48.5794, grad_norm: 256.7162
2025-06-23 01:41:13,706 - mmdet - INFO - Epoch [1][39900/45697]	lr: 1.250e-05, eta: 2 days, 3:23:11, time: 0.272, data_time: 0.013, memory: 3036, loss_r_cls: 4.7904, loss_sub_cls: 19.7458, loss_obj_cls: 19.4617, loss_match: 3.8554, loss: 47.8532, grad_norm: 234.4920
2025-06-23 01:41:27,220 - mmdet - INFO - Epoch [1][39950/45697]	lr: 1.250e-05, eta: 2 days, 3:22:43, time: 0.270, data_time: 0.014, memory: 3036, loss_r_cls: 4.9811, loss_sub_cls: 19.8387, loss_obj_cls: 19.7037, loss_match: 3.9224, loss: 48.4459, grad_norm: 256.8914
2025-06-23 01:41:40,860 - mmdet - INFO - Exp name: pairnet.py
2025-06-23 01:41:40,861 - mmdet - INFO - Epoch [1][40000/45697]	lr: 1.250e-05, eta: 2 days, 3:22:18, time: 0.273, data_time: 0.014, memory: 3036, loss_r_cls: 4.5922, loss_sub_cls: 19.3977, loss_obj_cls: 19.2368, loss_match: 4.5376, loss: 47.7643, grad_norm: 268.4638
2025-06-23 01:41:54,284 - mmdet - INFO - Epoch [1][40050/45697]	lr: 1.250e-05, eta: 2 days, 3:21:49, time: 0.268, data_time: 0.013, memory: 3036, loss_r_cls: 4.4748, loss_sub_cls: 19.5566, loss_obj_cls: 19.3977, loss_match: 4.4848, loss: 47.9138, grad_norm: 241.7622
2025-06-23 01:42:07,781 - mmdet - INFO - Epoch [1][40100/45697]	lr: 1.250e-05, eta: 2 days, 3:21:21, time: 0.270, data_time: 0.013, memory: 3036, loss_r_cls: 4.5092, loss_sub_cls: 19.6894, loss_obj_cls: 19.4575, loss_match: 4.1253, loss: 47.7815, grad_norm: 254.1084
2025-06-23 01:42:21,151 - mmdet - INFO - Epoch [1][40150/45697]	lr: 1.250e-05, eta: 2 days, 3:20:52, time: 0.267, data_time: 0.012, memory: 3036, loss_r_cls: 4.7499, loss_sub_cls: 19.6413, loss_obj_cls: 19.3752, loss_match: 4.0679, loss: 47.8342, grad_norm: 238.2669
